"""
App post-run calibration integration

This appended block applies persisted isotonic calibration and refined sigma
to today's enriched predictions and generates the calibrated stake sheet.
It runs safely after the main app logic, only if required artifacts exist.
"""
from __future__ import annotations

from pathlib import Path as _Path
import pandas as _pd
import subprocess as _subprocess

def _safe_read_csv(_p: _Path) -> _pd.DataFrame:
    try:
        return _pd.read_csv(_p)
    except Exception:
        return _pd.DataFrame()

def _apply_calibration_and_sigma_post_run():
    try:
        outputs = _Path('outputs')
        enriched = _safe_read_csv(outputs / 'predictions_history_enriched.csv')
        if enriched.empty or 'date' not in enriched.columns:
            return
        latest = str(sorted(enriched['date'].dropna().astype(str).unique())[-1])
        enriched['game_id'] = enriched['game_id'].astype(str).str.replace(r'\.0$','', regex=True)
        today_df = enriched[enriched['date'].astype(str) == latest].copy()
        # Import helper from src; adjust path if needed
        try:
            from src.calibration_utils import load_calibration_params, apply_calibration_to_df, apply_sigma_intervals
        except Exception:
            import sys as _sys
            _sys.path.append(str(_Path('.').resolve() / 'src'))
            from calibration_utils import load_calibration_params, apply_calibration_to_df, apply_sigma_intervals
        params = load_calibration_params(outputs / 'calibration_params.json')
        if params:
            today_df = apply_calibration_to_df(today_df, params)
        sigma_df = _safe_read_csv(outputs / 'predictions_history_sigma.csv')
        if not sigma_df.empty and {'date','game_id'}.issubset(sigma_df.columns):
            sigma_df['game_id'] = sigma_df['game_id'].astype(str).str.replace(r'\.0$','', regex=True)
            today_df = today_df.merge(sigma_df[['date','game_id','sigma_total','sigma_margin']], on=['date','game_id'], how='left')
        today_df = apply_sigma_intervals(today_df, sigma_total_col='sigma_total')
        # Prefer quantile intervals when available
        qdf = _safe_read_csv(outputs / 'quantiles_history.csv')
        if not qdf.empty and {'date','game_id'}.issubset(qdf.columns):
            qdf['game_id'] = qdf['game_id'].astype(str).str.replace(r'\.0$','', regex=True)
            qdf_latest = qdf[qdf['date'].astype(str) == latest]
            cols = [c for c in ['date','game_id','q10_total','q50_total','q90_total','q10_margin','q50_margin','q90_margin'] if c in qdf.columns]
            if cols:
                today_df = today_df.merge(qdf_latest[cols], on=['date','game_id'], how='left')
                if {'q10_total','q90_total'}.issubset(today_df.columns):
                    today_df['total_p10'] = today_df['q10_total']
                    today_df['total_p50'] = today_df.get('q50_total', today_df.get('pred_total'))
                    today_df['total_p90'] = today_df['q90_total']
                if {'q10_margin','q90_margin'}.issubset(today_df.columns):
                    today_df['margin_p10'] = today_df['q10_margin']
                    today_df['margin_p50'] = today_df.get('q50_margin', today_df.get('pred_margin'))
                    today_df['margin_p90'] = today_df['q90_margin']
        else:
            # Fallback: compute residual-based central intervals on the fly
            try:
                bt = _safe_read_csv(outputs / 'backtest_reports' / 'backtest_joined.csv')
                if not bt.empty and {'pred_total','actual_total','pred_margin','actual_margin'}.issubset(bt.columns):
                    bt['_date'] = _pd.to_datetime(bt['date'], errors='coerce')
                    ref = _pd.to_datetime(latest, errors='coerce')
                    if _pd.isna(ref):
                        ref = _pd.to_datetime(bt['_date'].max())
                    win = bt[(bt['_date'] >= ref - _pd.Timedelta(days=28)) & (bt['_date'] <= ref)]
                    resid_t = (win['actual_total'] - win['pred_total']).dropna()
                    resid_m = (win['actual_margin'] - win['pred_margin']).dropna()
                    if len(resid_t) and len(resid_m):
                        eps = (1.0 - 0.8) / 2.0
                        ql_t = float(np.nanquantile(resid_t, eps))
                        qm_t = float(np.nanquantile(resid_t, 0.5))
                        qh_t = float(np.nanquantile(resid_t, 1 - eps))
                        ql_m = float(np.nanquantile(resid_m, eps))
                        qm_m = float(np.nanquantile(resid_m, 0.5))
                        qh_m = float(np.nanquantile(resid_m, 1 - eps))
                        # apply
                        today_df['q10_total'] = today_df['pred_total'] + ql_t
                        today_df['q50_total'] = today_df['pred_total'] + qm_t
                        today_df['q90_total'] = today_df['pred_total'] + qh_t
                        today_df['q10_margin'] = today_df['pred_margin'] + ql_m
                        today_df['q50_margin'] = today_df['pred_margin'] + qm_m
                        today_df['q90_margin'] = today_df['pred_margin'] + qh_m
                        # ensure monotone
                        def _sort3(a,b,c,row):
                            v = sorted([row[a], row[b], row[c]])
                            return _pd.Series(v, index=[a,b,c])
                        today_df[['q10_total','q50_total','q90_total']] = today_df.apply(lambda r: _sort3('q10_total','q50_total','q90_total', r), axis=1)
                        today_df[['q10_margin','q50_margin','q90_margin']] = today_df.apply(lambda r: _sort3('q10_margin','q50_margin','q90_margin', r), axis=1)
                        today_df['total_p10'] = today_df['q10_total']
                        today_df['total_p50'] = today_df.get('q50_total', today_df.get('pred_total'))
                        today_df['total_p90'] = today_df['q90_total']
                        today_df['margin_p10'] = today_df['q10_margin']
                        today_df['margin_p50'] = today_df.get('q50_margin', today_df.get('pred_margin'))
                        today_df['margin_p90'] = today_df['q90_margin']
            except Exception:
                pass
        out = outputs / 'predictions_today_calibrated.csv'
        today_df.to_csv(out, index=False)
        # Invoke staking script
        try:
            _subprocess.run(['python', 'scripts/stake_calibrated_kelly.py'], check=True)
        except Exception:
            pass
    except Exception:
        # Swallow errors to avoid impacting primary app flow
        pass

# Run post-run calibration block
_apply_calibration_and_sigma_post_run()

import json
import shutil
import os
# --- Meta models/sidecars preload (lightweight) ---
from typing import Optional
_META_PRELOAD_DONE = False
_META_FEATURES_CACHE = {
    "cover": None,
    "over": None,
}

def _read_json_safe(_p: str) -> Optional[dict]:
    try:
        if os.path.exists(_p):
            with open(_p, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        return None
    return None

def preload_meta_models_and_sidecars(outputs_dir: Optional[str] = None) -> None:
    global _META_PRELOAD_DONE, _META_FEATURES_CACHE
    if _META_PRELOAD_DONE:
        return
    try:
        out_dir = outputs_dir or os.environ.get("NCAAB_OUTPUTS_DIR") or os.path.join(os.getcwd(), "outputs")
        cover_sidecar = os.path.join(out_dir, "meta_features_cover.json")
        total_sidecar = os.path.join(out_dir, "meta_features_total.json")
        _META_FEATURES_CACHE["cover"] = _read_json_safe(cover_sidecar)
        _META_FEATURES_CACHE["over"] = _read_json_safe(total_sidecar)
    except Exception:
        # Non-fatal: keep app start resilient
        pass
    finally:
        _META_PRELOAD_DONE = True

# Perform preload once when module is imported
preload_meta_models_and_sidecars()
# Load .env for configurable thresholds/settings if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass
from pathlib import Path
from typing import Any, Dict, List

from flask import Flask, render_template, jsonify, request, make_response, redirect
import logging
from flask import send_file
import pandas as pd
import datetime as dt
import numpy as np
from zoneinfo import ZoneInfo
import re
import json as _json

# Ensure src/ is importable
import sys
ROOT = Path(__file__).resolve().parent
SRC = ROOT / "src"
if str(SRC) not in sys.path:
    sys.path.insert(0, str(SRC))

from ncaab_model.config import settings  # type: ignore
from ncaab_model.data.merge_odds import normalize_name  # type: ignore
try:
    # Import CLI routines we can safely invoke programmatically
    from ncaab_model.cli import finalize_day as cli_finalize_day  # type: ignore
    from ncaab_model.cli import daily_run as cli_daily_run  # type: ignore
    import typer  # type: ignore
except Exception:
    cli_finalize_day = None  # type: ignore
    cli_daily_run = None  # type: ignore
    typer = None  # type: ignore

app = Flask(__name__)

# Guardrails and precedence helpers (added)
def _league_total_bounds(df: pd.DataFrame) -> tuple[float, float]:
    try:
        recent = df['market_total'].dropna() if 'market_total' in df.columns else pd.Series([], dtype=float)
        if len(recent) >= 10:
            q10 = float(recent.quantile(0.10))
            q90 = float(recent.quantile(0.90))
            return max(110.0, q10), min(180.0, q90)
    except Exception:
        pass
    return 110.0, 180.0

def apply_total_guardrails(df: pd.DataFrame) -> pd.DataFrame:
    """Clamp projected totals to realistic ranges using market anchors and league bounds.
    Expects `total_proj_raw` and optional `market_total`; writes `total_proj_clamped`.
    """
    if 'total_proj_raw' not in df.columns:
        return df
    low, high = _league_total_bounds(df)
    def clamp_val(x):
        try:
            v = float(x)
        except Exception:
            return None
        return max(low, min(high, v))
    df['total_proj_clamped'] = df['total_proj_raw'].apply(clamp_val)
    return df

# ---------------------------------
# Module-level time helpers (for reuse)
# ---------------------------------
def _ensure_index_row_global(r: dict) -> dict:
    try:
        existing_iso = r.get('start_time_iso')
        if existing_iso and re.search(r'(Z|[+-]\d{2}:?\d{2})$', str(existing_iso)):
            return r
        for cand_key in ['_start_dt','commence_time','start_time']:
            val = r.get(cand_key)
            if not val:
                continue
            s = str(val)
            has_offset = bool(re.search(r'(Z|[+-]\d{2}:?\d{2})$', s))
            if 'Z' in s:
                s = s.replace('Z', '+00:00')
            ts = pd.to_datetime(s, errors='coerce', utc=has_offset)
            if pd.isna(ts):
                continue
            try:
                if getattr(ts, 'tzinfo', None) is None:
                    ts = ts.tz_localize('America/Chicago')
                else:
                    ts = ts.tz_convert('UTC')
            except Exception:
                pass
            try:
                ts_utc = ts.tz_convert('UTC') if getattr(ts, 'tzinfo', None) else ts
            except Exception:
                ts_utc = ts
            r['start_time_iso'] = ts_utc.strftime('%Y-%m-%dT%H:%M:%SZ')
            break
    except Exception:
        pass
    return r

def _apply_site_display_global(r: dict, tz_name: str | None = None) -> dict:
    """Apply display fields (`display_date`, `display_time_str`, `date`) in the chosen site timezone.

    This is the generalized variant of the earlier Central-only helper. It:
    - Prefers `_start_dt` / `start_time_iso` (UTC-aware) as the canonical source
    - Falls back to `start_time_local(_venue)` + tz abbr when necessary
    - Converts the chosen instant into the site display timezone `tz_name`
    - Fills `display_date`, `display_time_str`, `start_time_display`, and normalizes `date`
    """
    try:
        # If the row already carries a finalized display string from a
        # persisted artifact (e.g., predictions_display_<date>.csv), honor
        # that as-is and just back-fill display_date/date when missing.
        existing_disp = str(r.get('start_time_display') or r.get('display_time_str') or '').strip()
        if existing_disp:
            parts = existing_disp.split()
            if len(parts) >= 2:
                date_part = parts[0]
                r['display_date'] = r.get('display_date') or date_part
                r['date'] = r.get('date') or date_part
            r['display_time_str'] = existing_disp
            r['start_time_display'] = existing_disp
            return r

        try:
            tz_name_eff = tz_name or _get_display_tz_name()
        except Exception:
            tz_name_eff = os.getenv("DISPLAY_TZ") or os.getenv("SCHEDULE_TZ") or "America/Chicago"
        try:
            site_tz = ZoneInfo(tz_name_eff)
        except Exception:
            site_tz = dt.timezone.utc

        # 1) Prefer canonical start datetime if already present
        ts = None
        try:
            _sd = r.get('_start_dt')
            if _sd is not None and not (isinstance(_sd, float) and pd.isna(_sd)):
                ts = pd.to_datetime(str(_sd).replace('Z', '+00:00'), errors='coerce', utc=True)
        except Exception:
            ts = None

        # 2) Fall back to ISO if needed
        if ts is None or pd.isna(ts):
            iso = r.get('start_time_iso') or r.get('start_time')
            if iso:
                s = str(iso)
                has_offset = bool(re.search(r'(Z|[+-]\d{2}:?\d{2})$', s))
                s2 = s if 'Z' in s else s.replace('Z', '+00:00')
                try:
                    ts = pd.to_datetime(s2, errors='coerce', utc=has_offset)
                except Exception:
                    ts = None

        # 3) As a last resort, derive from local + abbr
        if ts is None or pd.isna(ts):
            loc = str(r.get('start_time_local') or r.get('start_time_local_venue') or '').strip()
            abbr_raw = (r.get('start_tz_abbr') or r.get('start_tz_abbr_venue'))
            try:
                missing_abbr = (abbr_raw is None) or (isinstance(abbr_raw, float) and pd.isna(abbr_raw)) or (str(abbr_raw).strip() == '') or (str(abbr_raw).strip().lower() == 'nan')
            except Exception:
                missing_abbr = True
            abbr = '' if missing_abbr else str(abbr_raw).strip().upper()
            if loc:
                parts = loc.split()
                if len(parts) >= 2:
                    dstr, tstr = parts[0], parts[1]
                    off_map = {
                        'UTC': 0, 'Z': 0,
                        'HST': -10, 'AKST': -9,
                        'PST': -8, 'PDT': -7,
                        'MST': -7, 'MDT': -6,
                        'CST': -6, 'CDT': -5,
                        'EST': -5, 'EDT': -4,
                    }
                    label_abbr = abbr if abbr else 'UTC'
                    @app.route('/api/health/meta')
                    def api_health_meta():
                        try:
                            cover = _META_FEATURES_CACHE.get("cover") if isinstance(_META_FEATURES_CACHE, dict) else None
                            over = _META_FEATURES_CACHE.get("over") if isinstance(_META_FEATURES_CACHE, dict) else None
                            return jsonify({
                                "preload_done": bool(_META_PRELOAD_DONE),
                                "cover_sidecar_present": bool(cover),
                                "over_sidecar_present": bool(over),
                                "cover_keys": list(cover.keys()) if isinstance(cover, dict) else [],
                                "over_keys": list(over.keys()) if isinstance(over, dict) else [],
                            })
                        except Exception:
                            return jsonify({
                                "preload_done": False,
                                "cover_sidecar_present": False,
                                "over_sidecar_present": False,
                            })

                    # health route mistakenly inserted here previously; kept logic out of flow

                    try:
                        base_dt = pd.to_datetime(f"{dstr} {tstr}", errors='coerce')
                        if pd.notna(base_dt):
                            off = off_map.get(label_abbr, 0)
                            tzinfo = dt.timezone(dt.timedelta(hours=off))
                            aware = base_dt.replace(tzinfo=tzinfo)
                            ts = pd.to_datetime(aware, utc=True)
                    except Exception:
                        ts = None

        if ts is None or pd.isna(ts):
            return r

        # Convert canonical instant to site timezone
        try:
            tz_local = ts.tz_convert(site_tz)
        except Exception:
            tz_local = ts

        disp_date = tz_local.strftime('%Y-%m-%d')
        disp_time = tz_local.strftime('%H:%M')
        try:
            tz_abbr = tz_local.tzname() or ''
        except Exception:
            tz_abbr = ''
        if not tz_abbr:
            tz_abbr = tz_name_eff

        r['display_date'] = disp_date
        r['display_time_str'] = f"{disp_date} {disp_time} {tz_abbr}"
        r['start_time_display'] = r['display_time_str']
        # Normalize anchors used elsewhere
        r['date'] = disp_date
        try:
            # Ensure canonical UTC start stored
            r['_start_dt'] = ts.tz_convert(dt.timezone.utc) if getattr(ts, 'tzinfo', None) else ts
        except Exception:
            r['_start_dt'] = ts
        # Ensure tz abbr field populated for downstream consumers
        try:
            if not r.get('start_tz_abbr'):
                r['start_tz_abbr'] = tz_abbr
        except Exception:
            pass
    except Exception:
        pass
    return r

def enforce_calibrated_first(df: pd.DataFrame) -> pd.DataFrame:
    """Set display fields with calibrated-first precedence when available.
    Outputs `total_proj_display`, `margin_proj_display`, and precedence flags.
    """
    # Total precedence: calibrated -> clamped -> raw
    base_total = None
    if 'total_proj_cal' in df.columns:
        base_total = df['total_proj_cal']
    elif 'total_proj_clamped' in df.columns:
        base_total = df['total_proj_clamped']
    elif 'total_proj_raw' in df.columns:
        base_total = df['total_proj_raw']
    if base_total is not None:
        df['total_proj_display'] = base_total
    # Margin precedence: calibrated -> raw
    if 'margin_proj_cal' in df.columns:
        df['margin_proj_display'] = df['margin_proj_cal']
    elif 'margin_proj_raw' in df.columns:
        df['margin_proj_display'] = df['margin_proj_raw']
    # Precedence flags
    try:
        df['display_precedence_total'] = df.apply(
            lambda r: 'CAL' if pd.notna(r.get('total_proj_cal', None)) else 'CLAMP' if pd.notna(r.get('total_proj_clamped', None)) else 'RAW',
            axis=1
        )
    except Exception:
        pass
    try:
        df['display_precedence_margin'] = df.apply(
            lambda r: 'CAL' if pd.notna(r.get('margin_proj_cal', None)) else 'RAW',
            axis=1
        )
    except Exception:
        pass
    return df

# -----------------------------
# Quantile metrics API endpoint
# -----------------------------
@app.get('/api/quantile-metrics')
def api_quantile_metrics():
    try:
        out_dir = ROOT / 'outputs'
        qpath = out_dir / 'quantile_metrics.csv'
        if not qpath.exists():
            return jsonify({'status':'missing','data':[]})
        m = pd.read_csv(qpath)
        # Optional date filter
        date_q = request.args.get('date')
        if date_q:
            m = m[m['date'].astype(str) == str(date_q)]
        # Keep concise fields
        cols = [c for c in ['date','crps_total','crps_margin','covered_80_total','covered_80_margin'] if c in m.columns]
        data = m[cols].to_dict(orient='records')
        return jsonify({'status':'ok','data':data})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

    # New function to approximate CRPS using three quantiles
def _approx_crps_from_three(y, q10, q50, q90):
    """Approximate CRPS using three quantiles (q10, q50, q90).

    Uses a simple piecewise-linear interpolation between q10–q50 and q50–q90 and
    integrates the pinball (check) loss over a grid of tau values.
    """
    taus = np.linspace(0.05, 0.95, 19)

    def _interp3(a, b, c, t):
        if t <= 0.5:
            m = (b - a) / 0.4
            return a + (t - 0.1) * m
        m = (c - b) / 0.4
        return b + (t - 0.5) * m

    y = np.asarray(y, float)
    q10 = np.asarray(q10, float)
    q50 = np.asarray(q50, float)
    q90 = np.asarray(q90, float)
    losses: list[float] = []
    for t in taus:
        qt = _interp3(q10, q50, q90, t)
        e = y - qt
        losses.append(np.nanmean(np.maximum(t * e, (t - 1.0) * e)))
    return float(2.0 * np.nanmean(losses))

# -----------------------------
# Quantile metrics page
# -----------------------------
@app.get('/quantile-metrics')
def page_quantile_metrics():
    try:
        date_q = request.args.get('date')
        return render_template('quantile_metrics.html', date_q=date_q)
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# -----------------------------
# Drift weekly metrics API + page
# -----------------------------
@app.get('/api/drift-weekly')
def api_drift_weekly():
    try:
        out_dir = ROOT / 'outputs'
        dpath = out_dir / 'drift_summary_weekly.csv'
        if not dpath.exists():
            return jsonify({'status':'missing','data':[]})
        df = pd.read_csv(dpath)
        return jsonify({'status':'ok','data': df.to_dict(orient='records')})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

@app.get('/drift-weekly')
def page_drift_weekly():
    try:
        return render_template('drift_weekly.html')
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# -----------------------------
# Quantile trend weekly API + page
# -----------------------------
@app.get('/api/quantile-trend-weekly')
def api_quantile_trend_weekly():
    try:
        out_dir = ROOT / 'outputs'
        qpath = out_dir / 'quantile_trend_weekly.csv'
        if not qpath.exists():
            return jsonify({'status':'missing','data':[]})
        df = pd.read_csv(qpath)
        # Optional date filter
        date_q = request.args.get('date')
        if date_q:
            df = df[df['date'].astype(str) == str(date_q)]
        return jsonify({'status':'ok','data': df.to_dict(orient='records')})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

@app.get('/quantile-trend-weekly')
def page_quantile_trend_weekly():
    try:
        date_q = request.args.get('date')
        return render_template('quantile_trend_weekly.html', date_q=date_q)
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# -----------------------------
# Stake risk summary API + page
# -----------------------------
@app.get('/api/stake-risk-summary')
def api_stake_risk_summary():
    try:
        out_dir = ROOT / 'outputs'
        spath = out_dir / 'stake_risk_summary.csv'
        # Optional VaR/ES computed from today's stake and quantiles
        try:
            import os
        except Exception:
            os = None
        threshold_es99 = -500.0
        if os:
            try:
                threshold_es99 = float(os.environ.get('RISK_ES99_ALERT_THRESHOLD', threshold_es99))
            except Exception:
                pass
        var95 = None; var99 = None; es95 = None; es99 = None
        try:
            stake_csv = out_dir / 'stake_sheet_calibrated.csv'
            quant_csv = out_dir / 'quantiles_selected.csv'
            if stake_csv.exists() and quant_csv.exists():
                sdf = pd.read_csv(stake_csv)
                qdf = pd.read_csv(quant_csv)
                # Join by game_id and date
                for df in (sdf, qdf):
                    if 'game_id' in df.columns:
                        df['game_id'] = df['game_id'].astype(str).str.replace(r'\.0$', '', regex=True)
                cols = ['date','game_id','stake','bet_kind','line','price']
                sdf = sdf[[c for c in cols if c in sdf.columns]].copy()
                j = pd.merge(sdf, qdf, on=['date','game_id'], how='inner')
                # Approximate PnL distribution per bet using quantiles
                pnls = []
                for _, r in j.iterrows():
                    stake = float(r.get('stake') or 0.0)
                    price = float(r.get('price') or 0.0)
                    kind = str(r.get('bet_kind') or '').lower()
                    # probability from quantiles: over/under/spread implied via margin/total
                    # Simplified: use q50 as central and width as uncertainty
                    if 'total' in kind:
                        q50 = float(r.get('q50_total') or np.nan)
                        q10 = float(r.get('q10_total') or np.nan)
                        q90 = float(r.get('q90_total') or np.nan)
                    else:
                        q50 = float(r.get('q50_margin') or np.nan)
                        q10 = float(r.get('q10_margin') or np.nan)
                        q90 = float(r.get('q90_margin') or np.nan)
                    if not np.isfinite(q50) or not np.isfinite(q10) or not np.isfinite(q90):
                        continue
                    width = max(1e-6, q90 - q10)
                    # crude mapping: win prob increases as central exceeds line for over/spread
                    line = float(r.get('line') or 0.0)
                    if 'over' in kind:
                        edge = (q50 - line) / width
                    elif 'under' in kind:
                        edge = (line - q50) / width
                    elif 'spread' in kind or 'ats' in kind:
                        edge = (abs(q50) - abs(line)) / width
                    else:
                        edge = 0.0
                    p_win = max(0.0, min(1.0, 0.5 + 0.4 * edge))
                    # payoff: American odds price
                    win_ret = stake * (price/100.0) if price > 0 else stake * (100.0/abs(price))
                    lose_ret = -stake
                    # sample PnL from a Bernoulli on quantile grid for portfolio approximation
                    # represent as two-point distribution
                    pnls.append(win_ret)
                    pnls.append(lose_ret)
                if pnls:
                    arr = np.array(pnls, float)
                    arr.sort()
                    def _var(arr, lvl):
                        return float(np.quantile(arr, 1.0 - lvl))
                    def _es(arr, lvl):
                        # expected shortfall beyond VaR
                        cutoff = np.quantile(arr, 1.0 - lvl)
                        tail = arr[arr <= cutoff]
                        return float(np.mean(tail)) if len(tail) else cutoff
                    var95 = _var(arr, 0.95); var99 = _var(arr, 0.99)
                    es95 = _es(arr, 0.95); es99 = _es(arr, 0.99)
        except Exception:
            pass
        if not spath.exists():
            return jsonify({'status':'missing','data':[], 'var': {'var95': var95, 'var99': var99, 'es95': es95, 'es99': es99, 'threshold_es99': threshold_es99}})
        df = pd.read_csv(spath)
        return jsonify({'status':'ok','data': df.to_dict(orient='records'), 'var': {'var95': var95, 'var99': var99, 'es95': es95, 'es99': es99, 'threshold_es99': threshold_es99}})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

@app.get('/stake-risk-summary')
def page_stake_risk_summary():
    try:
        return render_template('stake_risk_summary.html')
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# (Dashboard route exists later as @app.route("/dashboard") with metrics.)

# -----------------------------
# Serve outputs CSVs securely
# -----------------------------
@app.get('/static/outputs/<path:fname>')
def serve_outputs_file(fname: str):
    try:
        base = ROOT / 'outputs'
        # prevent path traversal
        target = (base / fname).resolve()
        if base.resolve() not in target.parents and base.resolve() != target.parent:
            return jsonify({'status':'error','message':'invalid path'})
        if not target.exists():
            return jsonify({'status':'missing'})
        return send_file(str(target), as_attachment=True)
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# -----------------------------
# Aggregated alerts API
# -----------------------------
@app.get('/api/alerts-today')
def api_alerts_today():
    """Return a minimal alerts summary for today.

    This is currently a lightweight stub to avoid syntax errors while keeping
    the route alive. It can be expanded later to aggregate real alert sources.
    """
    try:
        out_dir = ROOT / 'outputs'
        _ = out_dir  # placeholder use to avoid lint about unused
        return jsonify({'status': 'ok', 'data': {}})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

# -----------------------------
# Backtest summary API
# -----------------------------
@app.get('/api/backtest-summary')
def api_backtest_summary():
    try:
        out_dir = ROOT / 'outputs'
        latest = out_dir / 'backtest_summary_latest.csv'
        df = None
        if latest.exists():
            df = pd.read_csv(latest)
        else:
            cand = sorted(out_dir.glob('backtest_summary_*.csv'), key=lambda p: p.stat().st_mtime, reverse=True)
            if cand:
                df = pd.read_csv(cand[0])
        if df is None or df.empty:
            return jsonify({'status':'missing'})
        row = df.iloc[0].to_dict()
        return jsonify({'status':'ok','data': row})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# -----------------------------
# Backtest ROI API
# -----------------------------
@app.get('/api/backtest-roi')
def api_backtest_roi():
    try:
        out_dir = ROOT / 'outputs'
        p = out_dir / 'backtest_roi_latest.csv'
        if not p.exists():
            return jsonify({'status':'missing'})
        df = pd.read_csv(p)
        return jsonify({'status':'ok','data': df.to_dict(orient='records')})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})

# -----------------------------
# Meta reliability API
# -----------------------------
@app.get('/api/meta-reliability')
def api_meta_reliability():
    try:
        out_dir = ROOT / 'outputs'
        p_json = out_dir / 'meta_ece.json'
        p_csv = out_dir / 'meta_reliability.csv'
        data = {}
        if p_json.exists():
            try:
                data = json.loads(p_json.read_text())
            except Exception:
                data = {'status': 'error'}
        rows = []
        if p_csv.exists():
            try:
                rows = pd.read_csv(p_csv).to_dict(orient='records')
            except Exception:
                rows = []
        return jsonify({'status':'ok','summary': data, 'reliability': rows})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})
# Post-process enriched artifact to ensure predictions/time/display coverage
def _postprocess_enriched_file(date_q: str):
    """Post-process the enriched artifact so it carries authoritative
    time/display fields for each game row and then persist a matching
    display snapshot for the cards.

    This is the single canonical place where we:
      - Fill missing predictions from unified/display fallbacks
      - Attach UTC start_time_iso / _start_dt
      - Apply midnight drift vs the slate date
      - Compute display_date/start_time_display in the site/display tz
      - Persist `predictions_display_<date>.csv` from this enriched view
    """
    try:
        out_dir = ROOT / 'outputs'
        enriched_path = out_dir / f'predictions_unified_enriched_{date_q}.csv'
        base_unified = out_dir / f'predictions_unified_{date_q}.csv'
        display_path = out_dir / f'predictions_display_{date_q}.csv'
        if not enriched_path.exists():
            return False
        enr = pd.read_csv(enriched_path)
        uni = pd.read_csv(base_unified) if base_unified.exists() else None
        dis = pd.read_csv(display_path) if display_path.exists() else None
        # Fallback fill for pred_total/pred_margin
        enr = _fallback_merge_predictions(enr, uni, dis)

        # Row-wise time enrichment anchored to the slate date.
        rows: list[dict[str, Any]] = []
        try:
            disp_tz_name = _get_display_tz_name()
        except Exception:
            disp_tz_name = None
        for r0 in enr.to_dict(orient='records'):
            r = dict(r0)
            try:
                if not r.get('start_time_iso'):
                    r['start_time_iso'] = _derive_start_iso(r)
            except Exception:
                pass
            try:
                r = _backfill_start_fields(r)
            except Exception:
                pass
            try:
                r = _correct_midnight_drift(r, slate_date=str(date_q) if date_q else None)
            except Exception:
                pass
            try:
                r = _apply_site_display_global(r, tz_name=str(disp_tz_name) if disp_tz_name else None)
            except Exception:
                pass
            # Ensure `date` and `display_date` are in sync based on
            # the final localized display timestamp.
            try:
                disp_str = str(r.get('start_time_display') or r.get('display_time_str') or '')
                parts = disp_str.split()
                if len(parts) >= 2:
                    disp_date_str = parts[0]
                    r['display_date'] = disp_date_str
                    r['date'] = disp_date_str
            except Exception:
                # Fallback: keep any existing date/display_date
                if 'display_date' not in r and r.get('date'):
                    r['display_date'] = r.get('date')
            rows.append(r)
        enr = pd.DataFrame(rows)
        enr.to_csv(enriched_path, index=False)

        # Persist a matching display snapshot from this enriched frame so
        # the cards can trust `predictions_display_<date>.csv` for
        # local dates/times instead of rebuilding from UTC.
        try:
            _persist_display(enr, date_q)
        except Exception:
            pass
        return True
    except Exception:
        return False

# Fallback + instrumentation helpers
def _is_utc_normalized(dt):
    try:
        if isinstance(dt, str):
            return dt.endswith('Z') or ('+00:00' in dt)
        return isinstance(dt, dt.__class__) and getattr(dt, 'tzinfo', None) is not None and getattr(dt, 'tzinfo') == dt.timezone.utc
    except Exception:
        return False

def _safe_to_utc_iso(val):
    if val is None or val == '':
        return None
    try:
        if isinstance(val, str):
            if val.endswith('Z'):
                return val
            return pd.to_datetime(val.replace('Z','+00:00'), errors='coerce', utc=True).strftime('%Y-%m-%dT%H:%M:%SZ')
        return pd.to_datetime(val, errors='coerce', utc=True).strftime('%Y-%m-%dT%H:%M:%SZ')
    except Exception:
        return val

def _fallback_merge_predictions(enriched_df: pd.DataFrame, unified_df: pd.DataFrame | None, display_df: pd.DataFrame | None) -> pd.DataFrame:
    if enriched_df is None or len(enriched_df) == 0:
        return enriched_df
    def make_map(df):
        if df is None:
            return {}
        cols = [c for c in ['game_id','date','pred_total','pred_margin'] if c in df.columns]
        if not set(['game_id','date']).issubset(set(cols)):
            return {}
        return {(str(r.game_id), str(r.date)):(r.pred_total, r.pred_margin) for r in df[cols].itertuples()}
    u_map = make_map(unified_df)
    d_map = make_map(display_df)
    for i, r in enriched_df.iterrows():
        if (('pred_total' in enriched_df.columns and pd.isna(r.get('pred_total'))) or ('pred_margin' in enriched_df.columns and pd.isna(r.get('pred_margin')))):
            key = (str(r.get('game_id')), str(r.get('date')))
            vals = u_map.get(key) or d_map.get(key)
            if vals:
                enriched_df.at[i, 'pred_total'] = vals[0]
                enriched_df.at[i, 'pred_margin'] = vals[1]
    return enriched_df

def _log_enrichment_drop(row: dict, drops_log: list):
    reasons = []
    if row.get('pred_total') is None and row.get('pred_margin') is None:
        reasons.append('missing_predictions')
    if not row.get('start_time_iso'):
        reasons.append('missing_start_time_iso')
    if not row.get('display_date'):
        reasons.append('missing_display_date')
    if (row.get('home_odds_decimal') is None) and (row.get('away_odds_decimal') is None):
        reasons.append('missing_odds')
    if reasons:
        drops_log.append({
            'game_id': row.get('game_id'),
            'date': row.get('date'),
            'home_team': row.get('home_team'),
            'away_team': row.get('away_team'),
            'reasons': '|'.join(reasons)
        })

# ------------------------------------------------------------------
# Feature fallback enrichment stub (prevents undefined symbol errors)
# ------------------------------------------------------------------
def _feature_fallback_enrich(df: pd.DataFrame) -> pd.DataFrame:
    """Lightweight enrichment placeholder.

    Intentionally minimal: returns DataFrame unchanged. Can be expanded to
    impute missing feature columns (off/def/tempo) if needed for fallback
    prediction derivations. Exists to satisfy references where a richer
    feature engineering stage may later populate derived fields.
    """
    try:
        if df is None or df.empty:
            return df
        # Example: ensure unique game_id rows if duplicates present
        if 'game_id' in df.columns:
            try:
                df = df.drop_duplicates(subset=['game_id'])
            except Exception:
                pass
    except Exception:
        return df
    return df

# --------------------------------------------------------------
# Midnight UTC drift correction
# --------------------------------------------------------------
_TZ_ABBR_MAP = {
    'UTC': 0, 'Z': 0,
    'HST': -10, 'AKST': -9,
    'PST': -8, 'PDT': -7,
    'MST': -7, 'MDT': -6,
    'CST': -6, 'CDT': -5,
    'EST': -5, 'EDT': -4,
}

def _correct_midnight_drift(row: dict, slate_date: str | None = None) -> dict:
    """Correct games whose UTC ISO crosses midnight but should belong to previous local day.

    Logic:
    1. If we have start_time_local + start_tz_abbr, parse them to a local datetime.
    2. Convert that local datetime to UTC and build canonical ISO.
    3. Compare date portion of existing ISO (if any) vs derived local->UTC ISO.
       If existing ISO's UTC date is +1 relative to local-derived UTC date (common midnight drift), override.
    4. Optionally validate against `slate_date` (requested date) – only adjust if local date matches slate_date.
    5. Update start_time_iso and start_time_display (preserve existing display if already matches local).
    """
    try:
        loc = str(row.get('start_time_local') or '').strip()
        abbr = str(row.get('start_tz_abbr') or '').upper().strip()
        if not loc or not abbr or abbr not in _TZ_ABBR_MAP:
            return row
        parts = loc.split()
        if len(parts) < 2:
            return row
        local_date_str, local_time_str = parts[0], parts[1]
        # Validate slate date match if provided
        if slate_date and local_date_str != slate_date:
            return row
        off_hours = _TZ_ABBR_MAP[abbr]
        # Build naive datetime then assign offset
        try:
            local_dt = dt.datetime.strptime(f"{local_date_str} {local_time_str}", "%Y-%m-%d %H:%M")
        except Exception:
            return row
        # Create timezone info from offset hours
        try:
            tzinfo = dt.timezone(dt.timedelta(hours=off_hours))
            local_aware = local_dt.replace(tzinfo=tzinfo)
            utc_dt = local_aware.astimezone(dt.timezone.utc)
        except Exception:
            return row
        derived_iso = utc_dt.strftime('%Y-%m-%dT%H:%M:%SZ')
        existing_iso = str(row.get('start_time_iso') or '').strip()
        if existing_iso:
            # Parse existing for comparison
            try:
                ex_dt = pd.to_datetime(existing_iso, errors='coerce', utc=True)
            except Exception:
                ex_dt = pd.NaT
            if pd.notna(ex_dt):

                # If difference in day and existing is +1 day vs derived, correct
                if ex_dt.date() != utc_dt.date() and ex_dt.date() == (utc_dt.date() + dt.timedelta(days=1)):
                    row['start_time_iso'] = derived_iso
                    # Adjust display if present and mismatched
                    disp = str(row.get('start_time_display') or '')
                    if disp:
                        # Replace date portion with intended local date
                        try:
                            row['start_time_display'] = f"{local_date_str} {local_time_str} {abbr}".strip()
                        except Exception:
                            pass
                    return row
                # Guard against accidental +300 minute drift when raw UTC provided
                raw = str(row.get('start_time') or '').strip()
                if raw:
                    try:
                        raw_dt = pd.to_datetime(raw, errors='coerce', utc=True)
                        delta = (ex_dt - raw_dt).total_seconds()
                        if delta == 300 * 60:
                            row['start_time_iso'] = raw_dt.strftime('%Y-%m-%dT%H:%M:%SZ')
                            return row
                    except Exception:
                        pass
                return row
        # No existing ISO or unparsable – set derived
        row['start_time_iso'] = derived_iso
        if not row.get('start_time_display'):
            row['start_time_display'] = f"{local_date_str} {local_time_str} {abbr}".strip()
    except Exception:
        return row
    return row

# -----------------------------------------------------------------------------
# Time derivation helper: robust tz-aware ISO for post-UTC-midnight games
# -----------------------------------------------------------------------------
def _derive_start_iso(row: dict[str, Any]) -> str | None:
    """Derive a tz-aware ISO-UTC string for a game's start time.

    Preference order (most reliable first):
    1) (`start_time_local`, `start_tz_abbr`) → local with abbr→offset map, then to UTC Z
    2) `_start_dt` (tz-aware) → UTC Z
    3) `commence_time` (parse with utc=True) → UTC Z
    4) `start_time` (naive assumed UTC; accept Z or add +00:00) → UTC Z

    Handles late CT/HST games crossing 00:00 UTC by anchoring to provided local time when available.
    """
    try:
        loc = row.get('start_time_local')
        abbr = (row.get('start_tz_abbr') or '').upper()
        if loc:
            # Expect loc like 'YYYY-MM-DD HH:MM'
            parts = str(loc).split(' ')
            if len(parts) >= 2:
                date, time = parts[0], parts[1]
                tz_map = {
                    'UTC': 0, 'Z': 0,
                    'HST': -10, 'AKST': -9,
                    'PST': -8, 'PDT': -7,
                    'MST': -7, 'MDT': -6,
                    'CST': -6, 'CDT': -5,
                    'EST': -5, 'EDT': -4,
                }
                off = tz_map.get(abbr, None)
                if off is not None:
                    # Build offset ISO then normalize to UTC
                    iso_local = f"{date}T{time}:00" + ("Z" if off == 0 else ("+" if off > 0 else "-") + str(abs(off)).rjust(2, '0') + ":00")
                    try:
                        d = pd.to_datetime(iso_local, errors='coerce', utc=True)
                        if pd.notna(d):
                            return d.strftime('%Y-%m-%dT%H:%M:%SZ')
                    except Exception:
                        pass
        # Fallbacks
        _start = pd.to_datetime(row.get('_start_dt'), errors='coerce')
        if pd.notna(_start):
            try:
                d = _start.tz_convert('UTC')
                return d.strftime('%Y-%m-%dT%H:%M:%SZ')
            except Exception:
                pass
        comm = pd.to_datetime(row.get('commence_time'), errors='coerce', utc=True)
        if pd.notna(comm):
            return comm.strftime('%Y-%m-%dT%H:%M:%SZ')
        st = row.get('start_time')
        if st:
            # Treat as UTC if naive
            try:
                d = pd.to_datetime(str(st).replace('Z','+00:00'), errors='coerce', utc=True)
                if pd.notna(d):
                    return d.strftime('%Y-%m-%dT%H:%M:%SZ')
            except Exception:
                pass
    except Exception:
        pass

    return None

# -----------------------------------------------------------------------------
# Backfill & normalization helper for late UTC rollover games
# -----------------------------------------------------------------------------
def _backfill_start_fields(r: dict[str, Any]) -> dict[str, Any]:
    """Populate missing start_time_iso/start_time_local/start_tz_abbr and display_date.

    Handles evening games whose UTC timestamp rolls into slate+1 but belong to the
    original slate's local calendar date. This occurs for late Central / Mountain /
    Pacific / Hawaii starts represented only by a raw UTC `start_time`.

    Rules:
      - If `start_time_iso` is missing but `start_time` present, derive ISO (UTC Z).
      - If local fields missing, attempt Eastern localization (schedule anchor) then
        fallback to UTC abbreviation.
      - If UTC date == slate_date + 1 AND localized (Eastern) date == slate_date,
        set `display_date` to slate_date (normalization for rollover) without
        mutating original date column.
      - Never override existing populated fields.
      - Prefer venue-local fields when available on enriched artifacts.
    """

    slate_raw = r.get('_slate_date') or r.get('date') or r.get('slate_date')
    slate_dt = None
    if slate_raw:
        try:
            slate_dt = pd.to_datetime(str(slate_raw), errors='coerce').date()
        except Exception:
            slate_dt = None
    # Parse UTC source
    utc_source = None
    iso_val = r.get('start_time_iso')
    if iso_val:
        try:
            utc_source = pd.to_datetime(str(iso_val), errors='coerce', utc=True)
        except Exception:
            utc_source = None
    if utc_source is None:
        st_raw = r.get('start_time')
        if st_raw:
            try:
                utc_source = pd.to_datetime(str(st_raw).replace('Z','+00:00'), errors='coerce', utc=True)
            except Exception:
                utc_source = None
    if utc_source is not None and pd.notna(utc_source):
        if not iso_val:
            r['start_time_iso'] = utc_source.strftime('%Y-%m-%dT%H:%M:%SZ')
        if not r.get('start_time_local') or not r.get('start_tz_abbr'):
            try:
                eastern_dt = utc_source.astimezone(ZoneInfo('America/New_York'))
                r.setdefault('start_time_local', eastern_dt.strftime('%Y-%m-%d %H:%M'))
                r.setdefault('start_tz_abbr', eastern_dt.tzname())
            except Exception:
                try:
                    r.setdefault('start_time_local', utc_source.strftime('%Y-%m-%d %H:%M'))
                    r.setdefault('start_tz_abbr', 'UTC')
                except Exception:
                    pass
        # Prefer venue-local if present on input row
        try:
            vloc = r.get('start_time_local_venue')
            vtz = r.get('start_tz_abbr_venue')
            if vloc and vtz:
                r['start_time_local'] = vloc
                r['start_tz_abbr'] = vtz
        except Exception:
            pass
        # Default abbr to CST for display pipeline when missing
        try:
            if not r.get('start_tz_abbr'):
                r['start_tz_abbr'] = 'CST'
        except Exception:
            pass
        # Persist stable UTC start datetime for downstream consumers
        try:
            if not r.get('_start_dt'):
                r['_start_dt'] = utc_source
        except Exception:
            pass
        if slate_dt is not None:
            try:
                if utc_source.date() == (slate_dt + dt.timedelta(days=1)):
                    eastern_dt2 = utc_source.astimezone(ZoneInfo('America/New_York'))
                    if eastern_dt2.date() == slate_dt:
                        r['display_date'] = slate_dt.strftime('%Y-%m-%d')
            except Exception:
                pass
    # Final fallback: set display_date to slate date when still missing
    try:
        if slate_dt is not None and not r.get('display_date'):
            r['display_date'] = slate_dt.strftime('%Y-%m-%d')
    except Exception:
        pass
    # Ultimate fallback: if `_start_dt` still missing but local+abbr present, derive UTC
    try:
        if not r.get('_start_dt'):
            loc = str(r.get('start_time_local') or '').strip()
            abbr = str(r.get('start_tz_abbr') or '').upper().strip()
            if loc and abbr and abbr in _TZ_ABBR_MAP:
                parts = loc.split()
                if len(parts) >= 2:
                    dstr, tstr = parts[0], parts[1]
                    off = _TZ_ABBR_MAP.get(abbr)
                    if off is not None:
                        try:
                            local_dt = dt.datetime.strptime(f"{dstr} {tstr}", "%Y-%m-%d %H:%M")
                            tzinfo = dt.timezone(dt.timedelta(hours=off))
                            aware = local_dt.replace(tzinfo=tzinfo)
                            r['_start_dt'] = pd.to_datetime(aware, utc=True)
                        except Exception:
                            pass
    except Exception:
        pass
    return r


@app.get('/api/debug-times')
def api_debug_times():
    """Debug endpoint: list start times and Central display times for a date.

    Query params:
      - date: YYYY-MM-DD (required)
    """
    try:
        date_q = request.args.get("date")
        if not date_q:
            return jsonify({"status": "error", "message": "missing date"}), 400
        gpath = ROOT / "outputs" / f"games_{date_q}.csv"
        if not gpath.exists():
            return jsonify(
                {
                    "status": "missing",
                    "message": f"games_{date_q}.csv not found",
                    "rows": [],
                }
            )
        df = pd.read_csv(gpath)
        if df.empty or "game_id" not in df.columns:
            return jsonify({"status": "empty", "rows": []})
        df["game_id"] = df["game_id"].astype(str)
        # Apply central-display helper row-wise
        rows = []
        for row in df.to_dict("records"):
            r2 = _apply_site_display_global(row)
            rows.append(
                {
                    "game_id": r2.get("game_id"),
                    "home_team": r2.get("home_team"),
                    "away_team": r2.get("away_team"),
                    "start_time_utc": r2.get("start_time") or r2.get("_start_dt"),
                    "start_time_local": r2.get("start_time_local"),
                    "start_tz_abbr": r2.get("start_tz_abbr"),
                    "display_time_str": r2.get("display_time_str"),
                    "display_date": r2.get("display_date"),
                }
            )
        return jsonify({"status": "ok", "date": date_q, "rows": rows})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

@app.route("/benchmarks")
def benchmarks_page():
    """Simple benchmarks dashboard using backtest artifacts with a fallback.

    Displays backtest daily rows and summary if present; otherwise aggregates
    minimal ATS daily metrics from results_*.csv.
    """
    out: Dict[str, Any] = {"summary": None, "daily_rows": [], "note": None}
    try:
        bt_sum = ROOT / "outputs" / "backtest_summary.json"
        bt_csv = ROOT / "outputs" / "backtest_daily.csv"
        if bt_sum.exists():
            with open(bt_sum, "r", encoding="utf-8") as f:
                out["summary"] = json.load(f)
        if bt_csv.exists():
            df = pd.read_csv(bt_csv)
            out["daily_rows"] = df.to_dict(orient="records")
        if not out["daily_rows"]:
            actuals = []
            outputs = ROOT / "outputs"
            for p in outputs.iterdir():
                if p.name.startswith("results_") and p.name.endswith(".csv"):
                    try:
                        actuals.append(pd.read_csv(p))
                    except Exception:
                        pass
            if actuals:
                df = pd.concat(actuals, ignore_index=True)
                if "date" in df.columns:
                    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                rows = []
                if "ats_result" in df.columns:
                    for iso, grp in df.groupby("date"):
                        ats_text = grp["ats_result"].fillna("")
                        ats_push = int((ats_text == "Push").sum())
                        ats_bets = int((ats_text != "").sum())
                        ats_hits = int(ats_text.str.contains("Cover", case=False).sum())
                        rows.append({
                            "date": iso,
                            "ats_bets": ats_bets,
                            "ats_hits": ats_hits,
                            "ats_push": ats_push,
                        })
                out["daily_rows"] = rows
                out["note"] = "Fallback metrics; run backtest-walkforward for full benchmarks."
    except Exception as e:
        out["note"] = f"Error loading benchmarks: {e}"
    return render_template("benchmarks.html", **out)

def apply_odds_backfill(df: pd.DataFrame) -> pd.DataFrame:
    """Backfill missing odds using available columns and simple mappings.
    - Fill `market_total` from `closing_total` when missing.
    - Fill `spread_home` from `closing_spread_home` when missing.
    - If duplicates per `game_id` exist with odds on some rows, propagate within the game.
    - As a fallback, if `game_id` missing but teams present, group by unordered team-pair.
    """
    if df.empty:
        return df
    try:
        # 1. Normalize provider team names using alias map to improve joins/backfill
        try:
            alias_path = ROOT / 'data' / 'provider_aliases.csv'
            if alias_path.exists():
                alias_df = pd.read_csv(alias_path)
                if {'from','to'}.issubset(alias_df.columns):
                    amap = {str(r['from']).strip().lower(): str(r['to']).strip() for _, r in alias_df.iterrows()}
                    def _norm_with_alias(x: Any) -> str:
                        s = str(x or '').strip()
                        key = s.lower()
                        return amap.get(key, s)
                    for col in ('home_team','away_team'):
                        if col in df.columns:
                            df[col] = df[col].map(_norm_with_alias)
        except Exception:
            pass
        # 2. Build canonical unordered pair key for propagation & late join attempts
        if {'home_team','away_team'}.issubset(df.columns):
            try:
                def _pair_key(a: str, b: str) -> str:
                    return '::'.join(sorted([_canon_slug(str(a)), _canon_slug(str(b))]))
                df['_pair_key'] = [
                    _pair_key(ht, at) for ht, at in zip(df['home_team'].astype(str), df['away_team'].astype(str))
                ]
            except Exception:
                df['_pair_key'] = None
        # 3. Parse datetime helpers (start_time, commence_time) for time-window matching
        # We allow a generous window (±3h, fallback ±5h) to bridge midnight UTC rollover & minute offsets.
        try:
            if 'start_time' in df.columns:
                df['_start_dt'] = pd.to_datetime(df['start_time'], errors='coerce', utc=True)
            if 'commence_time' in df.columns:
                df['_commence_dt'] = pd.to_datetime(df['commence_time'], errors='coerce', utc=True)
        except Exception:
            pass
        if {'market_total','closing_total'}.issubset(df.columns):
            mt = pd.to_numeric(df['market_total'], errors='coerce')
            ct = pd.to_numeric(df['closing_total'], errors='coerce')
            fill_mask = mt.isna() & ct.notna()
            if fill_mask.any():
                df.loc[fill_mask, 'market_total'] = ct[fill_mask]
        if {'spread_home','closing_spread_home'}.issubset(df.columns):
            sh = pd.to_numeric(df['spread_home'], errors='coerce')
            csh = pd.to_numeric(df['closing_spread_home'], errors='coerce')
            fill_mask = sh.isna() & csh.notna()
            if fill_mask.any():
                df.loc[fill_mask, 'spread_home'] = csh[fill_mask]
        # 4. Propagate within game_id (exact join already achieved elsewhere)
        if 'game_id' in df.columns:
            for col in ['market_total','closing_total','spread_home','closing_spread_home','ml_home','ml_away']:
                if col in df.columns:
                    df[col] = df.groupby(df['game_id'].astype(str))[col].transform(lambda s: s.bfill().ffill())
        else:
            # Fallback by unordered team pair
            if {'home_team','away_team'}.issubset(df.columns):
                pair = df['home_team'].astype(str) + '|' + df['away_team'].astype(str)
                rev = df['away_team'].astype(str) + '|' + df['home_team'].astype(str)
                key = pair.where(pair <= rev, rev)  # simple unordered key
                for col in ['market_total','closing_total','spread_home','closing_spread_home','ml_home','ml_away']:
                    if col in df.columns:
                        df[col] = df.groupby(key)[col].transform(lambda s: s.bfill().ffill())
        # 5. Late join propagation: for rows still missing odds but sharing _pair_key with an odds row.
        try:
            if '_pair_key' in df.columns:
                odds_cols = ['market_total','closing_total','spread_home','closing_spread_home','ml_home','ml_away']
                # Identify source odds rows (any populated odds column)
                def _has_any_odds(r: pd.Series) -> bool:
                    for c in odds_cols:
                        if c in r.index:
                            v = r.get(c)
                            if v is not None and (not pd.isna(v)):
                                return True
                    return False
                odds_rows = df[df.apply(_has_any_odds, axis=1)]
                if not odds_rows.empty:
                    # Map pair_key -> list of candidate odds rows
                    by_pair: dict[str, list[pd.Series]] = {}
                    for _, r in odds_rows.iterrows():
                        pk = r.get('_pair_key')
                        if not pk:
                            continue
                        by_pair.setdefault(str(pk), []).append(r)
                    # For each target row missing odds, attempt propagation from closest time match
                    window_primary = pd.Timedelta(hours=3)
                    window_fallback = pd.Timedelta(hours=5)
                    for idx, r in df.iterrows():
                        pk = r.get('_pair_key')
                        if not pk or pk not in by_pair:
                            continue
                        # Skip if already has odds
                        if _has_any_odds(r):
                            continue
                        candidates = by_pair[str(pk)]
                        # Time-based selection if possible
                        best = None
                        best_delta = None
                        st = r.get('_start_dt')
                        for cand in candidates:
                            ct = cand.get('_commence_dt') or cand.get('_start_dt')
                            if st is not None and pd.notna(st) and ct is not None and pd.notna(ct):
                                delta = abs(st - ct)
                                if (best is None) or (delta < best_delta):
                                    best = cand
                                    best_delta = delta
                            else:
                                # No time info; just take first
                                best = cand
                                best_delta = None
                        # Enforce window thresholds if we had a delta
                        if best is not None and (best_delta is None or best_delta <= window_primary or best_delta <= window_fallback):
                            for c in odds_cols:
                                if c in df.columns:
                                    val = best.get(c)
                                    if (r.get(c) is None or pd.isna(r.get(c))) and val is not None and (not pd.isna(val)):
                                        df.at[idx, c] = val
                            # Also propagate commence_time if missing
                            if 'commence_time' in df.columns and (not r.get('commence_time')):
                                df.at[idx, 'commence_time'] = best.get('commence_time')
        except Exception:
            pass
    except Exception:
        pass
    return df
# Basic logging setup (Render captures stdout/stderr)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ncaab_app")

# Lightweight auth token for ingestion endpoints (set NCAAB_INGEST_TOKEN env var).
_INGEST_TOKEN = os.getenv("NCAAB_INGEST_TOKEN", "").strip()

# ONNX Runtime provider diagnostics (one-time at startup)
# Avoid probing providers on platforms where ORT may crash (e.g., some Render Linux images).
_DISABLE_ORT_STARTUP = str(os.environ.get("NCAAB_DISABLE_ORT_STARTUP", "")).lower() in ("1", "true", "yes", "on") or \
                       str(os.environ.get("RENDER", "")).lower() in ("1", "true", "yes", "on")
if _DISABLE_ORT_STARTUP:
    logger.info("Skipping ONNX Runtime provider probing at startup (disabled via env)")
    _ORT_PROVIDERS = []
else:
    try:
        import onnxruntime as ort  # type: ignore
        # Optional hardening: force CPU-only providers and lazy import behavior on Render
        _ORT_FORCE_CPU = str(os.environ.get("NCAAB_ORT_FORCE_CPU", "")).lower() in ("1", "true", "yes", "on") or \
                         str(os.environ.get("RENDER", "")).lower() in ("1", "true", "yes", "on")
        if _ORT_FORCE_CPU:
            try:
                _orig_InferenceSession = ort.InferenceSession
                def _cpu_only_InferenceSession(*args, **kwargs):
                    # Enforce CPU EP if no providers specified
                    providers = kwargs.get('providers')
                    if not providers:
                        kwargs['providers'] = ['CPUExecutionProvider']
                    return _orig_InferenceSession(*args, **kwargs)
                ort.InferenceSession = _cpu_only_InferenceSession  # type: ignore
                logger.info("ONNX Runtime hardened: forcing CPUExecutionProvider for sessions")
            except Exception as _mp_e:
                logger.info("Failed to harden ORT sessions to CPU-only (%s)", _mp_e)
        _ORT_PROVIDERS = ort.get_available_providers()
        logger.info("ONNX Runtime providers available at startup: %s", _ORT_PROVIDERS)
        _dll_dir = os.environ.get("NCAAB_ORT_DLL_DIR")
        if _dll_dir:
            logger.info("NCAAB_ORT_DLL_DIR=%s", _dll_dir)
        _qnn_root = os.environ.get("QNN_SDK_ROOT") or os.environ.get("NCAAB_QNN_SDK_DIR")
        if _qnn_root:
            logger.info("QNN SDK root detected: %s", _qnn_root)
    except Exception as _e:
        logger.info("ONNX Runtime not available; predictions will use numpy fallback (%s)", _e)

# Sanity check route to verify API wiring
@app.route("/api/debug-slate2")
def api_debug_slate2():
    try:
        return jsonify({"ok": True, "message": "debug-slate2 route is active"})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)})

    # --------------------------------------------------------------------------------------
    # Debug endpoint: per-slate date/time source inspection
    # --------------------------------------------------------------------------------------
    @app.route("/api/debug-slate")
    def api_debug_slate():
        """Return per-row inspection of date/time fields and source precedence for a slate.

        Query params:
        - date: YYYY-MM-DD (optional; defaults to schedule tz 'today')
        """
        try:
            date_q = request.args.get("date")
        except Exception:
            date_q = None
        try:
            target_date = date_q or _today_local().strftime('%Y-%m-%d')
        except Exception:
            target_date = None
        rows = []
        try:
            OUT = ROOT / "outputs"
            candidates = []
            # Prefer fused/with_odds slates for richer fields
            if target_date:
                for name in [
                    f"games_with_odds_{target_date}.csv",
                    f"games_{target_date}_fused.csv",
                    f"games_{target_date}.csv",
                ]:
                    p = OUT / name
                    if p.exists():
                        try:
                            candidates.append(pd.read_csv(p))
                        except Exception:
                            pass
            # Fallback to current games
            p_curr = OUT / "games_curr.csv"
            if p_curr.exists():
                try:
                    dfc = pd.read_csv(p_curr)
                    candidates.append(dfc)
                except Exception:
                    pass
            if not candidates:
                return jsonify({"ok": False, "error": "No slate files found", "date": target_date, "rows": []})
            try:
                df_all = pd.concat(candidates, ignore_index=True)
            except Exception:
                df_all = candidates[0]
            # Filter to target date using schedule tz if possible
            try:
                tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                sched_tz = ZoneInfo(tz_name)
            except Exception:
                sched_tz = None
            if target_date:
                try:
                    date_col = pd.to_datetime(df_all.get('date'), errors='coerce').dt.strftime('%Y-%m-%d') if 'date' in df_all.columns else None
                    ct = pd.to_datetime(df_all.get('commence_time', pd.Series([None]*len(df_all))).astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                    ct_loc = ct.dt.tz_convert(sched_tz).dt.strftime('%Y-%m-%d') if sched_tz is not None else None
                    mask = None
                    if date_col is not None:
                        mask = (date_col == target_date)
                    if ct_loc is not None:
                        mask = ct_loc == target_date if mask is None else (mask | (ct_loc == target_date))
                    if mask is not None:
                        df_all = df_all[mask]
                except Exception:
                    pass
            # Build computed fields and source indicators
            def _src_for_row(r: dict[str, Any]) -> str:
                try:
                    # Prefer local fields
                    if r.get('start_time_local') and r.get('start_tz_abbr'):
                        return 'local+abbr'
                    # Next, explicit _start_dt
                    if r.get('_start_dt'):
                        return '_start_dt'
                    # Next, commence_time
                    if r.get('commence_time'):
                        txt = str(r.get('commence_time'))
                        return 'commence_time(offset)' if (txt.endswith('Z') or bool(re.search(r"[+-]\d{2}:\d{2}$", txt))) else 'commence_time(naive)'
                    # Finally, start_time
                    if r.get('start_time'):
                        txt = str(r.get('start_time'))
                        return 'start_time(offset)' if (txt.endswith('Z') or bool(re.search(r"[+-]\d{2}:\d{2}$", txt))) else 'start_time(naive)'
                except Exception:
                    pass
                return 'unknown'
            # Compute fields
            df_all = df_all.copy()
            df_all['_start_dt_parsed'] = pd.NaT
            try:
                # Use our helper precedence
                iso_vals = df_all.to_dict(orient='records')
                start_iso = []
                for r in iso_vals:
                    start_iso.append(_derive_start_iso(r))
                df_all['start_time_iso_canonical'] = start_iso
                df_all['_start_dt_parsed'] = pd.to_datetime(pd.Series(start_iso).astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
            except Exception:
                pass
            try:
                disp_tz_name = _get_display_tz_name()
                disp_tz = ZoneInfo(disp_tz_name)
            except Exception:
                disp_tz = dt.datetime.now().astimezone().tzinfo
            try:
                disp = pd.to_datetime(df_all['_start_dt_parsed'], errors='coerce').dt.tz_convert(disp_tz)
                df_all['start_time_display_dbg'] = disp.dt.strftime('%Y-%m-%d %H:%M')
                df_all['start_tz_abbr_dbg'] = disp.dt.tzname()
            except Exception:
                df_all['start_time_display_dbg'] = df_all.get('start_time')
                df_all['start_tz_abbr_dbg'] = None
            # Build output rows
            cols = ['game_id','home_team','away_team','venue','date','start_time','commence_time','start_time_local','start_tz_abbr','start_time_iso','start_time_iso_canonical','start_time_display_dbg','start_tz_abbr_dbg']
            for _, rr in df_all.iterrows():
                row = {c: (rr.get(c) if c in rr else None) for c in cols}
                row['source'] = _src_for_row(rr)
                try:
                    row['_start_dt_parsed'] = rr.get('_start_dt_parsed').isoformat() if pd.notna(rr.get('_start_dt_parsed')) else None
                except Exception:
                    row['_start_dt_parsed'] = None
                rows.append(row)
            return jsonify({"ok": True, "date": target_date, "count": len(rows), "rows": rows, "display_tz": str(disp_tz)})
        except Exception as e:
            return jsonify({"ok": False, "error": str(e)})

# --------------------------------------------------------------------------------------
# New provider diagnostics + micro-benchmark endpoints
# --------------------------------------------------------------------------------------
@app.route("/api/ort-providers")
def api_ort_providers():
    """Return available and preferred provider ordering along with any loaded model session info.

    If ONNX Runtime isn't importable returns an empty providers list.
    """
    try:
        import onnxruntime as ort  # type: ignore
        available = list(ort.get_available_providers())
    except Exception:
        available = []
    # Attempt to instantiate a lightweight session on a test model if present to see final provider order
    session_providers: list[str] = []
    test_models = [
        ROOT / "mlp_megatron_basic_test.onnx",
        ROOT / "mlp_megatron_basic_test.onnx",
        ROOT / "bart_mlp_megatron_basic_test.onnx",
    ]
    for m in test_models:
        if m.exists():
            try:
                # Prefer GPU providers first if available
                preferred = [p for p in ["QNNExecutionProvider","DmlExecutionProvider","CPUExecutionProvider"] if p in available]
                import onnxruntime as ort  # type: ignore
                sess = ort.InferenceSession(str(m), providers=preferred)
                session_providers = list(sess.get_providers())
                break
            except Exception:
                continue
    return jsonify({
        "available_providers": available,
        "session_providers": session_providers,
        "dll_dir": os.environ.get("NCAAB_ORT_DLL_DIR"),
        "qnn_sdk_root": os.environ.get("QNN_SDK_ROOT") or os.environ.get("NCAAB_QNN_SDK_DIR"),
    })


@app.route("/api/ort-benchmark")
def api_ort_benchmark():
    """Lightweight latency benchmark on a small ONNX test model (warmups + timed)."""
    try:
        import onnxruntime as ort  # type: ignore
    except Exception:
        return jsonify({"error": "onnxruntime not installed"}), 400
    test_models = [
        ROOT / "mlp_megatron_basic_test.onnx",
        ROOT / "bart_mlp_megatron_basic_test.onnx",
        ROOT / "self_attention_megatron_basic_test.onnx",
    ]
    model_path = next((p for p in test_models if p.exists()), None)
    if not model_path:
        return jsonify({"error": "no test model found"}), 404
    providers = list(ort.get_available_providers())
    preferred = [p for p in ["QNNExecutionProvider","DmlExecutionProvider","CPUExecutionProvider"] if p in providers]
    try:
        sess = ort.InferenceSession(str(model_path), providers=preferred)
    except Exception as e:
        return jsonify({"error": str(e), "providers": providers}), 500
    import numpy as _np
    x = _np.random.randn(1, 32).astype(_np.float32)
    # Warmup
    for _ in range(8):
        try:
            sess.run(None, {sess.get_inputs()[0].name: x})
        except Exception:
            pass
    import time
    times: list[float] = []
    for _ in range(24):
        t0 = time.time()
        sess.run(None, {sess.get_inputs()[0].name: x})
        times.append((time.time() - t0) * 1000.0)
    payload = {
        "model": model_path.name,
        "avg_ms": round(sum(times) / len(times), 4) if times else None,
        "providers": providers,
        "session_providers": sess.get_providers(),
        "n_runs": len(times),
    }
    return jsonify(payload)


# Coverage diagnostics + summary API (merged)
@app.route("/api/coverage-today")
def api_coverage_today():
    """Unified coverage endpoint.

    Returns both diagnostic fields (schedule vs display counts, blanks, odds
    missing metrics, quality heuristic) and summary fields (status_counts,
    non_placeholder_games, per-game rows with coverage_status). Also includes
    legacy report/missing team rows if present plus sidecar summary JSON.
    Optional query param: ?date=YYYY-MM-DD
    """
    import os, glob, json
    from datetime import datetime
    import pandas as pd
    from flask import request, jsonify

    today_str = datetime.utcnow().strftime('%Y-%m-%d')
    date_q = request.args.get('date')
    target_date = (date_q.strip() if isinstance(date_q, str) and date_q and date_q.strip() else today_str)
    OUT = os.path.join(os.getcwd(), 'outputs')

    # Schedule authoritative ids
    games_path = os.path.join(OUT, 'games_curr.csv')
    games_df = pd.read_csv(games_path) if os.path.exists(games_path) else pd.DataFrame()
    if not games_df.empty and 'date' in games_df.columns:
        try:
            games_df['date'] = pd.to_datetime(games_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
            games_df = games_df[games_df['date'] == target_date]
        except Exception:
            pass
    ids_sched = set(games_df['game_id'].astype(str)) if not games_df.empty and 'game_id' in games_df.columns else set()

    # Candidate display artifacts (prefer enriched)
    # Candidate artifacts in priority order (we will re-rank by coverage below)
    cand = [
        os.path.join(OUT, f'predictions_unified_enriched_{target_date}.csv'),
        os.path.join(OUT, f'predictions_unified_enriched_{target_date}_force_fill.csv'),
        os.path.join(OUT, f'predictions_unified_{target_date}.csv'),
        os.path.join(OUT, f'predictions_model_{target_date}.csv'),
        os.path.join(OUT, f'align_period_{target_date}.csv'),
    ]
    # Load all existing candidates, compute schedule coverage, pick best (max scheduled id intersection)
    artifact_frames = []
    for p in cand:
        if not os.path.exists(p):
            continue
        try:
            tmp = pd.read_csv(p)
        except Exception:
            continue
        if 'game_id' in tmp.columns:
            try:
                tmp['game_id'] = tmp['game_id'].astype(str)
            except Exception:
                pass
        cov = len(ids_sched.intersection(set(tmp['game_id'])) ) if ('game_id' in tmp.columns and ids_sched) else 0
        artifact_frames.append((p, tmp, cov))
    if artifact_frames:
        # Prefer artifact with full schedule coverage; break ties by enriched naming preference
        artifact_frames.sort(key=lambda x: (x[2], ('force_fill' in x[0]), x[0].endswith('_force_fill.csv')), reverse=True)
        # If top candidate does not reach full schedule coverage, but a force_fill does, select force_fill
        best_p, best_df, best_cov = artifact_frames[0]
        # Force promotion if canonical enriched has fewer rows than force_fill variant
        enriched_p = os.path.join(OUT, f'predictions_unified_enriched_{target_date}.csv')
        force_p = os.path.join(OUT, f'predictions_unified_enriched_{target_date}_force_fill.csv')
        try:
            if os.path.exists(force_p) and os.path.exists(enriched_p):
                en_df = pd.read_csv(enriched_p)
                ff_df = pd.read_csv(force_p)
                if 'game_id' in en_df.columns and 'game_id' in ff_df.columns:
                    en_ids = set(en_df['game_id'].astype(str))
                    ff_ids = set(ff_df['game_id'].astype(str))
                    if ids_sched and ff_ids.issuperset(ids_sched) and (not en_ids.issuperset(ids_sched)):
                        # Promote force-fill to canonical (non-destructive copy)
                        import shutil
                        shutil.copyfile(force_p, enriched_p)
                        best_p, best_df = enriched_p, ff_df
                        best_cov = len(ids_sched.intersection(ff_ids))
        except Exception:
            pass
        disp_path = best_p
        disp_df = best_df
    else:
        # Fallback to latest unified/model historical artifact
        files = glob.glob(os.path.join(OUT,'predictions_unified_*.csv')) + glob.glob(os.path.join(OUT,'predictions_model_*.csv'))
        files = sorted(files)
        disp_path = files[-1] if files else None
        disp_df = pd.read_csv(disp_path) if disp_path else pd.DataFrame()

    # Restrict to today's scheduled rows and root on canonical ESPN schedule times
    games_today = None
    if target_date:
        try:
            gpath = os.path.join(OUT, f'games_{target_date}.csv')
            if os.path.exists(gpath):
                games_today = pd.read_csv(gpath)
        except Exception:
            games_today = None

    if not disp_df.empty and 'game_id' in disp_df.columns:
        try:
            disp_df['game_id'] = disp_df['game_id'].astype(str)
            if 'date' in disp_df.columns:
                disp_df['date'] = pd.to_datetime(disp_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                disp_df = disp_df[disp_df['date'] == target_date]
            # Join canonical schedule for start_time/_start_dt/start_time_local/start_tz_abbr
            if games_today is not None and not games_today.empty and 'game_id' in games_today.columns:
                try:
                    g = games_today.copy()
                    g['game_id'] = g['game_id'].astype(str)
                    keep_cols = [c for c in ['game_id','start_time','start_time_local','start_tz_abbr','date'] if c in g.columns]
                    g = g[keep_cols].drop_duplicates(subset=['game_id']) if keep_cols else g
                    disp_df = disp_df.merge(g, on='game_id', how='left', suffixes=('', '_schedtime'))
                    # Canonical UTC start
                    if 'start_time_schedtime' in disp_df.columns:
                        disp_df['_start_dt'] = disp_df['start_time_schedtime']
                        disp_df['start_time'] = disp_df['start_time_schedtime']
                    # Local clock + tz abbr
                    if 'start_time_local_schedtime' in disp_df.columns:
                        disp_df['start_time_local'] = disp_df['start_time_local'].where(
                            disp_df['start_time_local'].notna() & disp_df['start_time_local'].astype(str).str.strip().ne(''),
                            disp_df['start_time_local_schedtime']
                        )
                    if 'start_tz_abbr_schedtime' in disp_df.columns:
                        disp_df['start_tz_abbr'] = disp_df['start_tz_abbr'].where(
                            disp_df['start_tz_abbr'].notna() & disp_df['start_tz_abbr'].astype(str).str.strip().ne(''),
                            disp_df['start_tz_abbr_schedtime']
                        )
                    drop_cols = [c for c in disp_df.columns if c.endswith('_schedtime')]
                    if drop_cols:
                        try:
                            disp_df.drop(columns=drop_cols, inplace=True)
                        except Exception:
                            pass
                except Exception:
                    pass
            if ids_sched:
                disp_df = disp_df[disp_df['game_id'].isin(ids_sched)]
        except Exception:
            pass
    ids_disp_initial = set(disp_df['game_id'].astype(str)) if not disp_df.empty and 'game_id' in disp_df.columns else set()

    # Blank team detection pre-enrichment
    ht = disp_df['home_team'] if 'home_team' in disp_df.columns else disp_df.get('home')
    at = disp_df['away_team'] if 'away_team' in disp_df.columns else disp_df.get('away')
    blank_mask = None
    if ht is not None and at is not None:
        blank_mask = ((ht.isna() | ht.astype(str).str.strip().eq('')) | (at.isna() | at.astype(str).str.strip().eq('')))
    blank_ids = disp_df.loc[blank_mask, 'game_id'].astype(str).tolist() if blank_mask is not None and 'game_id' in disp_df.columns else []

    # Opportunistic enrichment of team names
    try:
        if blank_ids and not disp_df.empty and 'game_id' in disp_df.columns:
            sched_path = os.path.join(OUT, 'games_curr.csv')
            sched_df = pd.read_csv(sched_path) if os.path.exists(sched_path) else pd.DataFrame()
            if not sched_df.empty and 'game_id' in sched_df.columns:
                sched_df['game_id'] = sched_df['game_id'].astype(str)
                hcol = 'home_team' if 'home_team' in sched_df.columns else ('home' if 'home' in sched_df.columns else None)
                acol = 'away_team' if 'away_team' in sched_df.columns else ('away' if 'away' in sched_df.columns else None)
                lut_cols = ['game_id']
                if hcol: lut_cols.append(hcol)
                if acol: lut_cols.append(acol)
                lut = sched_df[lut_cols].drop_duplicates()
                disp_df['game_id'] = disp_df['game_id'].astype(str)
                disp_df = disp_df.merge(lut, on='game_id', how='left', suffixes=('', '_sched'))
                if hcol and f'{hcol}_sched' in disp_df.columns:
                    base_h = 'home_team' if 'home_team' in disp_df.columns else hcol
                    disp_df[base_h] = disp_df[base_h].where(disp_df[base_h].notna() & disp_df[base_h].astype(str).str.strip().ne(''), disp_df[f'{hcol}_sched'])
                if acol and f'{acol}_sched' in disp_df.columns:
                    base_a = 'away_team' if 'away_team' in disp_df.columns else acol
                    disp_df[base_a] = disp_df[base_a].where(disp_df[base_a].notna() & disp_df[base_a].astype(str).str.strip().ne(''), disp_df[f'{acol}_sched'])
            # Secondary game sources
            g_candidates = [
                os.path.join(OUT, f'games_with_last_{target_date}.csv'),
                os.path.join(OUT, f'games_with_closing_{target_date}.csv'),
                os.path.join(OUT, 'games_with_last.csv'),
                os.path.join(OUT, 'games_with_closing.csv')
            ]
            for gp in g_candidates:
                if not os.path.exists(gp):
                    continue
                try:
                    gdf = pd.read_csv(gp)
                except Exception:
                    continue
                if gdf.empty or 'game_id' not in gdf.columns:
                    continue
                gdf['game_id'] = gdf['game_id'].astype(str)
                if 'home_team' not in gdf.columns and 'home' in gdf.columns:
                    gdf['home_team'] = gdf['home']
                if 'away_team' not in gdf.columns and 'away' in gdf.columns:
                    gdf['away_team'] = gdf['away']
                keep = [c for c in ['game_id','home_team','away_team'] if c in gdf.columns]
                gdf = gdf[keep].drop_duplicates(subset=['game_id']) if keep else gdf
                if set(['game_id','home_team','away_team']).issubset(gdf.columns):
                    disp_df = disp_df.merge(gdf, on='game_id', how='left', suffixes=('', '_gsrc'))
                    for side in ('home','away'):
                        base_col = f'{side}_team' if f'{side}_team' in disp_df.columns else side
                        src_col = f'{base_col}_gsrc'
                        if src_col in disp_df.columns:
                            disp_df[base_col] = disp_df[base_col].where(disp_df[base_col].notna() & disp_df[base_col].astype(str).str.strip().ne(''), disp_df[src_col])
                    drop_cols = [c for c in ['home_team_gsrc','away_team_gsrc','home_gsrc','away_gsrc'] if c in disp_df.columns]
                    if drop_cols:
                        try:
                            disp_df.drop(columns=drop_cols, inplace=True)
                        except Exception:
                            pass
            if not disp_df.empty and 'game_id' in disp_df.columns:
                before_rows = len(disp_df)
                disp_df = disp_df.sort_values('game_id').drop_duplicates(subset=['game_id'], keep='first')
                dedup_delta = before_rows - len(disp_df)
            else:
                dedup_delta = 0
            # Recompute blanks post enrichment
            ht2 = disp_df['home_team'] if 'home_team' in disp_df.columns else disp_df.get('home')
            at2 = disp_df['away_team'] if 'away_team' in disp_df.columns else disp_df.get('away')
            if ht2 is not None and at2 is not None:
                blank_mask2 = ((ht2.isna() | ht2.astype(str).str.strip().eq('')) | (at2.isna() | at2.astype(str).str.strip().eq('')))
                blank_ids = disp_df.loc[blank_mask2, 'game_id'].astype(str).tolist() if 'game_id' in disp_df.columns else []
        else:
            dedup_delta = 0
    except Exception:
        dedup_delta = 0

    if not disp_df.empty and 'game_id' in disp_df.columns:
        disp_df['game_id'] = disp_df['game_id'].astype(str)
        # Drop purely synthetic matchups from core coverage payload; they lack ESPN-rooted times
        try:
            synth_mask = disp_df['game_id'].astype(str).str.startswith('synthetic:')
            if synth_mask.any():
                disp_df = disp_df[~synth_mask].copy()
        except Exception:
            pass
        ids_disp_final = set(disp_df['game_id'])
    else:
        ids_disp_final = ids_disp_initial

    # Coverage status synthesis if absent
    status_counts = {}
    if not disp_df.empty:
        if 'coverage_status' not in disp_df.columns:
            pt = disp_df.get('pred_total'); pm = disp_df.get('pred_margin'); mt = disp_df.get('market_total'); sh = disp_df.get('spread_home')
            ht3 = disp_df['home_team'] if 'home_team' in disp_df.columns else disp_df.get('home')
            at3 = disp_df['away_team'] if 'away_team' in disp_df.columns else disp_df.get('away')
            synth = []
            for i in range(len(disp_df)):
                home = str(ht3.iloc[i]) if ht3 is not None else ''
                away = str(at3.iloc[i]) if at3 is not None else ''
                if home == 'TBD' and away == 'TBD':
                    synth.append('placeholder'); continue
                has_preds = pt is not None and pd.notna(pt.iloc[i]) and pm is not None and pd.notna(pm.iloc[i])
                has_odds = (mt is not None and pd.notna(mt.iloc[i])) or (sh is not None and pd.notna(sh.iloc[i]))
                if has_preds and has_odds:
                    synth.append('full')
                elif has_preds:
                    synth.append('no_odds')
                else:
                    synth.append('missing_preds')
            disp_df['coverage_status'] = synth
        try:
            status_counts = disp_df['coverage_status'].value_counts().to_dict()
        except Exception:
            status_counts = {}

    odds_market_missing = int(disp_df['market_total'].isna().sum()) if 'market_total' in disp_df.columns else None
    odds_spread_missing = int(disp_df['spread_home'].isna().sum()) if 'spread_home' in disp_df.columns else None
    sched_total = len(ids_sched)
    disp_total = len(ids_disp_final)
    if len(blank_ids) > 0:
        quality = 'fail'
    else:
        missing_pct = (odds_market_missing / sched_total) if (odds_market_missing is not None and sched_total > 0) else None
        if missing_pct is not None and missing_pct > 0.25:
            quality = 'degraded'
        elif missing_pct is not None and missing_pct > 0.05:
            quality = 'partial'
        else:
            quality = 'ok'

    # Per-game payload
    keep_cols = ['game_id','home_team','away_team','start_time','pred_total','pred_margin','market_total','spread_home','coverage_status']
    games_payload = []
    if not disp_df.empty:
        for r in disp_df[[c for c in keep_cols if c in disp_df.columns]].to_dict('records'):
            games_payload.append(r)

    resp = {
        'ok': True,
        'date': target_date,
        'artifact': os.path.basename(disp_path) if disp_path else None,
        'schedule_total': sched_total,
        'display_total': disp_total,
        'display_total_raw': len(ids_disp_initial),
        'dedup_removed_rows': dedup_delta,
        'missing_ids': sorted(list(ids_sched - ids_disp_final)),
        'blank_team_count': len(blank_ids),
        'blank_team_ids': blank_ids[:50],
        'odds_market_total_missing': odds_market_missing,
        'odds_spread_home_missing': odds_spread_missing,
        'has_calibrated_totals': bool(('pred_total_basis' in disp_df.columns) and disp_df['pred_total_basis'].astype(str).str.contains('cal', case=False).any()),
        'coverage_quality': quality,
        'status_counts': status_counts,
        'non_placeholder_games': int(disp_total - status_counts.get('placeholder',0)) if status_counts else 0,
        'games': games_payload,
        'coverage_missing_today': None,
        'coverage_present_today': None,
        'd1_expected_today': None,
        'report_rows': [],
        'missing_teams_rows': []
    }

    # Legacy report integration
    try:
        rep_path = os.path.join(OUT, 'coverage_report_today.csv')
        if os.path.exists(rep_path):
            df_rep = pd.read_csv(rep_path)
            if not df_rep.empty:
                resp['report_rows'] = df_rep.to_dict('records')
                resp['coverage_missing_today'] = int(len(df_rep))
    except Exception:
        pass
    try:
        mt_path = os.path.join(OUT, 'coverage_missing_teams_today.csv')
        if os.path.exists(mt_path):
            df_mt = pd.read_csv(mt_path)
            if not df_mt.empty:
                resp['missing_teams_rows'] = df_mt.to_dict('records')
    except Exception:
        pass
    try:
        if resp['d1_expected_today'] is None:
            resp['d1_expected_today'] = sched_total
        miss = resp['coverage_missing_today'] or 0
        resp['coverage_present_today'] = max(0, int(resp['d1_expected_today']) - int(miss))
    except Exception:
        pass

    # Sidecar summary if present
    sidecar = os.path.join(OUT, f'coverage_status_summary_{target_date}.json')
    if os.path.exists(sidecar):
        try:
            with open(sidecar, 'r', encoding='utf-8') as fh:
                resp['sidecar'] = json.load(fh)
        except Exception:
            pass

    # Persist snapshot for auditing
    try:
        snap_path_latest = os.path.join(OUT, 'coverage_snapshot_latest.json')
        with open(snap_path_latest, 'w', encoding='utf-8') as fh:
            json.dump(resp, fh, indent=2)
        arch_dir = os.path.join(OUT, 'coverage_archive')
        os.makedirs(arch_dir, exist_ok=True)
        arch_path = os.path.join(arch_dir, f'coverage_{target_date}.json')
        if not os.path.exists(arch_path):
            with open(arch_path, 'w', encoding='utf-8') as fh:
                json.dump(resp, fh, indent=2)
    except Exception:
        pass

    return jsonify(resp)


@app.route("/api/pipeline-stats")
def api_pipeline_stats():
    """Return the last cached pipeline_stats snapshot.

    This reflects the most recent index() execution. Include diag=1 on / to ensure
    full stats population in template if needed. Here we return whatever was cached.
    """
    try:
        snap = _LAST_PIPELINE_STATS or {}
    except Exception:
        snap = {}
    return jsonify({"ok": True, "stats": snap})
    """Return D1 coverage diagnostics for today or selected date.
    Includes counts and rows from coverage_report_today.csv and
    coverage_missing_teams_today.csv when available."""
    try:
        today_str = datetime.now().strftime('%Y-%m-%d')
        date_q = request.args.get('date')
        target_date = str(date_q) if date_q else today_str
    except Exception:
        target_date = None
    out = {
        'date': target_date,
        'd1_expected_today': None,
        'coverage_present_today': None,
        'coverage_missing_today': None,
        'report_rows': [],
        'missing_teams_rows': []
    }
    try:
        games_curr_path = OUT / 'games_curr.csv'
        g_today = _safe_read_csv(games_curr_path) if games_curr_path.exists() else pd.DataFrame()
        if not g_today.empty:
            if 'date' in g_today.columns and target_date:
                try:
                    g_today['date'] = pd.to_datetime(g_today['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                    g_today = g_today[g_today['date'] == target_date]
                except Exception:
                    pass
            out['d1_expected_today'] = int(g_today['game_id'].astype(str).nunique()) if 'game_id' in g_today.columns else 0
    except Exception:
        pass
    try:
        rep_path = OUT / 'coverage_report_today.csv'
        if rep_path.exists():
            df_rep = _safe_read_csv(rep_path)
            if not df_rep.empty:
                out['report_rows'] = df_rep.to_dict(orient='records')
                if out['coverage_missing_today'] is None:
                    out['coverage_missing_today'] = int(len(df_rep))
    except Exception:
        pass
    try:
        mt_path = OUT / 'coverage_missing_teams_today.csv'
        if mt_path.exists():
            df_mt = _safe_read_csv(mt_path)
            if not df_mt.empty:
                out['missing_teams_rows'] = df_mt.to_dict(orient='records')
    except Exception:
        pass
    try:
        if out['coverage_present_today'] is None and out['d1_expected_today'] is not None:
            miss = out['coverage_missing_today'] or 0
            out['coverage_present_today'] = max(0, int(out['d1_expected_today']) - int(miss))
    except Exception:
        pass
    return jsonify(out)


@app.route("/api/set_tz", methods=["GET", "POST"])
def api_set_tz():
    """Set the end-user display timezone via cookie.

    Accepts IANA timezone name via `tz` (query or form).
    If `json=1` is present, returns JSON instead of redirecting.
    Optional `redirect` param to control post-set navigation.
    """
    try:
        tz = (request.values.get("tz") or "").strip()
    except Exception:
        tz = ""
    want_json = (request.values.get("json") or "").strip() in ("1", "true", "True")
    redir = request.values.get("redirect") or request.referrer or "/"
    ok = False
    if tz:
        try:
            ZoneInfo(tz)
            ok = True
        except Exception:
            ok = False
    if want_json:
        resp = make_response(jsonify({"ok": bool(ok), "tz": tz if ok else None}))
    else:
        resp = make_response(redirect(redir))
    if ok:
        try:
            resp.set_cookie("display_tz", tz, max_age=60*60*24*365, samesite="Lax")
        except Exception:
            pass
    return resp

@app.route("/api/coverage-trend")
def api_coverage_trend():
    """Return coverage metrics for the past N days (default 7)."""
    import os, glob, json
    from datetime import datetime, timedelta
    import pandas as pd
    try:
        days = int(request.args.get('days', '7'))
    except Exception:
        days = 7
    days = max(1, min(days, 30))  # safety bounds
    OUT = os.path.join(os.getcwd(), 'outputs')
    today_str = datetime.now().strftime('%Y-%m-%d')
    results = []
    for i in range(days):
        d = datetime.now() - timedelta(days=i)
        date_str = d.strftime('%Y-%m-%d')
        # Schedule
        games_path = os.path.join(OUT, 'games_curr.csv')
        games_df = pd.read_csv(games_path) if os.path.exists(games_path) else pd.DataFrame()
        if not games_df.empty and 'date' in games_df.columns:
            try:
                games_df['date'] = pd.to_datetime(games_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                games_df = games_df[games_df['date'] == date_str]
            except Exception:
                pass
        sched_ids = set(games_df['game_id'].astype(str)) if not games_df.empty and 'game_id' in games_df.columns else set()
        # Display artifact preference
        cand = [
            os.path.join(OUT, f'predictions_unified_enriched_{date_str}.csv'),
            os.path.join(OUT, f'predictions_unified_{date_str}.csv'),
            os.path.join(OUT, f'predictions_model_{date_str}.csv'),
            os.path.join(OUT, f'align_period_{date_str}.csv'),
        ]
        disp_path = next((p for p in cand if os.path.exists(p)), None)
        disp_df = pd.read_csv(disp_path) if disp_path else pd.DataFrame()
        if not disp_df.empty and 'game_id' in disp_df.columns:
            try:
                disp_df['game_id'] = disp_df['game_id'].astype(str)
                if 'date' in disp_df.columns:
                    disp_df['date'] = pd.to_datetime(disp_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                    disp_df = disp_df[disp_df['date'] == date_str]
                if sched_ids:
                    disp_df = disp_df[disp_df['game_id'].isin(sched_ids)]
            except Exception:
                pass
        # Blank count
        if not disp_df.empty:
            htc = 'home_team' if 'home_team' in disp_df.columns else ('home' if 'home' in disp_df.columns else None)
            atc = 'away_team' if 'away_team' in disp_df.columns else ('away' if 'away' in disp_df.columns else None)
            if htc and atc:
                hser = disp_df[htc]
                aser = disp_df[atc]
                blank_mask = (hser.isna() | hser.astype(str).str.strip().eq('')) | (aser.isna() | aser.astype(str).str.strip().eq(''))
                blank_cnt = int(blank_mask.sum())
                blank_ids = disp_df.loc[blank_mask, 'game_id'].astype(str).tolist() if 'game_id' in disp_df.columns else []
            else:
                blank_cnt = 0
                blank_ids = []
        else:
            blank_cnt = 0
            blank_ids = []
        # Odds coverage simple metrics
        mt_missing = int(disp_df['market_total'].isna().sum()) if 'market_total' in disp_df.columns else None
        sp_missing = int(disp_df['spread_home'].isna().sum()) if 'spread_home' in disp_df.columns else None
        results.append({
            'date': date_str,
            'schedule_total': len(sched_ids),
            'display_total': int(disp_df['game_id'].nunique()) if 'game_id' in disp_df.columns and not disp_df.empty else 0,
            'blank_team_count': blank_cnt,
            'blank_team_ids_sample': blank_ids[:10],
            'artifact': disp_path,
            'market_total_missing': mt_missing,
            'spread_home_missing': sp_missing,
        })
    payload = {'ok': True, 'days': days, 'trend': results}
    # Persist latest trend snapshot
    try:
        latest_path = os.path.join(OUT, 'coverage_trend_latest.json')
        with open(latest_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        arch_dir = os.path.join(OUT, 'coverage_archive')
        os.makedirs(arch_dir, exist_ok=True)
        arch_path = os.path.join(arch_dir, f'coverage_trend_{today_str}_d{days}.json')
        with open(arch_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
    except Exception:
        pass
    return jsonify(payload)
# NOTE: Removed earlier duplicate /api/health definition; unified health endpoint defined later.

@app.route('/api/coverage-retro')
def api_coverage_retro():
    """Retroactive coverage audit for past N days.
    Scans enriched prediction artifacts (or fallbacks) to verify zero blank teams,
    odds and prediction completeness. Persists audit snapshot.
    Query params:
      days: number of past days to audit (default 7, inclusive of today)
    """
    import json, datetime, pandas as pd, numpy as np, os
    try:
        days = int(request.args.get('days', '7'))
    except Exception:
        days = 7
    days = max(1, min(days, 30))  # cap for safety
    today = datetime.date.today()
    audit_rows = []
    for offset in range(days):
        d = today - datetime.timedelta(days=offset)
        ds = d.isoformat()
        enriched_path = OUT / f'predictions_unified_enriched_{ds}.csv'
        base_path = OUT / f'predictions_unified_{ds}.csv'
        model_path = OUT / f'predictions_model_{ds}.csv'
        games_path = OUT / 'games_curr.csv'
        odds_candidates = [OUT / f'games_with_closing_{ds}.csv', OUT / f'games_with_last_{ds}.csv', OUT / 'games_with_closing.csv', OUT / 'games_with_last.csv']
        status = 'ok'
        source_used = None
        try:
            if enriched_path.exists():
                df = pd.read_csv(enriched_path)
                source_used = 'enriched'
            elif base_path.exists():
                df = pd.read_csv(base_path)
                source_used = 'unified'
            elif model_path.exists():
                df = pd.read_csv(model_path)
                source_used = 'model'
            else:
                df = pd.DataFrame()
                status = 'missing_artifacts'
        except Exception:
            df = pd.DataFrame()
            status = 'read_error'
        # Attach odds if totals missing widely and odds file exists
        try:
            if (df.empty or ('market_total' not in df.columns) or df['market_total'].isna().all()) and not df.empty:
                for oc in odds_candidates:
                    if oc.exists():
                        try:
                            o = pd.read_csv(oc)
                        except Exception:
                            continue
                        if o.empty: continue
                        if 'game_id' in o.columns and 'game_id' in df.columns:
                            o['game_id'] = o['game_id'].astype(str)
                            df['game_id'] = df['game_id'].astype(str)
                            keep_cols = [c for c in ['game_id','closing_total','market_total','spread_home','spread_away'] if c in o.columns]
                            if keep_cols:
                                df = df.merge(o[keep_cols], on='game_id', how='left', suffixes=('','_o'))
                        break
        except Exception:
            pass
        # Metrics
        row_count = int(len(df))
        blank_team = 0
        missing_market_total = 0
        missing_spread = 0
        missing_pred_total = 0
        missing_pred_margin = 0
        if not df.empty:
            if {'home_team','away_team'}.issubset(df.columns):
                ht = df['home_team'].astype(str).str.strip()
                at = df['away_team'].astype(str).str.strip()
                blank_team = int(((ht == '') | (ht.str.lower() == 'nan') | (at == '') | (at.str.lower() == 'nan')).sum())
            if 'market_total' in df.columns:
                missing_market_total = int(df['market_total'].isna().sum())
            elif 'closing_total' in df.columns:
                missing_market_total = int(df['closing_total'].isna().sum())
            else:
                missing_market_total = row_count
            if 'spread_home' in df.columns:
                missing_spread = int(df['spread_home'].isna().sum())
            elif 'spread_away' in df.columns:
                missing_spread = int(df['spread_away'].isna().sum())
            else:
                missing_spread = row_count
            if 'pred_total' in df.columns:
                missing_pred_total = int(pd.to_numeric(df['pred_total'], errors='coerce').isna().sum())
            else:
                missing_pred_total = row_count
            if 'pred_margin' in df.columns:
                missing_pred_margin = int(pd.to_numeric(df['pred_margin'], errors='coerce').isna().sum())
            else:
                missing_pred_margin = row_count
        quality = 'ok'
        if status != 'ok':
            quality = status
        elif blank_team > 0:
            quality = 'teams_missing'
        elif missing_pred_total > 0 or missing_pred_margin > 0:
            quality = 'preds_missing'
        elif missing_market_total > (0.3 * row_count) and row_count > 0:
            quality = 'odds_missing'
        audit_rows.append({
            'date': ds,
            'rows': row_count,
            'blank_team_count': blank_team,
            'missing_market_total': missing_market_total,
            'missing_spread': missing_spread,
            'missing_pred_total': missing_pred_total,
            'missing_pred_margin': missing_pred_margin,
            'quality': quality,
            'source_used': source_used,
            'status': status
        })
    payload = {'ok': True, 'days': days, 'audit': audit_rows}
    # Persist snapshot
    try:
        latest = OUT / 'coverage_retro_latest.json'
        with open(latest, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
        arch_dir = OUT / 'coverage_archive'
        arch_dir.mkdir(exist_ok=True)
        arch_path = arch_dir / f'coverage_retro_{today.isoformat()}_d{days}.json'
        with open(arch_path, 'w', encoding='utf-8') as fh:
            json.dump(payload, fh, indent=2)
    except Exception:
        pass
    # Escalation flag if any quality not ok
    if any(r['quality'] != 'ok' for r in audit_rows):
        try:
            if 'pipeline_stats' in globals():
                pipeline_stats['retro_coverage_escalation'] = True
        except Exception:
            pass
    return jsonify(payload)

# Global diagnostic state variables
_PREDICTIONS_SOURCE_PATH: str | None = None
_MODEL_PREDICTIONS_SOURCE_PATH: str | None = None
pipeline_stats: dict = {}

@app.route('/api/coverage-missing')
def api_coverage_missing():
    """Expose rows with missing REAL predictions or odds for a date (?date=YYYY-MM-DD).
    No synthetic promotion; strictly surfaces gaps so upstream ingestion can remediate.
    """
    date_q = (request.args.get('date') or '').strip()
    target_date = date_q or _today_local().strftime('%Y-%m-%d')
    enriched_path = OUT / f'predictions_unified_enriched_{target_date}.csv'
    if not enriched_path.exists():
        return jsonify({'ok': False, 'error': 'enriched_not_found', 'path': str(enriched_path), 'date': target_date})
    try:
        df = pd.read_csv(enriched_path)
    except Exception as e:
        return jsonify({'ok': False, 'error': 'read_failed', 'detail': str(e)[:160], 'date': target_date})
    if df.empty:
        return jsonify({'ok': True, 'date': target_date, 'rows': 0, 'gap_rows': 0, 'gaps': []})
    gaps = []
    for _, r in df.iterrows():
        miss = []
        if 'pred_total' in df.columns and pd.isna(r.get('pred_total')): miss.append('pred_total')
        if 'pred_margin' in df.columns and pd.isna(r.get('pred_margin')): miss.append('pred_margin')
        if 'market_total' in df.columns and pd.isna(r.get('market_total')): miss.append('market_total')
        if 'spread_home' in df.columns and pd.isna(r.get('spread_home')): miss.append('spread_home')
        if miss:
            gaps.append({
                'game_id': str(r.get('game_id')),
                'home_team': r.get('home_team'),
                'away_team': r.get('away_team'),
                'missing': miss
            })
    return jsonify({'ok': True, 'date': target_date, 'rows': int(len(df)), 'gap_rows': int(len(gaps)), 'gaps': gaps})
# Cache last in-request pipeline_stats for out-of-band diagnostics
_LAST_PIPELINE_STATS: dict[str, Any] | None = None

def ensure_runtime_artifacts() -> None:
    """Best-effort bootstrap of minimal daily artifacts on cold start.

    Creates today's `games_curr.csv` if absent by extracting rows from any games_*.csv
    that contain today's date. Autogeneration of predictions_<date>.csv is handled inside
    _load_predictions_current; we trigger that load once to make sure promotion/shell
    creation runs early. Idempotent via sentinel file.
    """
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    if not today_str:
        return
    sentinel = OUT / ".bootstrap_done"
    if sentinel.exists():
        return
    try:
        games_curr = OUT / "games_curr.csv"
        need_games = True
        if games_curr.exists():
            try:
                gdf = pd.read_csv(games_curr, engine='python')
                if not gdf.empty and "date" in gdf.columns and (gdf["date"].astype(str) == today_str).any():
                    need_games = False
            except Exception:
                need_games = True
        if need_games:
            collected: list[pd.DataFrame] = []
            for p in OUT.glob("games_*.csv"):
                try:
                    if p.name.startswith("games_curr"):  # skip accidental patterns
                        continue
                    df = pd.read_csv(p, engine='python')
                    if df.empty or "date" not in df.columns:
                        continue
                    mask = df["date"].astype(str) == today_str
                    if mask.any():
                        collected.append(df[mask].copy())
                except Exception:
                    continue
            if collected:
                merged = pd.concat(collected, ignore_index=True)
                try:
                    merged.to_csv(games_curr, index=False)
                    logger.warning("Bootstrap wrote games_curr.csv for %s (rows=%s)", today_str, len(merged))
                except Exception as e:
                    logger.error("Bootstrap failed writing games_curr.csv: %s", e)
            else:
                logger.warning("Bootstrap found no games rows for %s; games_curr.csv remains absent or empty", today_str)
        # Trigger predictions autogeneration logic (promotion/shell) once.
        try:
            _ = _load_predictions_current()
        except Exception:
            pass
        # Optional startup refresh to ensure real (non-synthetic, non-uniform) predictions file in commit mode.
        try:
            commit_mode = (os.getenv("NCAAB_COMMIT_PREDICTIONS_MODE", "").strip().lower() in ("1","true","yes"))
            refresh_flag = (os.getenv("NCAAB_STARTUP_REFRESH", "").strip().lower() in ("1","true","yes"))
            if commit_mode and refresh_flag and today_str:
                pred_path = OUT / f"predictions_{today_str}.csv"
                need_refresh = (not pred_path.exists())
                # Light uniform heuristic: if file exists but > 80% of non-null totals share same value
                if not need_refresh:
                    try:
                        pdf = pd.read_csv(pred_path)
                        if not pdf.empty and 'pred_total' in pdf.columns:
                            ptv = pd.to_numeric(pdf['pred_total'], errors='coerce')
                            if ptv.notna().sum() > 10:
                                vc = ptv.value_counts()
                                if len(vc) and (vc.iloc[0] / ptv.notna().sum()) > 0.80:
                                    need_refresh = True
                                    logger.warning("Startup refresh triggered: uniform predictions detected (%s appears in %.1f%% of rows)", vc.index[0], 100.0 * vc.iloc[0]/ptv.notna().sum())
                    except Exception as e:
                        logger.warning("Uniform heuristic failed: %s", e)
                if need_refresh:
                    mode = os.getenv("NCAAB_STARTUP_REFRESH_MODE", "light").strip().lower()
                    logger.warning("Startup refresh beginning (mode=%s) to populate predictions for %s", mode, today_str)
                    py_exe = sys.executable or "python"
                    if mode == "full":
                        # Run daily-run limited scope (skip picks threshold by inflating threshold)
                        cmd = [py_exe, '-m', 'ncaab_model.cli', 'daily-run', '--date', today_str, '--provider', 'espn', '--region', 'us', '--segment', 'team', '--preseason-weight', '0.4', '--threshold', '9999', '--default-price', '-110']
                    else:
                        # Light mode: run infer + calibrate + interval, then promote if not in commit-mode (so manually copy for commit-mode)
                        cmd = [py_exe, '-m', 'src.modeling.infer', '--date', today_str]
                    import subprocess, time as _t
                    try:
                        t0 = _t.time()
                        proc = subprocess.run(cmd, cwd=str(ROOT), capture_output=True, text=True, timeout=300)
                        logger.warning("Startup refresh command: %s", ' '.join(cmd))
                        if proc.returncode != 0:
                            logger.error("Startup refresh failed (rc=%s): %s", proc.returncode, proc.stderr[:500])
                        else:
                            logger.warning("Startup refresh completed in %.1fs", _t.time() - t0)
                            # If light mode and still no predictions_<date>.csv, attempt manual promotion from calibrated.
                            if mode != 'full' and not pred_path.exists():
                                calib = OUT / f"predictions_model_calibrated_{today_str}.csv"
                                rawm = OUT / f"predictions_model_{today_str}.csv"
                                srcp = calib if calib.exists() else (rawm if rawm.exists() else None)
                                if srcp:
                                    try:
                                        mdf = pd.read_csv(srcp, engine='python')
                                        if not mdf.empty and 'game_id' in mdf.columns:
                                            pt_col = next((c for c in mdf.columns if c.startswith('pred_total')), None)
                                            pm_col = next((c for c in mdf.columns if c.startswith('pred_margin')), None)
                                            if pt_col and pm_col:
                                                out_df = pd.DataFrame({'game_id': mdf['game_id'].astype(str), 'date': mdf.get('date'), 'pred_total': mdf[pt_col], 'pred_margin': mdf[pm_col]})
                                                out_df.to_csv(pred_path, index=False)
                                                logger.warning("Startup refresh promoted predictions file from %s -> %s (rows=%s)", srcp, pred_path, len(out_df))
                                    except Exception as e:
                                        logger.error("Startup light promotion failed: %s", e)
                    except Exception as e:
                        logger.error("Startup refresh invocation error: %s", e)
        except Exception as e:
            logger.error("Startup refresh wrapper failure: %s", e)
        try:
            sentinel.write_text(f"bootstrapped {today_str}\n", encoding="utf-8")
        except Exception:
            pass
    except Exception as e:
        logger.error("ensure_runtime_artifacts error: %s", e)

# (Deferred bootstrap invocation moved below prediction loader definition to avoid calling before function exists.)
def _load_eval_metrics() -> dict[str, Any]:
    out: dict[str, Any] = {}
    # Calibration metrics
    calib = OUT / "calibration" / "metrics.json"
    if calib.exists():
        try:
            out["calibration"] = json.loads(calib.read_text(encoding="utf-8"))
        except Exception:
            pass
    # Closing backtest summary
    closing = OUT / "eval_closing" / "overall_summary.json"
    if closing.exists():
        try:
            out["closing_backtest"] = json.loads(closing.read_text(encoding="utf-8"))
        except Exception:
            pass
    return out


def _resolve_outputs_dir() -> Path:
    """Determine outputs directory using env override, settings, or fallback."""
    env_dir = os.getenv("NCAAB_OUTPUTS_DIR", "").strip()
    if env_dir:
        try:
            p = Path(env_dir).resolve()
            if p.exists() and p.is_dir():
                logger.info("Using outputs dir (env): %s", p)
                return p
        except Exception:
            pass
    # settings.outputs_dir may be a Path-like or string
    try:
        p2 = Path(str(settings.outputs_dir))
        if p2.exists() and p2.is_dir():
            logger.info("Using outputs dir (settings): %s", p2)
            return p2
    except Exception:
        pass
    # common fallbacks
    for cand in [ROOT / "outputs", ROOT]:
        try:
            if cand.exists() and cand.is_dir():
                logger.warning("Using fallback outputs dir: %s", cand)
                return cand
        except Exception:
            continue
    return ROOT

OUT = _resolve_outputs_dir()
                    

# Normalize basis labels in existing unified artifacts on import (best-effort)
try:
    import pandas as _pd
    for _p in list(OUT.glob('predictions_unified_*.csv')):
        try:
            _df = _pd.read_csv(_p)
            changed = False
            if 'pred_total_basis' in _df.columns:
                _new_t = _df['pred_total_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                if not _new_t.equals(_df['pred_total_basis']):
                    _df['pred_total_basis'] = _new_t
                    changed = True
            if 'pred_margin_basis' in _df.columns:
                _new_m = _df['pred_margin_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                if not _new_m.equals(_df['pred_margin_basis']):
                    _df['pred_margin_basis'] = _new_m
                    changed = True
            if changed:
                _df.to_csv(_p, index=False)
        except Exception:
            continue
except Exception:
    pass



# Optional custom team map: allow overriding/augmenting normalize_name via CSV
_CUSTOM_TEAM_SLUG_MAP: dict[str, str] | None = None

def _load_custom_team_map() -> dict[str, str]:
    """Load data/team_map.csv into a slug->slug mapping for canonicalization.

    CSV columns expected: 'raw', 'canonical' (flexible casing). We canonicalize both sides
    through normalize_name to build a stable mapping that can catch provider variants.
    """
    global _CUSTOM_TEAM_SLUG_MAP
    if _CUSTOM_TEAM_SLUG_MAP is not None:
        return _CUSTOM_TEAM_SLUG_MAP
    mapping: dict[str, str] = {}
    try:
            path = settings.data_dir / "team_map.csv"
            if path.exists():
                df = pd.read_csv(path)
                cols = {c.lower().strip(): c for c in df.columns}
                raw_col = cols.get("raw") or list(df.columns)[0]
                can_col = cols.get("canonical") or (list(df.columns)[1] if len(df.columns) > 1 else raw_col)
                for _, r in df.iterrows():
                    raw = str(r.get(raw_col) or "").strip()
                    can = str(r.get(can_col) or "").strip()
                    if not raw or not can:
                        continue
                    # IMPORTANT: avoid recursive _canon_slug -> _load_custom_team_map calls; use base normalize_name here.
                    raw_slug = normalize_name(raw)
                    can_slug = normalize_name(can)
                    if raw_slug and can_slug and raw_slug != can_slug:
                        mapping[raw_slug] = can_slug
    except Exception:
        mapping = {}
    _CUSTOM_TEAM_SLUG_MAP = mapping
    return mapping

def _canon_slug(name: str) -> str:
    """Normalize a team name to a canonical slug, applying custom map if available."""
    slug = normalize_name(name)
    m = _load_custom_team_map()
    return m.get(slug, slug)


def _today_local() -> dt.date:
    """Return 'today' in the configured schedule timezone.

    Prefers SCHEDULE_TZ (new), falls back to NCAAB_SCHEDULE_TZ (legacy), then America/New_York.
    Render dynos run UTC; college basketball slates are anchored to US/Eastern. Without
    this adjustment early-morning UTC can cause us to still see yesterday's date and undercount games.
    """
    tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
    try:
        tz = ZoneInfo(tz_name)
        return dt.datetime.now(tz).date()
    except Exception:
        return dt.date.today()


def _get_display_tz_name() -> str:
    """Resolve the display timezone name, driven by end-user context.

    Preference order:
    1) Query param `tz` (IANA timezone from the client)
    2) Cookie `display_tz`
    3) Env `DISPLAY_TZ`
    4) Env `SCHEDULE_TZ`
    5) Default 'America/Chicago'
    """
    try:
        # Prefer explicit query arg when provided
        tz_candidate = (request.args.get("tz") or "").strip()
        if not tz_candidate:
            # Next, prefer cookie set previously
            tz_candidate = (request.cookies.get("display_tz") or "").strip()
        if not tz_candidate:
            # Fall back to environment configuration
            tz_candidate = os.getenv("DISPLAY_TZ") or os.getenv("SCHEDULE_TZ") or "America/Chicago"
        # Validate candidate; if invalid, fall back to schedule tz then default
        try:
            ZoneInfo(tz_candidate)
            return tz_candidate
        except Exception:
            fallback = os.getenv("SCHEDULE_TZ") or "America/Chicago"
            try:
                ZoneInfo(fallback)
                return fallback
            except Exception:
                return "America/Chicago"
    except Exception:
        # On any unexpected error, use a safe default
        return os.getenv("SCHEDULE_TZ") or "America/Chicago"


def _safe_read_csv(p: Path) -> pd.DataFrame:
    try:
        if p.exists():
            return pd.read_csv(p)
    except Exception:
        pass
    return pd.DataFrame()


def _load_predictions_current() -> pd.DataFrame:
    """Load predictions file using priority order with environment override.

    Priority:
      1) NCAAB_PREDICTIONS_FILE (absolute or relative to OUT)
      2) OUT/predictions_week.csv
      3) OUT/predictions.csv
      4) OUT/predictions_all.csv
      5) OUT/predictions_last2.csv
      6) Largest non-empty OUT/predictions_*.csv (by size)
    """
    global _PREDICTIONS_SOURCE_PATH
    _PREDICTIONS_SOURCE_PATH = None
    env_path = (os.getenv("NCAAB_PREDICTIONS_FILE") or "").strip()
    today_str = None
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    # Auto-generate today's predictions file if missing by promoting model outputs or synthesizing from games.
    try:
        if today_str:
            today_pred = OUT / f"predictions_{today_str}.csv"
            model_cal = OUT / f"predictions_model_calibrated_{today_str}.csv"
            model_raw = OUT / f"predictions_model_{today_str}.csv"
            src_path = model_cal if model_cal.exists() else (model_raw if model_raw.exists() else None)
            # Commit-mode: if set, never auto-promote or synthesize shells; rely solely on committed file.
            commit_mode = (os.getenv("NCAAB_COMMIT_PREDICTIONS_MODE", "").strip().lower() in ("1","true","yes"))
            disable_shell = commit_mode or (os.getenv("NCAAB_DISABLE_SHELL", "").strip().lower() in ("1","true","yes"))
            # Stale model guard: if the newest calibrated/raw model file present is NOT dated today
            # (e.g. predictions_model_calibrated_YYYY-MM-DD.csv with different date) then skip promotion.
            # This prevents reusing older artifacts silently when commit mode is off but fresh model is absent.
            try:
                if src_path is not None and not commit_mode:
                    stem = src_path.stem  # e.g., predictions_model_calibrated_2025-11-20
                    parts = stem.split('_')
                    maybe_date = parts[-1] if len(parts) > 2 else None
                    if maybe_date and maybe_date != today_str:
                        logger.warning("Stale model artifact detected (date=%s != today=%s); skipping promotion.", maybe_date, today_str)
                        src_path = None  # force no promotion path; may synthesize shell unless disabled
            except Exception:
                pass
            need_rebuild = False
            if src_path is not None and not commit_mode:
                try:
                    # Determine if current predictions file is absent, shell, or stale vs model file mtime
                    if not today_pred.exists():
                        need_rebuild = True
                    else:
                        try:
                            pred_df = pd.read_csv(today_pred)
                        except Exception:
                            pred_df = pd.DataFrame()
                        if pred_df.empty or 'game_id' not in pred_df.columns:
                            need_rebuild = True
                        else:
                            # Shell detection: all NaN in pred_total & pred_margin or columns missing
                            pt = pd.to_numeric(pred_df.get('pred_total'), errors='coerce') if 'pred_total' in pred_df.columns else pd.Series([], dtype='float64')
                            pm = pd.to_numeric(pred_df.get('pred_margin'), errors='coerce') if 'pred_margin' in pred_df.columns else pd.Series([], dtype='float64')
                            if ('pred_total' not in pred_df.columns and 'pred_margin' not in pred_df.columns) or ((pt.notna().sum() == 0) and (pm.notna().sum() == 0)):
                                need_rebuild = True
                            else:
                                # Staleness: model file newer by >60s AND row count differs or any NaN that model can fill
                                try:
                                    mtime_model = src_path.stat().st_mtime
                                    mtime_pred = today_pred.stat().st_mtime
                                    if mtime_model - mtime_pred > 60:
                                        model_df_tmp = pd.read_csv(src_path)
                                        if not model_df_tmp.empty and 'game_id' in model_df_tmp.columns:
                                            if len(model_df_tmp) != len(pred_df) or (pt.notna().sum() < len(model_df_tmp)):
                                                need_rebuild = True
                                except Exception:
                                    pass
                except Exception as e:
                    logger.error("Model promotion pre-check failed: %s", e)
                if need_rebuild:
                    try:
                        mdf = pd.read_csv(src_path)
                        if not mdf.empty and 'game_id' in mdf.columns:
                            pt_col = next((c for c in mdf.columns if c.startswith('pred_total')), None)
                            pm_col = next((c for c in mdf.columns if c.startswith('pred_margin')), None)
                            build = pd.DataFrame({'game_id': mdf['game_id'].astype(str)})
                            if 'date' in mdf.columns:
                                build['date'] = mdf['date']
                            if pt_col:
                                build['pred_total'] = pd.to_numeric(mdf[pt_col], errors='coerce')
                            if pm_col:
                                build['pred_margin'] = pd.to_numeric(mdf[pm_col], errors='coerce')
                            build['pred_total_basis'] = np.where(build.get('pred_total').notna(), 'model_v1', None)
                            build['pred_margin_basis'] = np.where(build.get('pred_margin').notna(), 'model_v1', None)
                            build.to_csv(today_pred, index=False)
                            logger.warning("Rebuilt today's predictions from model source (reason=%s): %s -> %s (rows=%s)",
                                           'absent/shell/stale', src_path, today_pred, len(build))
                        else:
                            logger.warning("Model source for today exists but empty or missing game_id: %s", src_path)
                    except Exception as e:
                        logger.error("Failed rebuilding predictions from model source %s: %s", src_path, e)
            # If still missing and no model source, synthesize shell from games unless disabled
            if not today_pred.exists() and not disable_shell:
                games_curr = OUT / 'games_curr.csv'
                try:
                    if games_curr.exists():
                        gdf = pd.read_csv(games_curr)
                        if 'date' in gdf.columns:
                            gdf['date'] = pd.to_datetime(gdf['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                            gdf = gdf[gdf['date'] == today_str]
                        cols = [c for c in ['game_id','date','home_team','away_team'] if c in gdf.columns]
                        shell = gdf[cols].copy()
                        if 'game_id' in shell.columns:
                            shell['game_id'] = shell['game_id'].astype(str)
                        shell['pred_total'] = np.nan
                        shell['pred_margin'] = np.nan
                        shell['pred_total_basis'] = None
                        shell['pred_margin_basis'] = None
                        shell.to_csv(today_pred, index=False)
                        logger.warning("Created synthetic shell predictions file (no model) at %s (rows=%s)", today_pred, len(shell))
                except Exception as e:
                    logger.error("Failed generating synthetic shell predictions file: %s", e)
            elif not today_pred.exists() and disable_shell:
                logger.warning("Commit mode / shell disabled: no predictions file found for %s; UI will show pending state.", today_str)
    except Exception:
        pass
    candidates: list[Path] = []
    # 1) Explicit env override
    if env_path:
        p = Path(env_path)
        if not p.is_absolute():
            p = OUT / env_path
        candidates.append(p)
    # 2) Date-specific file for today first (if exists) to avoid picking a large historical file
    if today_str:
        # Prefer blended predictions for today if present
        candidates.append(OUT / f"predictions_blend_{today_str}.csv")
        candidates.append(OUT / f"predictions_{today_str}.csv")
    # 3) Conventional aggregate files
    for name in ("predictions_blend.csv", "predictions_week.csv", "predictions.csv", "predictions_all.csv", "predictions_last2.csv"):
        candidates.append(OUT / name)
    # 4) All predictions_*.csv (other dates) ordered by size so richest historical fallback last
    try:
        globbed = list(OUT.glob("predictions_*.csv"))
        # Put today's file (if present) at front; others sorted by size desc
        globbed_other = [p for p in globbed if not today_str or p.name != f"predictions_{today_str}.csv"]
        globbed_other = sorted(globbed_other, key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)
        for p in globbed_other:
            if p not in candidates:
                candidates.append(p)
    except Exception:
        pass
    loaded: list[pd.DataFrame] = []
    chosen_path: Path | None = None
    for p in candidates:
        try:
            if p.exists():
                df = pd.read_csv(p)
                if not df.empty:
                    # If this is a dated file and matches today, select immediately
                    if today_str and p.name in {f"predictions_{today_str}.csv", f"predictions_blend_{today_str}.csv"}:
                        logger.info("Loaded today's predictions from: %s (rows=%s)", p, len(df))
                        _PREDICTIONS_SOURCE_PATH = str(p)
                        return df
                    if chosen_path is None:
                        chosen_path = p
                        loaded.append(df)
        except Exception:
                
            continue
    if loaded:
        df = loaded[0]
        logger.info("Loaded predictions fallback from: %s (rows=%s)", chosen_path, len(df))
        _PREDICTIONS_SOURCE_PATH = str(chosen_path)
        return df
    logger.warning("No predictions file found or all empty in %s; proceeding without predictions", OUT)
    return pd.DataFrame()

    
    
## End _load_predictions_current

# Perform bootstrap now that _load_predictions_current is defined.
# Defer runtime bootstrap to first request or __main__ to avoid heavy IO at import
try:
    if str(os.environ.get("NCAAB_BOOTSTRAP_ON_IMPORT", "")).lower() in ("1","true","yes"):
        ensure_runtime_artifacts()
except Exception:
    pass

# --------------------------------------------------------------------------------------
# Commit-mode status diagnostics endpoint
# --------------------------------------------------------------------------------------
@app.route("/api/commit-mode-status")
def api_commit_mode_status():
    """Return commit-mode / shell suppression flags and artifact presence for today's date.

    Fields:
      date: ISO date
      commit_mode: bool (env NCAAB_COMMIT_PREDICTIONS_MODE)
      disable_shell: bool (commit_mode OR env NCAAB_DISABLE_SHELL)
      predictions_file_present: bool (predictions_<date>.csv exists)
      model_artifacts_present: {raw, calibrated, interval}
      stale_model_guard_active: bool (any model artifact date != today)
      env: echo of relevant env vars (non-secret)
    """
    today_str = _today_local().strftime("%Y-%m-%d")
    commit_mode = (os.getenv("NCAAB_COMMIT_PREDICTIONS_MODE", "").strip().lower() in ("1","true","yes"))
    disable_shell = commit_mode or (os.getenv("NCAAB_DISABLE_SHELL", "").strip().lower() in ("1","true","yes"))
    pred_path = OUT / f"predictions_{today_str}.csv"
    model_raw = OUT / f"predictions_model_{today_str}.csv"
    model_cal = OUT / f"predictions_model_calibrated_{today_str}.csv"
    model_interval = OUT / f"predictions_model_interval_{today_str}.csv"
    stale_guard_triggered = False
    for p in [model_cal, model_raw]:
        if p.exists():
            stem_parts = p.stem.split('_')
            maybe_date = stem_parts[-1] if len(stem_parts) > 2 else None
            if maybe_date and maybe_date != today_str:
                stale_guard_triggered = True
                break
    payload = {
        "date": today_str,
        "commit_mode": commit_mode,
        "disable_shell": disable_shell,
        "predictions_file_present": pred_path.exists(),
        "model_artifacts_present": {
            "raw": model_raw.exists(),
            "calibrated": model_cal.exists(),
            "interval": model_interval.exists(),
        },
        "stale_model_guard_active": stale_guard_triggered,
        "paths": {
            "predictions_source_path": _PREDICTIONS_SOURCE_PATH,
            "model_predictions_source_path": _MODEL_PREDICTIONS_SOURCE_PATH,
        },
        "env": {
            "NCAAB_COMMIT_PREDICTIONS_MODE": os.getenv("NCAAB_COMMIT_PREDICTIONS_MODE"),
            "NCAAB_DISABLE_SHELL": os.getenv("NCAAB_DISABLE_SHELL"),
            "NCAAB_UNIFORM_NULL": os.getenv("NCAAB_UNIFORM_NULL"),
            "NCAAB_UNIFORM_STRICT": os.getenv("NCAAB_UNIFORM_STRICT"),
            "NCAAB_STARTUP_REFRESH": os.getenv("NCAAB_STARTUP_REFRESH"),
            "NCAAB_STARTUP_REFRESH_MODE": os.getenv("NCAAB_STARTUP_REFRESH_MODE"),
        }
    }
    # Lightweight inline diagnostics: basis counts, synthetic baseline flags, uniform summary.
    try:
        if pred_path.exists():
            df_diag = pd.read_csv(pred_path)
            if not df_diag.empty and "pred_total" in df_diag.columns:
                ptv = pd.to_numeric(df_diag["pred_total"], errors="coerce")
                non_null = ptv.notna().sum()
                if non_null:
                    vc = ptv.value_counts()
                    payload["pred_total_unique_count"] = int(ptv.nunique())
                    payload["pred_total_top_value"] = float(vc.index[0]) if len(vc) else None
                    payload["pred_total_top_fraction"] = float(vc.iloc[0] / non_null) if len(vc) else None
                    payload["pred_total_uniform_flag"] = bool(len(vc) and (vc.iloc[0] / non_null) > 0.90)
            # Basis counts
            synthetic_labels = {"synthetic_baseline","synthetic_baseline_nomkt","market_copy","blended_low"}
            if "pred_total_basis" in df_diag.columns:
                basis_ser = df_diag["pred_total_basis"].astype(str).fillna("")
                counts = basis_ser.value_counts().to_dict()
                payload["pred_total_basis_counts"] = counts
                payload["synthetic_baseline_rows"] = int(basis_ser.str.lower().isin({s.lower() for s in synthetic_labels}).sum())
            if "pred_margin_basis" in df_diag.columns:
                payload["pred_margin_basis_counts"] = df_diag["pred_margin_basis"].astype(str).value_counts().to_dict()
        # Auto-heal diagnostics (count odds-only persisted rows for today)
        try:
            auto_heal_enabled = os.getenv("NCAAB_AUTO_HEAL_GAMES", "").strip().lower() in ("1","true","yes")
            payload["auto_heal_games_enabled"] = auto_heal_enabled
            if auto_heal_enabled and today_str:
                g_curr = pd.read_csv(OUT / "games_curr.csv") if (OUT / "games_curr.csv").exists() else pd.DataFrame()
                if not g_curr.empty and {"game_id","date"}.issubset(g_curr.columns):
                    try:
                        g_curr["date"] = pd.to_datetime(g_curr["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                    except Exception:
                        pass
                    healed_rows = g_curr[(g_curr["date"].astype(str) == today_str) & g_curr["game_id"].astype(str).str.startswith("odds:")]
                    payload["auto_heal_odds_rows_today"] = int(len(healed_rows))
        except Exception as _ah_e:
            payload["auto_heal_diagnostics_error"] = str(_ah_e)
    except Exception as e:
        payload["diagnostics_error"] = str(e)
    # Action recommendations computed dynamically.
    recs: list[str] = []
    if not (model_raw.exists() or model_cal.exists()):
        recs.append("generate model artifacts (daily-run or infer) to replace synthetic baselines")
    if not commit_mode:
        recs.append("enable commit mode (NCAAB_COMMIT_PREDICTIONS_MODE=1) for strict parity and to avoid late synthetic shells")
    if pred_path.exists() and payload.get("pred_total_uniform_flag"):
        recs.append("uniform collapse detected; set NCAAB_UNIFORM_STRICT=1 or regenerate predictions")
    if os.getenv("NCAAB_STARTUP_REFRESH", "").strip().lower() not in ("1","true","yes"):
        recs.append("enable startup refresh (NCAAB_STARTUP_REFRESH=1) to auto-heal missing/uniform predictions on cold start")
    payload["action_recommendations"] = recs
    return jsonify(payload)

# ---------------------------------------------------------------
# Calibration & prediction source debug endpoint (remote aid)
# Provides: chosen predictions source file, basis distributions,
# count of CAL-tagged rows, presence of calibrated columns, sample.
# ---------------------------------------------------------------
@app.route("/api/cal-debug")
def cal_debug():
    from flask import jsonify
    import pandas as pd
    result: dict[str, any] = {}
    # Helper: stringify dict keys to avoid JSON sorting TypeErrors when None/NaN present
    def _stringify_keys(d: dict) -> dict:
        out: dict[str, any] = {}
        try:
            import numpy as np  # local import to avoid top-level dependency
        except Exception:
            np = None  # type: ignore
        for k, v in d.items():
            try:
                if k is None:
                    key = "unknown"
                elif np is not None and isinstance(k, float) and np.isnan(k):
                    key = "unknown"
                else:
                    key = str(k)
            except Exception:
                key = "unknown"
            out[key] = v
        return out
    try:
        result['model_predictions_source'] = str(_MODEL_PREDICTIONS_SOURCE_PATH) if '_MODEL_PREDICTIONS_SOURCE_PATH' in globals() else None
    except Exception:
        result['model_predictions_source'] = None
    df = globals().get('last_index_df')
    if isinstance(df, pd.DataFrame) and not df.empty:
        for col in ('pred_total_basis','pred_margin_basis'):
            if col in df.columns:
                try:
                    counts_dict = df[col].value_counts(dropna=False).to_dict()
                    # Sanitize keys: ensure strings and map None/NaN to 'unknown' to prevent jsonify sort errors
                    result[f'{col}_counts'] = _stringify_keys(counts_dict)
                except Exception:
                    result[f'{col}_counts_error'] = True
        if 'pred_total_basis' in df.columns:
            result['cal_rows_total'] = int(df['pred_total_basis'].eq('cal').sum())
        if 'pred_margin_basis' in df.columns:
            result['cal_rows_margin'] = int(df['pred_margin_basis'].eq('cal').sum())
        pred_cols = [c for c in df.columns if c.startswith('pred_total') or c.startswith('pred_margin')]
        result['prediction_columns'] = pred_cols
        sample_cols = [c for c in ['game_id','home_team','away_team','pred_total','pred_total_basis','pred_total_calibrated','pred_margin','pred_margin_basis','pred_margin_calibrated'] if c in df.columns]
        try:
            result['sample'] = df[sample_cols].head(5).to_dict(orient='records') if sample_cols else []
        except Exception:
            result['sample_error'] = True
    else:
        result['no_snapshot'] = True
    ps = globals().get('last_index_pipeline_stats', {})
    if isinstance(ps, dict):
        # Ensure pipeline_stats keys are strings for robust JSON encoding
        try:
            result['pipeline_stats'] = _stringify_keys(ps)
        except Exception:
            result['pipeline_stats'] = ps
    result['has_calibrated_total_col'] = 'pred_total_calibrated' in (df.columns if isinstance(df, pd.DataFrame) else [])
    result['has_calibrated_margin_col'] = 'pred_margin_calibrated' in (df.columns if isinstance(df, pd.DataFrame) else [])
    return jsonify(result)

# --------------------------------------------------------------------------------------
# Ingestion endpoints for deploying local artifacts to Render for parity
# --------------------------------------------------------------------------------------
@app.route("/api/ingest/predictions", methods=["POST"])
def api_ingest_predictions():
    """Ingest a predictions_<date>.csv file to the outputs directory for parity.

    Accepts:
      - multipart/form-data with file field 'file'
      - raw CSV text in body (Content-Type text/csv)
    Optional query/body fields:
      date: ISO date (YYYY-MM-DD). If omitted, attempt to parse from filename or rows; else assume today.

    Requires header 'X-Ingest-Token' matching env NCAAB_INGEST_TOKEN (if set). If token not set, open.
    Returns JSON with write status and md5 hash for verification.
    """
    try:
        if _INGEST_TOKEN:
            tok = request.headers.get("X-Ingest-Token", "").strip()
            if tok != _INGEST_TOKEN:
                return jsonify({"error": "unauthorized"}), 401
    except Exception:
        return jsonify({"error": "auth_check_failed"}), 500
    date_param = (request.args.get("date") or request.form.get("date") or "").strip()
    raw_bytes: bytes | None = None
    filename: str | None = None
    if "file" in request.files:
        f = request.files["file"]
        raw_bytes = f.read()
        filename = f.filename
    else:
        try:
            raw_bytes = request.get_data() or None
        except Exception:
            raw_bytes = None
    if not raw_bytes:
        return jsonify({"error": "no_file"}), 400
    # Decode to text
    try:
        text = raw_bytes.decode("utf-8", errors="replace")
    except Exception:
        return jsonify({"error": "decode_failed"}), 400
    # Attempt to detect date from filename if pattern present
    date_detected = None
    try:
        if filename and "predictions_" in filename and filename.endswith(".csv"):
            stem = Path(filename).stem
            parts = stem.split("_")
            maybe = parts[-1]
            if len(maybe) == 10 and maybe.count("-") == 2:
                date_detected = maybe
    except Exception:
        date_detected = None
    # Fallback: scan first 20 lines for a date column
    if not date_param and not date_detected:
        try:
            head_lines = text.splitlines()[:20]
            if head_lines:
                # naive parse: if header has 'date' and a row contains YYYY-MM-DD use that
                if head_lines[0].lower().split(",").count("date"):
                    for ln in head_lines[1:]:
                        if len(ln) >= 10 and ln[:10].count("-") == 2:
                            date_detected = ln[:10]
                            break
        except Exception:
            pass
    today_iso = None
    try:
        today_iso = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_iso = None
    date_use = date_param or date_detected or today_iso
    if not date_use:
        return jsonify({"error": "no_date_resolved"}), 400
    target = OUT / f"predictions_{date_use}.csv"
    try:
        target.write_text(text, encoding="utf-8")
    except Exception as e:
        return jsonify({"error": f"write_failed: {e}"}), 500
    # Basic integrity: read back & compute md5
    try:
        back = target.read_bytes()
        import hashlib
        md5 = hashlib.md5(back).hexdigest()
        rows = 0
        try:
            import pandas as _pd
            df_tmp = _pd.read_csv(target)
            rows = len(df_tmp)
        except Exception:
            pass
        global _PREDICTIONS_SOURCE_PATH
        _PREDICTIONS_SOURCE_PATH = str(target)
        return jsonify({"ok": True, "path": str(target), "date": date_use, "rows": rows, "md5": md5})
    except Exception as e:
        return jsonify({"error": f"post_read_failed: {e}"}), 500

@app.route("/api/ingest/model-predictions", methods=["POST"])
def api_ingest_model_predictions():
    """Ingest calibrated/raw model predictions artifact to enable server-side promotion.

    Accepts file field 'file'. Determines whether calibrated vs raw by column presence or filename.
    Writes to predictions_model_calibrated_<date>.csv if columns pred_total_calibrated exist else predictions_model_<date>.csv.
    Same auth/token semantics as predictions ingest.
    """
    try:
        if _INGEST_TOKEN:
            tok = request.headers.get("X-Ingest-Token", "").strip()
            if tok != _INGEST_TOKEN:
                return jsonify({"error": "unauthorized"}), 401
    except Exception:
        return jsonify({"error": "auth_check_failed"}), 500
    if "file" not in request.files:
        return jsonify({"error": "no_file"}), 400
    f = request.files["file"]
    raw_bytes = f.read()
    try:
        text = raw_bytes.decode("utf-8", errors="replace")
    except Exception:
        return jsonify({"error": "decode_failed"}), 400
    date_param = (request.args.get("date") or request.form.get("date") or "").strip()
    today_iso = None
    try:
        today_iso = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_iso = None
    date_use = date_param or today_iso
    if not date_use:
        return jsonify({"error": "no_date"}), 400
    # Decide filename by inspecting header
    header = text.splitlines()[0] if text else ""
    calibrated = "pred_total_calibrated" in header or "pred_margin_calibrated" in header
    fname = f"predictions_model_calibrated_{date_use}.csv" if calibrated else f"predictions_model_{date_use}.csv"
    target = OUT / fname
    try:
        target.write_text(text, encoding="utf-8")
    except Exception as e:
        return jsonify({"error": f"write_failed: {e}"}), 500
    # Promote immediately by forcing rebuild logic: delete existing today promotions so next load regenerates
    try:
        promoted = OUT / f"predictions_{date_use}.csv"
        if promoted.exists():
            # Only remove if file looks shell (all NaNs) or user explicitly wants parity refresh
            try:
                import pandas as _pd
                dft = _pd.read_csv(promoted)
                core_cols = [c for c in ["pred_total", "pred_margin"] if c in dft.columns]
                if core_cols and all(_pd.to_numeric(dft[c], errors="coerce").isna().all() for c in core_cols):
                    promoted.unlink(missing_ok=True)  # force regeneration
            except Exception:
                pass
    except Exception:
        pass
    # Read back for md5
    try:
        back = target.read_bytes()
        import hashlib
        md5 = hashlib.md5(back).hexdigest()
        global _MODEL_PREDICTIONS_SOURCE_PATH
        _MODEL_PREDICTIONS_SOURCE_PATH = str(target)
        return jsonify({"ok": True, "path": str(target), "date": date_use, "calibrated": calibrated, "md5": md5})
    except Exception as e:
        return jsonify({"error": f"post_read_failed: {e}"}), 500

def _load_model_predictions(date_str: str | None = None) -> pd.DataFrame:
    """Load model-only prediction outputs produced by inference harness.

    File precedence:
      1) Explicit env var NCAAB_MODEL_PREDICTIONS_FILE (absolute or relative to OUT)
      2) OUT/predictions_model_<date>.csv when date provided
      3) OUT/predictions_model_<today>.csv
      4) Most recently modified OUT/predictions_model_*.csv (fallback)
    Returns empty DataFrame if none found or all empty.
    Sets global _MODEL_PREDICTIONS_SOURCE_PATH for diagnostics.
    Expected columns (if present): game_id, pred_total_model, pred_margin_model, date
    """
    global _MODEL_PREDICTIONS_SOURCE_PATH
    _MODEL_PREDICTIONS_SOURCE_PATH = None
    env_path = (os.getenv("NCAAB_MODEL_PREDICTIONS_FILE") or "").strip()
    today_str = None
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    candidates: list[Path] = []
    # 1) Env override
    if env_path:
        p = Path(env_path)
        if not p.is_absolute():
            p = OUT / env_path
        candidates.append(p)
    # 2) Explicit date (prefer unified -> enriched -> calibrated model -> raw model)
    if date_str:
        candidates.append(OUT / f"predictions_unified_{date_str}.csv")
        candidates.append(OUT / f"predictions_enriched_{date_str}.csv")
        candidates.append(OUT / f"predictions_model_calibrated_{date_str}.csv")
        candidates.append(OUT / f"predictions_model_{date_str}.csv")
    # 3) Today (prefer unified -> enriched -> calibrated model -> raw model)
    if today_str and (not date_str or date_str != today_str):
        candidates.append(OUT / f"predictions_unified_{today_str}.csv")
        candidates.append(OUT / f"predictions_enriched_{today_str}.csv")
        candidates.append(OUT / f"predictions_model_calibrated_{today_str}.csv")
        candidates.append(OUT / f"predictions_model_{today_str}.csv")
    # 4) Any historical model predictions – choose newest non-empty
    try:
        globbed = list(OUT.glob("predictions_model_calibrated_*.csv")) + list(OUT.glob("predictions_model_*.csv"))
        globbed = sorted(globbed, key=lambda p: p.stat().st_mtime if p.exists() else 0, reverse=True)
        for p in globbed:
            if p not in candidates:
                candidates.append(p)
    except Exception:
        pass
    for p in candidates:
        try:
            if p.exists():
                df = pd.read_csv(p)
                if not df.empty:
                    # Unified/enriched files already contain display-ready pred_total / pred_margin and basis columns.
                    # We still populate pred_total_model / pred_margin_model for downstream logic expecting those names.
                    # Preference order for model totals:
                    #   1. Explicit pred_total_model
                    #   2. Calibrated pred_total_calibrated
                    #   3. Display pred_total
                    if 'pred_total_model' not in df.columns:
                        if 'pred_total_calibrated' in df.columns:
                            try:
                                df['pred_total_model'] = pd.to_numeric(df['pred_total_calibrated'], errors='coerce')
                            except Exception:
                                df['pred_total_model'] = df['pred_total_calibrated']
                        elif 'pred_total' in df.columns:
                            df['pred_total_model'] = pd.to_numeric(df['pred_total'], errors='coerce')
                    if 'pred_margin_model' not in df.columns:
                        if 'pred_margin_calibrated' in df.columns:
                            try:
                                df['pred_margin_model'] = pd.to_numeric(df['pred_margin_calibrated'], errors='coerce')
                            except Exception:
                                df['pred_margin_model'] = df['pred_margin_calibrated']
                        elif 'pred_margin' in df.columns:
                            df['pred_margin_model'] = pd.to_numeric(df['pred_margin'], errors='coerce')
                    _MODEL_PREDICTIONS_SOURCE_PATH = str(p)
                    logger.info("Loaded model/unified predictions from: %s (rows=%s, cols=%s)", p, len(df), list(df.columns))
                    return df
        except Exception:
            continue
    logger.info("No model predictions file resolved for date=%s (env=%s)", date_str, env_path)
    return pd.DataFrame()


def _load_csv_first(paths: list[Path]) -> pd.DataFrame:
    """Utility: return first existing CSV as DataFrame else empty."""
    for p in paths:
        try:
            if p.exists():
                df = pd.read_csv(p)
                if not df.empty:
                    return df
        except Exception:
            continue
    return pd.DataFrame()


def _load_stake_sheet(kind: str, date_str: str | None = None) -> pd.DataFrame:
    """Load stake sheet variant.

    kind: one of 'orig','cal','compare'.
    File conventions (today-only for now):
      - stake_sheet_today.csv (original)
      - stake_sheet_today_cal.csv (calibrated)
      - stake_sheet_today_compare.csv (side-by-side deltas)
    """
    # If a date is provided, prefer dated filenames (stake_sheet_YYYY-MM-DD*.csv) with fallbacks to today files.
    candidates: list[Path] = []
    if date_str:
        if kind == "orig":
            candidates.append(OUT / f"stake_sheet_{date_str}.csv")
            candidates.append(OUT / "stake_sheet_today.csv")
        elif kind == "cal":
            candidates.append(OUT / f"stake_sheet_{date_str}_cal.csv")
            candidates.append(OUT / "stake_sheet_today_cal.csv")
        elif kind == "compare":
            candidates.append(OUT / f"stake_sheet_{date_str}_compare.csv")
            candidates.append(OUT / "stake_sheet_today_compare.csv")
    else:
        if kind == "orig":
            candidates.append(OUT / "stake_sheet_today.csv")
        elif kind == "cal":
            candidates.append(OUT / "stake_sheet_today_cal.csv")
        elif kind == "compare":
            candidates.append(OUT / "stake_sheet_today_compare.csv")
    return _load_csv_first(candidates)


def _summarize_stake_sheet(df: pd.DataFrame) -> dict[str, Any]:
    if df.empty:
        return {"n": 0}
    out: dict[str, Any] = {"n": len(df)}
    # Common columns: stake, kelly_fraction, prob, ev
    for col in ["stake","kelly_fraction","prob","ev","price","line","delta_prob","delta_kelly","delta_ev","delta_stake"]:
        if col in df.columns:
            try:
                ser = pd.to_numeric(df[col], errors="coerce")
                out[f"sum_{col}"] = float(ser.dropna().sum())
                out[f"mean_{col}"] = float(ser.dropna().mean())
            except Exception:
                pass
    # Aggregate stake by book if present
    if "book" in df.columns:
        try:
            book_stakes = (
                df.groupby("book")["stake"].sum().sort_values(ascending=False).to_dict()
                if "stake" in df.columns else {}
            )
            out["book_stakes"] = book_stakes
        except Exception:
            pass
    return out


def _load_calibration_artifact() -> dict[str, Any] | None:
    """Load calibration artifact JSON and annotate provisional status.

    Provisional heuristics:
      - rows_used/n_rows/window_rows/calibration_rows < 40 => provisional
      - If no explicit row count and no bucket/bin keys present => provisional
    Attaches: provisional (bool), rows_used_detected (int|None)
    """
    candidates = [OUT / "calibration" / "artifact_totals.json", OUT / "calibration_totals.json"]
    for p in candidates:
        if p.exists():
            try:
                data = json.loads(p.read_text(encoding="utf-8"))
                if not isinstance(data, dict):
                    continue
                rows_used = None
                for key in ("rows_used","n_rows","window_rows","calibration_rows"):
                    v = data.get(key)
                    if isinstance(v, (int, float)):
                        rows_used = int(v)
                        break
                if rows_used is not None:
                    data["rows_used_detected"] = rows_used
                    data["provisional"] = bool(rows_used < 40)
                else:
                    # Detect presence of bucket/bin keys as crude signal of empirical depth
                    has_bins = any(k.lower().startswith("bucket") or k.lower().startswith("bin") for k in data.keys())
                    data["provisional"] = not has_bins
                return data
            except Exception:
                continue
    return None


def _compute_coverage_snapshot() -> dict[str, Any]:
    """Compute basic coverage counts & intersections for games/predictions/odds/picks.

    Returns dict with counts and unmatched ID lists (truncated) for surfacing diagnostics.
    """
    games = _safe_read_csv(OUT / "games_curr.csv")
    preds = _load_predictions_current()
    odds = _safe_read_csv(OUT / "games_with_last.csv")
    picks = _safe_read_csv(OUT / "picks_raw.csv")
    def id_set(df: pd.DataFrame) -> set[str]:
        if df.empty or "game_id" not in df.columns:
            return set()
        try:
            return set(df["game_id"].astype(str).dropna())
        except Exception:
            return set()
    g_ids = id_set(games)
    p_ids = id_set(preds)
    o_ids = id_set(odds)
    k_ids = id_set(picks)
    universe = g_ids | p_ids | o_ids | k_ids
    snap: dict[str, Any] = {
        "n_games": len(g_ids),
        "n_preds": len(p_ids),
        "n_odds": len(o_ids),
        "n_picks": len(k_ids),
        "n_universe": len(universe),
        "intersect_game_pred": len(g_ids & p_ids),
        "intersect_game_odds": len(g_ids & o_ids),
        "intersect_pred_odds": len(p_ids & o_ids),
        "intersect_all": len(g_ids & p_ids & o_ids),
    }
    # Unmatched lists (truncate for UI)
    def trunc(s: set[str]) -> list[str]:
        lim = 40
        arr = sorted(list(s))
        return arr[:lim]
    snap["games_without_preds"] = trunc(g_ids - p_ids)
    snap["preds_without_games"] = trunc(p_ids - g_ids)
    snap["games_without_odds"] = trunc(g_ids - o_ids)
    snap["odds_without_games"] = trunc(o_ids - g_ids)
    snap["preds_without_odds"] = trunc(p_ids - o_ids)
    snap["picks_without_odds"] = trunc(k_ids - o_ids)
    # Basic ratios
    def ratio(a: int, b: int) -> float | None:
        try:
            return round(a / b, 3) if b else None
        except Exception:
            return None
    snap["coverage_pred_vs_games"] = ratio(len(g_ids & p_ids), len(g_ids))
    snap["coverage_odds_vs_games"] = ratio(len(g_ids & o_ids), len(g_ids))
    snap["coverage_picks_vs_preds"] = ratio(len(k_ids & p_ids), len(p_ids))
    # Healed odds-only rows: game_ids starting with odds: present in games
    try:
        healed_odds = {gid for gid in g_ids if gid.startswith("odds:")}
        snap["healed_odds_rows"] = len(healed_odds)
        snap["healed_odds_without_preds"] = sorted(list(healed_odds - p_ids))[:40]
    except Exception:
        snap["healed_odds_rows"] = 0
    return snap


def _load_games_current() -> pd.DataFrame:
    # Prefer fused current day, then seasonal, then historical full sets
    for name in ("games_curr.csv", "games_fused.parquet", "games_fused.csv", "games_hist_fused.csv", "games_all.csv", "games.csv"):
        df = _safe_read_csv(OUT / name)
        if not df.empty:
            return df
    return pd.DataFrame()


def _load_picks() -> pd.DataFrame:
    for name in ("picks_clean.csv", "picks_today.csv", "picks.csv"):
        df = _safe_read_csv(OUT / name)
        if not df.empty:
            return df
    return pd.DataFrame()


def _load_odds_joined(date_str: str | None = None) -> pd.DataFrame:
    """Load per-game odds joined to ESPN game_id when available.

    Preference order (most reliable first):
      - games_with_odds_today_edges.csv (has game_id + per-book quotes + enriched edges)
      - merged_odds_predictions_today.csv (joined pairs with game_id and odds)
      - games_with_odds_today.csv (may be stale or partial but joined)
      - games_with_last.csv (historical last odds join)
      - games_with_closing.csv (heuristic/closing-only join)
    """
    candidate_files: list[Path] = []
    # If viewing a past date, try date-specific odds joins first
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    if date_str and today_str and date_str != today_str:
        for base in (
            f"games_with_odds_{date_str}_edges.csv",
            f"merged_odds_predictions_{date_str}.csv",
            f"games_with_odds_{date_str}.csv",
        ):
            candidate_files.append(OUT / base)
        # Pattern expansion: handle enriched/dist/blend/cal variants produced by pipeline
        try:

            # Declare globals early to avoid Python complaining about prior usage before global statement
            global _PREDICTIONS_SOURCE_PATH, _MODEL_PREDICTIONS_SOURCE_PATH
            # Glob all files starting with games_with_odds_{date_str}
            pattern = f"games_with_odds_{date_str}*.csv"
            for p in sorted(OUT.glob(pattern)):
                if p not in candidate_files:
                    candidate_files.append(p)
        except Exception:
            pass
    # Prefer different priority depending on whether date_str targets today or a past/future date
    if date_str and today_str and date_str != today_str:
        # Past/future date: prefer historical baseline joins before considering today's temp files
        for base in ("games_with_last.csv", "games_with_closing.csv"):
            candidate_files.append(OUT / base)
        # Only consider today's temp joins as a last resort (may not intersect the requested date)
        for base in (
            "games_with_odds_today_edges.csv",
            "merged_odds_predictions_today.csv",
            "games_with_odds_today.csv",
        ):
            candidate_files.append(OUT / base)
    else:
        # Today or unspecified date: try today's temp joins first, then historical baselines
        for base in (
            "games_with_odds_today_edges.csv",
            "merged_odds_predictions_today.csv",
            "games_with_odds_today.csv",
        ):
            candidate_files.append(OUT / base)
        # Include raw odds snapshot as last-resort source for surfacing unmatched games (no join yet)
        candidate_files.append(OUT / "odds_today.csv")
        for base in ("games_with_last.csv", "games_with_closing.csv"):
            candidate_files.append(OUT / base)
    for path in candidate_files:
        df = _safe_read_csv(path)
        if not df.empty:
            return df
    return pd.DataFrame()

def _load_edges() -> pd.DataFrame:
    """Load per-book edge metrics if present (games_with_last_edges.csv)."""
    for name in ("games_with_last_edges.csv", "edges.csv"):
        df = _safe_read_csv(OUT / name)
        if not df.empty:
            return df
    return pd.DataFrame()

def _aggregate_full_game_totals(odds: pd.DataFrame) -> pd.DataFrame:
    """Return per-game aggregated full-game totals odds (median total + list of quotes).

    Output columns: game_id, market_total, quotes (list[dict]), commence_time (earliest)
    """
    if odds.empty or "game_id" not in odds.columns:
        return pd.DataFrame()
    o = odds.copy()
    try:
        o["game_id"] = o["game_id"].astype(str)
    except Exception:
        pass
    # Drop rows with missing/placeholder game_id to avoid spurious 'nan' entries
    try:
        bad_keys = {"nan", "none", "", "null"}
        mask_bad = o["game_id"].astype(str).str.strip().str.lower().isin(bad_keys)
        o = o[~mask_bad]
    except Exception:
        pass
    # Filter to totals + full game periods
    if "market" in o.columns:
        o = o[o["market"].astype(str).str.lower() == "totals"]
    if "period" in o.columns:
        o = o[o["period"].astype(str).str.lower().isin(["full_game", "fg", "full game"])]
    if o.empty:
        return pd.DataFrame()
    # Build quote lists & median totals
    quotes_map: dict[str, list[dict[str, Any]]] = {}
    commence_map: dict[str, Any] = {}
    if "commence_time" in o.columns:
        try:
            o["_commence"] = pd.to_datetime(o["commence_time"], errors="coerce")
        except Exception:
            o["_commence"] = pd.NaT
    else:
        o["_commence"] = pd.NaT
    for gid, g in o.groupby("game_id"):
        # Sort by book for determinism
        g2 = g.sort_values("book") if "book" in g.columns else g
        items: list[dict[str, Any]] = []
        for _, r in g2.iterrows():
            items.append({"book": r.get("book"), "total": r.get("total"), "price_over": r.get("price_over"), "price_under": r.get("price_under")})
            if len(items) >= 10:  # cap length
                break
        quotes_map[str(gid)] = items
        if g2["_commence"].notna().any():
            try:
                commence_map[str(gid)] = g2["_commence"].min().strftime("%Y-%m-%d %H:%M")
            except Exception:
                pass
    med = o.groupby("game_id")["total"].median().rename("market_total")
    out = med.reset_index()
    out["quotes"] = out["game_id"].map(lambda x: quotes_map.get(str(x), []))
    out["commence_time"] = out["game_id"].map(lambda x: commence_map.get(str(x)))
    return out


def _load_accuracy_summary() -> Dict[str, Any] | None:
    """Load accuracy summary JSON from eval directories if present."""
    for p in [OUT / "eval_last2" / "accuracy_summary.json", OUT / "eval" / "accuracy_summary.json"]:
        if p.exists():
            try:
                return json.loads(p.read_text(encoding="utf-8"))
            except Exception:
                continue
    return None


def _load_daily_results_for(date_str: str) -> pd.DataFrame:
    """Load per-day results CSV if it exists: outputs/daily_results/results_YYYY-MM-DD.csv"""
    try:
        p = OUT / "daily_results" / f"results_{date_str}.csv"
        if p.exists():
            return pd.read_csv(p)
    except Exception:
        pass
    return pd.DataFrame()


def _load_schedule_coverage() -> pd.DataFrame:
    """Load per-day schedule coverage counts and anomaly flags if present."""
    p = OUT / "schedule_coverage.csv"
    if p.exists():
        try:
            return pd.read_csv(p)
        except Exception:
            return pd.DataFrame()
    # Fallback to simple day counts
    q = OUT / "schedule_day_counts.csv"
    if q.exists():
        try:
            df = pd.read_csv(q)
            if {"date", "n_games"}.issubset(df.columns):
                if "anomaly" not in df.columns:
                    df["anomaly"] = False
                return df[[c for c in ["date","n_games","anomaly"] if c in df.columns]]
        except Exception:
            return pd.DataFrame()
    return pd.DataFrame()

# -----------------------------
# Quantile selection summary API
# -----------------------------
@app.get('/api/quantile-selection')
def api_quantile_selection():
    try:
        out_dir = ROOT / 'outputs'
        meta_p = out_dir / 'quantile_model_selection.json'
        sel_p = out_dir / 'quantiles_selected.csv'
        result: Dict[str, Any] = {}
        if meta_p.exists():
            with open(meta_p, 'r', encoding='utf-8') as f:
                result['summary'] = _json.load(f)
        else:
            result['summary'] = {}
        if sel_p.exists():
            df = pd.read_csv(sel_p)
            result['count'] = int(len(df))
            # include latest date if present
            if 'date' in df.columns and not df.empty:
                result['latest_date'] = str(sorted(df['date'].astype(str).unique())[-1])
        else:
            result['count'] = 0
        return jsonify({'status':'ok','data': result})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})


# -----------------------------
# Quantile segment metrics API
# -----------------------------
@app.get('/api/quantile-segment-metrics')
def api_quantile_segment_metrics():
    try:
        out_dir = ROOT / 'outputs'
        bt_path = out_dir / 'backtest_reports' / 'backtest_joined.csv'
        qhist_path = out_dir / 'quantiles_history.csv'
        meta_path = out_dir / 'quantile_model_selection.json'
        if not bt_path.exists() or not qhist_path.exists():
            return jsonify({'status': 'missing'})
        bt = pd.read_csv(bt_path)
        qh = pd.read_csv(qhist_path)
        # normalize types
        qh['game_id'] = qh['game_id'].astype(str).str.replace(r'\.0$', '', regex=True)
        if 'game_id' in bt.columns:
            bt['game_id'] = bt['game_id'].astype(str).str.replace(r'\.0$', '', regex=True)
        # choose pred columns
        p_total = 'pred_total_calibrated' if 'pred_total_calibrated' in bt.columns else 'pred_total'
        p_margin = 'pred_margin_calibrated' if 'pred_margin_calibrated' in bt.columns else 'pred_margin'
        if any(c not in bt.columns for c in [p_total, p_margin, 'actual_total', 'actual_margin']):
            return jsonify({'status': 'error', 'message': 'backtest_joined missing required columns'})
        # join
        cols = ['date','game_id', p_total, p_margin, 'actual_total', 'actual_margin']
        j = pd.merge(qh, bt[cols], on=['date','game_id'], how='inner')
        if j.empty:
            return jsonify({'status': 'ok', 'data': {'count': 0}})
        # window by latest date
        latest_date = str(sorted(j['date'].dropna().astype(str).unique())[-1])
        j['date_dt'] = pd.to_datetime(j['date'], errors='coerce')
        ref = pd.to_datetime(latest_date, errors='coerce')
        start = ref - pd.Timedelta(days=28)
        jw = j[(j['date_dt'] >= start) & (j['date_dt'] <= ref)].copy()
        # thresholds from meta
        total_thr = (135.0, 155.0)
        margin_thr = (5.0, 10.0)
        try:
            if meta_path.exists():
                m = json.loads(meta_path.read_text(encoding='utf-8'))
                sr = m.get('segment_rules') or {}
                tt = sr.get('total_thresholds'); mt = sr.get('margin_thresholds')
                if isinstance(tt, (list, tuple)) and len(tt) == 2:
                    total_thr = (float(tt[0]), float(tt[1]))
                if isinstance(mt, (list, tuple)) and len(mt) == 2:
                    margin_thr = (float(mt[0]), float(mt[1]))
        except Exception:
            pass
        # segment labels
        def lab_total(v: float) -> str:
            a,b = total_thr
            try:
                v = float(v)
            except Exception:
                return 'mid'
            if v <= a: return 'low'
            if v <= b: return 'mid'
            return 'high'
        def lab_margin(v: float) -> str:
            a,b = margin_thr
            try:
                v = abs(float(v))
            except Exception:
                return 'med'
            if v <= a: return 'small'
            if v <= b: return 'med'
            return 'large'
        jw['_seg_total'] = jw[p_total].apply(lab_total)
        jw['_seg_margin'] = jw[p_margin].apply(lab_margin)
        # metrics helpers
        def approx_crps_three(y, q10, q50, q90):
            taus = np.linspace(0.05, 0.95, 19)
            def interp3(a,b,c,t):
                if t <= 0.5:
                    m = (b - a) / 0.4
                    return a + (t - 0.1) * m
                m = (c - b) / 0.4
                return b + (t - 0.5) * m
            y = np.asarray(y, float); q10 = np.asarray(q10, float); q50 = np.asarray(q50, float); q90 = np.asarray(q90, float)
            los = []
            for t in taus:
                qt = interp3(q10, q50, q90, t)
                e = y - qt
                los.append(np.nanmean(np.maximum(t * e, (t - 1.0) * e)))
            return float(2.0 * np.nanmean(los))
        def agg(df, kind: str):
            if df.empty:
                return {'n': 0}
            if kind == 'total':
                cov = float(np.nanmean((df['actual_total'] >= df['q10_total']) & (df['actual_total'] <= df['q90_total'])))
                width = float(np.nanmedian(df['q90_total'] - df['q10_total']))
                crps = approx_crps_three(df['actual_total'], df['q10_total'], df['q50_total'], df['q90_total'])
            else:
                cov = float(np.nanmean((df['actual_margin'] >= df['q10_margin']) & (df['actual_margin'] <= df['q90_margin'])))
                width = float(np.nanmedian(df['q90_margin'] - df['q10_margin']))
                crps = approx_crps_three(df['actual_margin'], df['q10_margin'], df['q50_margin'], df['q90_margin'])
            return {'n': int(len(df)), 'coverage': cov, 'width_median': width, 'crps': float(crps)}
        # overall and per segment
        res = {
            'latest_date': latest_date,
            'window_days': 28,
            'thresholds': {'total': total_thr, 'margin': margin_thr},
            'totals': {
                'overall': agg(jw, 'total'),
                'low': agg(jw[jw['_seg_total']=='low'], 'total'),
                'mid': agg(jw[jw['_seg_total']=='mid'], 'total'),
                'high': agg(jw[jw['_seg_total']=='high'], 'total'),
            },
            'margins': {
                'overall': agg(jw, 'margin'),
                'small': agg(jw[jw['_seg_margin']=='small'], 'margin'),
                'med': agg(jw[jw['_seg_margin']=='med'], 'margin'),
                'large': agg(jw[jw['_seg_margin']=='large'], 'margin'),
            }
        }
        # include target coverage if available
        try:
            if meta_path.exists():
                m = json.loads(meta_path.read_text(encoding='utf-8'))
                res['target_coverage'] = float(m.get('target_coverage', 0.8))
        except Exception:
            res['target_coverage'] = 0.8
        return jsonify({'status': 'ok', 'data': res})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})


# -----------------------------
# Quantile segment trend API (8-week)
# -----------------------------
@app.get('/api/quantile-segment-trend')
def api_quantile_segment_trend():
    try:
        out_dir = ROOT / 'outputs'
        bt_path = out_dir / 'backtest_reports' / 'backtest_joined.csv'
        qhist_path = out_dir / 'quantiles_history.csv'
        meta_path = out_dir / 'quantile_model_selection.json'
        if not bt_path.exists() or not qhist_path.exists():
            return jsonify({'status': 'missing'})
        bt = pd.read_csv(bt_path)
        qh = pd.read_csv(qhist_path)
        qh['game_id'] = qh['game_id'].astype(str).str.replace(r'\.0$', '', regex=True)
        if 'game_id' in bt.columns:
            bt['game_id'] = bt['game_id'].astype(str).str.replace(r'\.0$', '', regex=True)
        p_total = 'pred_total_calibrated' if 'pred_total_calibrated' in bt.columns else 'pred_total'
        p_margin = 'pred_margin_calibrated' if 'pred_margin_calibrated' in bt.columns else 'pred_margin'
        cols = ['date','game_id', p_total, p_margin, 'actual_total', 'actual_margin']
        j = pd.merge(qh, bt[cols], on=['date','game_id'], how='inner')
        if j.empty:
            return jsonify({'status': 'ok', 'data': []})
        j['date_dt'] = pd.to_datetime(j['date'], errors='coerce')
        j['week'] = j['date_dt'].dt.to_period('W').astype(str)
        # thresholds from meta
        total_thr = (135.0, 155.0)
        margin_thr = (5.0, 10.0)
        try:
            if meta_path.exists():
                m = json.loads(meta_path.read_text(encoding='utf-8'))
                sr = m.get('segment_rules') or {}
                tt = sr.get('total_thresholds'); mt = sr.get('margin_thresholds')
                if isinstance(tt, (list, tuple)) and len(tt) == 2:
                    total_thr = (float(tt[0]), float(tt[1]))
                if isinstance(mt, (list, tuple)) and len(mt) == 2:
                    margin_thr = (float(mt[0]), float(mt[1]))
        except Exception:
            pass
        def lab_total(v: float) -> str:
            a,b = total_thr
            try: v = float(v)
            except Exception: return 'mid'
            if v <= a: return 'low'
            if v <= b: return 'mid'
            return 'high'
        def lab_margin(v: float) -> str:
            a,b = margin_thr
            try: v = abs(float(v))
            except Exception: return 'med'
            if v <= a: return 'small'
            if v <= b: return 'med'
            return 'large'
        j['_seg_total'] = j[p_total].apply(lab_total)
        j['_seg_margin'] = j[p_margin].apply(lab_margin)
        # aggregate per week per segment
        def agg(df, kind):
            if df.empty:
                return {'n': 0}
            if kind == 'total':
                cov = float(np.nanmean((df['actual_total'] >= df['q10_total']) & (df['actual_total'] <= df['q90_total'])))
                width = float(np.nanmedian(df['q90_total'] - df['q10_total']))
                crps = float(np.nanmean(_approx_crps_from_three(df['actual_total'].to_numpy(float), df['q10_total'].to_numpy(float), df['q50_total'].to_numpy(float), df['q90_total'].to_numpy(float))))
            else:
                cov = float(np.nanmean((df['actual_margin'] >= df['q10_margin']) & (df['actual_margin'] <= df['q90_margin'])))
                width = float(np.nanmedian(df['q90_margin'] - df['q10_margin']))
                crps = float(np.nanmean(_approx_crps_from_three(df['actual_margin'].to_numpy(float), df['q10_margin'].to_numpy(float), df['q50_margin'].to_numpy(float), df['q90_margin'].to_numpy(float))))
            return {'n': int(len(df)), 'coverage': cov, 'width_median': width, 'crps': crps}
        weeks = sorted(j['week'].dropna().unique())
        if len(weeks) > 8:
            weeks = weeks[-8:]
        out = []
        for w in weeks:
            jw = j[j['week'] == w]
            row = {
                'week': w,
                'totals': {
                    'low': agg(jw[jw['_seg_total']=='low'], 'total'),
                    'mid': agg(jw[jw['_seg_total']=='mid'], 'total'),
                    'high': agg(jw[jw['_seg_total']=='high'], 'total'),
                },
                'margins': {
                    'small': agg(jw[jw['_seg_margin']=='small'], 'margin'),
                    'med': agg(jw[jw['_seg_margin']=='med'], 'margin'),
                    'large': agg(jw[jw['_seg_margin']=='large'], 'margin'),
                }
            }
            out.append(row)
        return jsonify({'status': 'ok', 'data': out, 'thresholds': {'total': total_thr, 'margin': margin_thr}})
    except Exception as e:
        return jsonify({'status':'error','message':str(e)})


def _load_branding_map() -> dict[str, dict[str, Any]]:
    """Load team branding: logo URL and colors.

    CSV columns (flexible headers accepted):
      - team (canonical team name)
      - logo (URL or /static path)
      - primary_color, secondary_color, text_color (CSS color strings)

    Returns dict keyed by normalized team name.
    """
    path = settings.data_dir / "team_branding.csv"
    out: dict[str, dict[str, Any]] = {}
    if not path.exists():
        return out
    try:
        df = pd.read_csv(path)
        # Resolve likely column names
        cols = {c.lower().strip(): c for c in df.columns}
        team_col = cols.get("team") or cols.get("name") or cols.get("canonical")
        logo_col = cols.get("logo") or cols.get("logo_url")
        pri_col = cols.get("primary_color") or cols.get("primary")
        sec_col = cols.get("secondary_color") or cols.get("secondary")
        txt_col = cols.get("text_color") or cols.get("text")
        if not team_col:
            return out
        for _, r in df.iterrows():
            team = str(r.get(team_col) or "").strip()
            if not team:
                continue
            key = normalize_name(team)
            out[key] = {
                "logo": r.get(logo_col) if logo_col and pd.notna(r.get(logo_col)) else None,
                "primary": r.get(pri_col) if pri_col and pd.notna(r.get(pri_col)) else None,
                "secondary": r.get(sec_col) if sec_col and pd.notna(r.get(sec_col)) else None,
                "text": r.get(txt_col) if txt_col and pd.notna(r.get(txt_col)) else None,
                "team": team,
            }
    except Exception:
        return {}
    return out


def _load_d1_team_set() -> set[str]:
    """Load normalized Division I team names from data/d1_conferences.csv.

    Returns a set of normalized team names using normalize_name(). If the file
    is missing or unreadable, returns an empty set.
    """
    path = settings.data_dir / "d1_conferences.csv"
    d1set: set[str] = set()
    if not path.exists():
        return d1set
    try:
        df = pd.read_csv(path)
        # Prefer a column named 'team'/'school'/'name', else fallback to first column
        cols = {c.lower().strip(): c for c in df.columns}
        team_col = cols.get("team") or cols.get("school") or cols.get("name") or list(df.columns)[0]
        # Drop comment/header lines beginning with '#'
        df = df[~df[team_col].astype(str).str.strip().str.startswith('#')]
        ser = df[team_col].astype(str).map(normalize_name)
        d1set = set(ser.dropna().astype(str))
    except Exception:
        d1set = set()
    return d1set


def _load_all_finals(limit: int | None = 1000) -> pd.DataFrame:
    """Load all per-day results files and return a consolidated DataFrame of finals.

    Columns include: date, game_id, home_team, away_team, home_score, away_score, actual_total,
    plus optional predicted and market totals when present.
    """
    dr_dir = OUT / "daily_results"
    if not dr_dir.exists():
        return pd.DataFrame()
    files = sorted([p for p in dr_dir.glob("results_*.csv")])
    if not files:
        return pd.DataFrame()
    parts: list[pd.DataFrame] = []
    for p in files:
        try:
            df = pd.read_csv(p)
            # Normalize date from filename if column missing or unreliable
            date_iso = p.stem.split("_")[-1]
            df["date"] = pd.to_datetime(df.get("date", date_iso), errors="coerce").dt.strftime("%Y-%m-%d")
            # Keep key columns
            keep = [
                "date", "game_id", "home_team", "away_team", "home_score", "away_score",
                "actual_total", "pred_total", "closing_total",
            ]
            cols = [c for c in keep if c in df.columns]
            sub = df[cols].copy()
            parts.append(sub)
        except Exception:
            continue
    if not parts:
        return pd.DataFrame()
    out = pd.concat(parts, ignore_index=True)
    # Only rows with completed games
    if "actual_total" in out.columns:
        try:
            out["actual_total"] = pd.to_numeric(out["actual_total"], errors="coerce")
            out = out[out["actual_total"] > 0]
        except Exception:
            pass
    # Sort by date desc then home team
    if "date" in out.columns:
        try:
            out["date"] = pd.to_datetime(out["date"], errors="coerce")
            out = out.sort_values(["date", "home_team"], ascending=[False, True]).reset_index(drop=True)
            out["date"] = out["date"].dt.strftime("%Y-%m-%d")
        except Exception:
            pass
    # Apply optional limit
    if limit is not None and len(out) > int(limit):
        out = out.head(int(limit))
    return out


def _build_results_df(date_str: str, force_use_daily: bool = False) -> tuple[pd.DataFrame, dict[str, Any]]:
    """Construct a per-date results DataFrame similar to index() but lean for API.

    Preference order:
      1. Use daily_results file if it has scores or predictive columns (or forced).
      2. Else build from predictions merged with games metadata for that date.
      3. Else fallback to games-only slate for that date.

    Returns (df, meta) where meta contains summary counts & flags.
    """
    meta: dict[str, Any] = {"date": date_str, "daily_used": False}
    games = _load_games_current()
    preds = _load_predictions_current()
    odds = _load_odds_joined(date_str)
    # Normalize date columns
    for df_ in (games, preds):
        if not df_.empty and "date" in df_.columns:
            try:
                df_["date"] = pd.to_datetime(df_["date"], errors="coerce").dt.strftime("%Y-%m-%d")
            except Exception:
                pass
    # Ensure game_id is consistently string-typed across inputs to avoid merge dtype conflicts
    for df_ in (games, preds):
        try:
            if not df_.empty and "game_id" in df_.columns:
                df_["game_id"] = df_["game_id"].astype(str)
        except Exception:
            pass
    # Filter to date
    if date_str:
        if "date" in games.columns:
            games = games[games["date"].astype(str) == date_str]
        if "date" in preds.columns:
            preds = preds[preds["date"].astype(str) == date_str]
        # Augment games with any odds-only events not present (today only or specified date) so they surface in UI.
        try:
            if not odds.empty and {"home_team_name","away_team_name","commence_time"}.issubset(odds.columns):
                # Build existing unordered canonical pairs set from games
                existing_pairs: set[str] = set()
                if not games.empty and {"home_team","away_team"}.issubset(games.columns):
                    for _, gr in games.iterrows():
                        ht = str(gr.get("home_team") or "").strip()
                        at = str(gr.get("away_team") or "").strip()
                        if ht and at:
                            a = _canon_slug(ht)
                            b = _canon_slug(at)
                            existing_pairs.add("::".join(sorted([a, b])))
                add_rows: list[dict[str, Any]] = []
                # Filter odds rows to target date via commence_time
                try:
                    odds["_commence_dt"] = pd.to_datetime(odds["commence_time"], errors="coerce")
                    odds["_commence_date"] = odds["_commence_dt"].dt.strftime("%Y-%m-%d")
                    o_target = odds[odds["_commence_date"].astype(str) == str(date_str)]
                except Exception:
                    o_target = odds
                for _, r in o_target.iterrows():
                    ht = str(r.get("home_team_name") or "").strip()
                    at = str(r.get("away_team_name") or "").strip()
                    if not ht or not at:
                        continue
                    # Unordered key prevents double-inserting reversed (Away/Home) variants
                    pair_key = "::".join(sorted([_canon_slug(ht), _canon_slug(at)]))
                    if pair_key in existing_pairs:
                        continue  # already present
                    # Construct synthetic game_id (prefix odds:) to avoid collision with ESPN IDs
                    raw_gid = str(r.get("game_id") or "").strip()
                    synth_gid = f"odds:{raw_gid}" if raw_gid else f"odds:{pair_key}:{date_str}"
                    start_time = None
                    try:
                        dt_val = pd.to_datetime(r.get("commence_time"), errors="coerce")
                        if pd.notna(dt_val):
                            start_time = dt_val.strftime("%Y-%m-%d %H:%M")
                    except Exception:
                        pass
                    add_rows.append({
                        "game_id": synth_gid,
                        "home_team": ht,
                        "away_team": at,
                        "date": date_str,
                        "start_time": start_time,
                        "status": "scheduled",
                    })
                if add_rows:
                    aug = pd.DataFrame(add_rows)
                    # Concatenate and ensure uniqueness by game_id
                    games = pd.concat([games, aug], ignore_index=True)
                    try:
                        games = games.drop_duplicates(subset=["game_id"], keep="first")
                    except Exception:
                        pass
                    logger.info("Augmented games with %s odds-only events (date=%s)", len(add_rows), date_str)
                    # Optional persistence (auto-heal) to games_curr.csv so downstream predictions can reference
                    try:
                        auto_heal = os.getenv("NCAAB_AUTO_HEAL_GAMES", "").strip().lower() in ("1","true","yes")
                        if auto_heal:
                            curr_path = OUT / "games_curr.csv"
                            if curr_path.exists():
                                try:
                                    g_curr = pd.read_csv(curr_path)
                                except Exception:
                                    g_curr = pd.DataFrame()
                            else:
                                g_curr = pd.DataFrame()
                            # Normalize date if present
                            if not g_curr.empty and "date" in g_curr.columns:
                                try:
                                    g_curr["date"] = pd.to_datetime(g_curr["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                                except Exception:
                                    pass
                            healed = pd.concat([g_curr, aug], ignore_index=True)
                            try:
                                healed = healed.drop_duplicates(subset=["game_id"], keep="first")
                            except Exception:
                                pass
                            healed.to_csv(curr_path, index=False)
                            logger.info("Persisted auto-heal of %s odds-only events to %s (env NCAAB_AUTO_HEAL_GAMES)", len(add_rows), curr_path)
                    except Exception as _heal_e:
                        logger.warning("Auto-heal persistence failed: %s", _heal_e)
        except Exception as _e:
            logger.warning("Odds-based game augmentation failed: %s", _e)
    daily_df = _load_daily_results_for(date_str)
    if not daily_df.empty:
        has_scores = False
        has_preds = False
        try:
            if {"home_score","away_score"}.issubset(daily_df.columns):
                sc_sum = pd.to_numeric(daily_df["home_score"], errors="coerce") + pd.to_numeric(daily_df["away_score"], errors="coerce")
                has_scores = bool((sc_sum > 0).any())
            for c in ("pred_total","pred_margin","market_total","closing_total"):
                if c in daily_df.columns and daily_df[c].notna().any():
                    has_preds = True
                    break
        except Exception:
            has_scores = False
        # Past-date retention: keep placeholder daily rows even if no scores/preds
        is_past = False
        try:
            if date_str:
                is_past = dt.date.fromisoformat(date_str) < _today_local()
        except Exception:
            is_past = False
        if force_use_daily or has_scores or has_preds or is_past:
            df = daily_df.copy()
            meta["daily_used"] = True
            meta["results_pending"] = (not has_scores)
        else:
            df = pd.DataFrame()
    else:
        df = pd.DataFrame()
    # Build upcoming slate if daily unused / empty
    if df.empty:
        if not preds.empty:
            df = preds.copy()
            if not games.empty and "game_id" in preds.columns and "game_id" in games.columns:
                try:
                    preds["game_id"] = preds["game_id"].astype(str)
                except Exception:
                    pass
                try:
                    games["game_id"] = games["game_id"].astype(str)
                except Exception:
                    pass
                keep = [c for c in ["game_id","home_team","away_team","home_score","away_score","start_time","status","date"] if c in games.columns]
                df = df.merge(games[keep], on="game_id", how="left", suffixes=("","_g"))
        elif not games.empty:
            base_cols = [c for c in ["game_id","home_team","away_team","home_score","away_score","start_time","status","date"] if c in games.columns]
            df = games[base_cols].copy()
        meta["results_pending"] = True

    # Odds-only fallback: if still empty but odds contain rows for the requested date, build a minimal slate from odds
    try:
        if df.empty and not odds.empty and date_str:
            o = odds.copy()
            # Filter odds to totals, full-game, and target date if possible
            if "market" in o.columns:
                o = o[o["market"].astype(str).str.lower() == "totals"]
            if "period" in o.columns:
                vals = o["period"].astype(str).str.lower()
                o = o[vals.isin(["full_game","fg","full game","game","match"]) | vals.isna()]
            # Date filter via commence_time/date_line if available
            if "commence_time" in o.columns:
                try:
                    o["_commence_date"] = pd.to_datetime(o["commence_time"], errors="coerce").dt.strftime("%Y-%m-%d")
                    o = o[o["_commence_date"] == str(date_str)]
                except Exception:
                    pass
            elif "date_line" in o.columns:
                o = o[o["date_line"].astype(str) == str(date_str)]
            if not o.empty:
                # Prefer grouping by game_id when present, else by normalized team pair
                grp_key = "game_id" if "game_id" in o.columns else None
                rows: list[dict[str, Any]] = []
                if grp_key:
                    o["game_id"] = o["game_id"].astype(str)
                    for gid, g in o.groupby("game_id"):
                        r = {
                            "game_id": str(gid),
                            "home_team": g.get("home_team").dropna().astype(str).iloc[0] if "home_team" in g.columns and g["home_team"].notna().any() else None,
                            "away_team": g.get("away_team").dropna().astype(str).iloc[0] if "away_team" in g.columns and g["away_team"].notna().any() else None,
                            "start_time": None,
                            "pred_total": None,
                            "pred_margin": None,
                            "market_total": pd.to_numeric(g.get("total"), errors="coerce").median() if "total" in g.columns else None,
                            "date": str(date_str),
                        }
                        # earliest commence_time if available
                        if "commence_time" in g.columns:
                            try:
                                t = pd.to_datetime(g["commence_time"], errors="coerce").min()
                                r["start_time"] = t.strftime("%Y-%m-%d %H:%M") if pd.notna(t) else None
                            except Exception:
                                pass
                        rows.append(r)
                    o2 = o.dropna(subset=["_pair_key"]) if "_pair_key" in o.columns else pd.DataFrame()
                    if not o2.empty:
                        for pk, g in o2.groupby("_pair_key"):
                            # reconstruct teams from first row; create surrogate game_id
                            hn = g.get("_home_norm").dropna().astype(str).iloc[0] if "_home_norm" in g.columns and g["_home_norm"].notna().any() else None
                            an = g.get("_away_norm").dropna().astype(str).iloc[0] if "_away_norm" in g.columns and g["_away_norm"].notna().any() else None
                            ht = g.get("home_team").dropna().astype(str).iloc[0] if "home_team" in g.columns and g["home_team"].notna().any() else hn
                            at = g.get("away_team").dropna().astype(str).iloc[0] if "away_team" in g.columns and g["away_team"].notna().any() else an
                            gid = f"{date_str}:{an}:{hn}" if an and hn else f"{date_str}:{pk}"
                            r = {
                                "game_id": gid,
                                "home_team": ht,
                                "away_team": at,
                                "start_time": None,
                                "pred_total": None,
                                "pred_margin": None,
                                "market_total": pd.to_numeric(g.get("total"), errors="coerce").median() if "total" in g.columns else None,
                                "date": str(date_str),
                            }
                            if "commence_time" in g.columns:
                                try:
                                    t = pd.to_datetime(g["commence_time"], errors="coerce").min()
                                    r["start_time"] = t.strftime("%Y-%m-%d %H:%M") if pd.notna(t) else None
                                except Exception:
                                    pass
                            rows.append(r)
                if rows:
                    df = pd.DataFrame(rows)
                    results_note = f"Odds-only slate for {date_str} (no games/predictions available)"
                    meta["results_pending"] = True
                    # Surface the constructed odds-only slate rationale in meta for API consumers
                    meta["results_note"] = results_note
    except Exception:
        pass
    # Compute actual_total if scores present
    if {"home_score","away_score"}.issubset(df.columns):
        try:
            hs = pd.to_numeric(df["home_score"], errors="coerce")
            as_ = pd.to_numeric(df["away_score"], errors="coerce")
            df["actual_total"] = hs + as_
            df["actual_margin"] = hs - as_
        except Exception:
            pass
    # Attach market_total median from odds when missing
        try:
            o = odds.copy()
            o["game_id"] = o["game_id"].astype(str)
            if "market" in o.columns:
                o = o[o["market"].astype(str).str.lower() == "totals"]
            if "period" in o.columns:
                vals = o["period"].astype(str).str.lower()
                o = o[vals.isin(["full_game","fg","full game","game","match"]) | vals.isna()]
            if "total" in o.columns:
                med = o.groupby("game_id")["total"].median().rename("market_total")
                df = df.merge(med, on="game_id", how="left") if "game_id" in df.columns else df
        except Exception:
            pass
    # Edge vs market_total if predictions available
    if {"pred_total","market_total"}.issubset(df.columns):
        try:
            df["edge_total"] = pd.to_numeric(df["pred_total"], errors="coerce") - pd.to_numeric(df["market_total"], errors="coerce")
        except Exception:
            df["edge_total"] = None
    # Quotes count badge support: derive quotes_count if quotes column present
    try:
        if "quotes" in df.columns and "quotes_count" not in df.columns:
            def _qc(val):
                try:
                    if isinstance(val, list):
                        return len(val)
                    if isinstance(val, str) and val.strip().startswith("["):
                        import json
                        arr = json.loads(val)
                        return len(arr) if isinstance(arr, list) else None
                    return None
                except Exception:
                    return None
            df["quotes_count"] = df["quotes"].map(_qc)
            qc_valid = pd.to_numeric(df["quotes_count"], errors="coerce").dropna()
            if not qc_valid.empty:
                meta["quotes_count_median"] = float(qc_valid.median())
                meta["quotes_count_min"] = float(qc_valid.min())
                meta["quotes_count_max"] = float(qc_valid.max())
    except Exception:
        pass
    # Synthesize missing odds for rows with complete predictions but no market_total
    try:
        if not df.empty:
            need_mt = ('market_total' not in df.columns) or df['market_total'].isna()
            if isinstance(need_mt, pd.Series) and need_mt.any():
                src = None
                for cand in ('pred_total_calibrated','pred_total_model_unified','pred_total_model','derived_total','pred_total'):
                    if cand in df.columns:
                        tmp = pd.to_numeric(df[cand], errors='coerce')
                        if tmp.notna().any():
                            src = tmp
                            break
                if src is not None:
                    typical = 148.0
                    mt_syn = 0.85 * src + 0.15 * typical
                    df.loc[need_mt, 'market_total'] = mt_syn[need_mt]
                    df['market_basis'] = df.get('market_basis')
                    df['market_basis'] = df['market_basis'].where(df['market_basis'].notna(), 'synthetic_pred_inferred')
    except Exception:
        pass
    # Basic meta stats
    meta["n_rows"] = len(df)
    meta["n_rows"] = len(df)
    if "actual_total" in df.columns:
        try:
            at = pd.to_numeric(df["actual_total"], errors="coerce")
            meta["n_finals"] = int((at > 0).sum())
            meta["n_pending"] = int(len(df) - (at > 0).sum())
            meta["all_final"] = bool(len(df) > 0 and meta["n_finals"] == len(df))
        except Exception:
            meta["n_finals"] = 0
            meta["n_pending"] = len(df)
            meta["all_final"] = False
    else:
        meta["n_finals"] = 0
        meta["n_pending"] = len(df)
        meta["all_final"] = False
    meta["columns"] = list(df.columns)
    return df, meta


@app.route("/")
def index():
    # Optional date filter (?date=YYYY-MM-DD)
    date_q = (request.args.get("date") or "").strip()
    force_use_daily = (request.args.get("use_daily") or "").strip() in ("1","true","yes")
    # Optional view preference: force-calibrated display precedence even for tiny diffs
    prefer_cal_param = (request.args.get("prefer_cal") or "").strip().lower() in ("1","true","yes")
    prefer_cal_eff = prefer_cal_param
    # Compact card mode toggle
    try:
        q_compact = (request.args.get("compact") or "").strip().lower()
        compact_mode = True if q_compact == "" else (q_compact in ("1","true","yes"))
    except Exception:
        compact_mode = True
    # Probability calibration enable flag (?cal_probs=1 or env CALIBRATE_PROBS=1)
    try:
        calibrate_probs_param = (request.args.get("cal_probs") or "").strip().lower() in ("1","true","yes")
    except Exception:
        calibrate_probs_param = False
    try:
        calibrate_probs_env = str(os.getenv("CALIBRATE_PROBS","0")).strip().lower() in ("1","true","yes")
    except Exception:
        calibrate_probs_env = False
    calibrate_probs_enabled = calibrate_probs_param or calibrate_probs_env

    # Declare globals early for prediction source path tracking and stats caching
    global _PREDICTIONS_SOURCE_PATH, _MODEL_PREDICTIONS_SOURCE_PATH, _LAST_PIPELINE_STATS

    # Initialize pipeline_stats dict early (used by many downstream blocks) if not already.
    if 'pipeline_stats' not in locals():
        pipeline_stats = {}

    # Record the effective display timezone, preferring end-user provided values
    try:
        pipeline_stats["display_tz_used"] = _get_display_tz_name()
    except Exception:
        pass

    # Define today string once for consistent downstream comparisons
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None

    games = _load_games_current()
    preds = _load_predictions_current()
    odds = _load_odds_joined(date_q)
    model_preds = _load_model_predictions(date_q if date_q else None)
    # Initial diagnostics snapshot before filtering
    diag_enabled = (request.args.get("diag") or "").strip().lower() in ("1","true","yes")
    pipeline_stats: dict[str, Any] = {
        "date_param": date_q,
        "games_load_rows": len(games),
        "preds_load_rows": len(preds),
        "odds_load_rows": len(odds),
        "model_preds_load_rows": len(model_preds),
        "outputs_dir": str(OUT),
        "prefer_cal": prefer_cal_param,
        "prefer_cal_effective": prefer_cal_eff,
        "prob_calibrate_enabled": calibrate_probs_enabled,
    }
    # Stable display mode: render directly from persisted display artifact to ensure parity with remote.
    # Enabled via query param ?stable=1 or env STABLE_DISPLAY=1 (default on if env set).
    try:
        stable_q = (request.args.get("stable") or "").strip().lower() in ("1","true","yes")
    except Exception:
        stable_q = False
    try:
        import os as _os_mod
        stable_env = str(_os_mod.environ.get("STABLE_DISPLAY","0")).strip().lower() in ("1","true","yes")
    except Exception:
        stable_env = False
    # Force stable display for rollover-affected dates to ensure consistent times
    force_stable_dates = set([str((dt.datetime.utcnow()).strftime('%Y-%m-%d'))])
    if today_str:
        force_stable_dates.add(str(today_str))
    # Also include the requested date if present and known problematic
    known_rollover_dates = {"2025-11-29", "2025-12-02"}
    stable_mode = stable_q or stable_env or (date_q in known_rollover_dates)
    if stable_mode:
        try:
            # Resolve date
            date_stable = date_q if date_q else (today_str or dt.datetime.utcnow().strftime('%Y-%m-%d'))
            path = OUT / f'predictions_display_{date_stable}.csv'
            if path.exists():
                try:
                    df_stable = pd.read_csv(path)
                except Exception:
                    df_stable = pd.DataFrame()
            else:
                df_stable = getattr(app, 'last_index_df', pd.DataFrame())
                if not df_stable.empty:
                    _persist_display(df_stable, date_stable)
            # Enrich stable frame with odds commence_time for authority alignment
            if isinstance(odds, pd.DataFrame) and not odds.empty and 'game_id' in odds.columns and 'game_id' in df_stable.columns:
                try:
                    o2 = odds.copy()
                    o2['game_id'] = o2['game_id'].astype(str)
                    if 'commence_time' in o2.columns:
                        # pick earliest commence per game_id
                        o2['_ct'] = pd.to_datetime(o2['commence_time'], errors='coerce')
                        ct_map = o2.groupby('game_id')['_ct'].min()
                        df_stable['game_id'] = df_stable['game_id'].astype(str)
                        # Only add commence_time if not present or missing
                        if 'commence_time' in df_stable.columns:
                            mask_missing_ct = df_stable['commence_time'].isna() | df_stable['commence_time'].astype(str).str.strip().eq("")
                            if mask_missing_ct.any():
                                df_stable.loc[mask_missing_ct, 'commence_time'] = df_stable.loc[mask_missing_ct, 'game_id'].map(ct_map)
                        else:
                            df_stable['commence_time'] = df_stable['game_id'].map(ct_map)
                except Exception:
                    pass
            # Fallback mapping by unordered team pair when game_id differs (e.g., ESPN id vs provider id)
            o_cols1 = {'home_team_name','away_team_name','commence_time'}
            o_cols2 = {'home_team','away_team','commence_time'}
            if isinstance(odds, pd.DataFrame) and not odds.empty and (o_cols1.issubset(odds.columns) or o_cols2.issubset(odds.columns)) and {'home_team','away_team'}.issubset(df_stable.columns):
                try:
                    o3 = odds.copy()
                    def _slug(x):
                        try:
                            return _canon_slug(str(x))
                        except Exception:
                            return normalize_name(str(x))
                    hn_col = 'home_team_name' if 'home_team_name' in o3.columns else 'home_team'
                    an_col = 'away_team_name' if 'away_team_name' in o3.columns else 'away_team'
                    o3['_hn'] = o3[hn_col].map(_slug)
                    o3['_an'] = o3[an_col].map(_slug)
                    o3['_pair'] = o3.apply(lambda r: '::'.join(sorted([r['_hn'], r['_an']])), axis=1)
                    o3['_ct'] = pd.to_datetime(o3['commence_time'], errors='coerce')
                    ct_pair = o3.groupby('_pair')['_ct'].min()
                    df_stable['_hn'] = df_stable['home_team'].map(_slug)
                    df_stable['_an'] = df_stable['away_team'].map(_slug)
                    df_stable['_pair'] = df_stable.apply(lambda r: '::'.join(sorted([str(r.get('_hn') or ''), str(r.get('_an') or '')])), axis=1)
                    if 'commence_time' in df_stable.columns:
                        missing_ct2 = df_stable['commence_time'].isna() | df_stable['commence_time'].astype(str).str.strip().eq("")
                        if missing_ct2.any():
                            df_stable.loc[missing_ct2, 'commence_time'] = df_stable.loc[missing_ct2, '_pair'].map(ct_pair)
                    else:
                        df_stable['commence_time'] = df_stable['_pair'].map(ct_pair)
                except Exception:
                    pass
            # For stable mode, trust the pre-enriched display CSV for
            # all time/date fields and avoid recomputing them here.
            # We still allow _normalize_display to run for non-time
            # cosmetic/diagnostic fields.
            try:
                df_stable = _normalize_display(df_stable)
            except Exception:
                pass
            # Filter stable display by its own display_date/date columns
            # instead of re-deriving from UTC.
            try:
                if isinstance(df_stable, pd.DataFrame) and not df_stable.empty:
                    target_date = str(date_stable) if date_stable else None
                    if target_date:
                        if 'display_date' in df_stable.columns:
                            df_stable = df_stable[df_stable['display_date'].astype(str) == target_date]
                        elif 'date' in df_stable.columns:
                            df_stable = df_stable[df_stable['date'].astype(str) == target_date]
            except Exception:
                pipeline_stats['stable_display_filter_error'] = True
            # Build rows for template without mutating time/date fields
            # that were already finalized in the display artifact.
            try:
                df_tpl = df_stable.where(pd.notna(df_stable), None)
            except Exception:
                df_tpl = df_stable
            raw_rows = [_brand_row(r) for r in df_tpl.to_dict(orient("records"))]
            rows: list[dict[str, Any]] = [dict(_r) for _r in raw_rows]
            total_rows = len(rows)
            # Minimal coverage summary (totals/spreads/ML presence)
            coverage_summary = {"full": 0, "partial": 0, "none": 0}
            try:
                if "game_id" in df_tpl.columns:
                    for _, r in df_tpl.iterrows():
                        has_total = (r.get("market_total") is not None) or (r.get("closing_total") is not None)
                        has_spread = (r.get("spread_home") is not None) or (r.get("closing_spread_home") is not None)
                        has_ml = (r.get("ml_home") is not None)
                        sig = int(has_total) + int(has_spread) + int(has_ml)
                        if sig >= 2:
                            coverage_summary["full"] += 1
                        elif sig == 1:
                            coverage_summary["partial"] += 1
                        else:
                            coverage_summary["none"] += 1
            except Exception:
                pass
            # Diagnostics
            pipeline_stats['stable_display_mode'] = True
            pipeline_stats['stable_display_path'] = str(path)
            pipeline_stats['stable_display_rows'] = total_rows
            accuracy = _load_accuracy_summary()
            archive_dates = []
            try:
                pat = _re_mod.compile(r'^predictions_display_(\d{4}-\d{2}-\d{2})\.csv$')
                for p in OUT.glob('predictions_display_*.csv'):
                    m = pat.match(p.name)
                    if m:
                        archive_dates.append(m.group(1))
                archive_dates = sorted(set(archive_dates))
            except Exception:
                archive_dates = []
            # Early return rendering from stable artifact
            return render_template(
                "index.html",
                rows=rows,
                total_rows=total_rows,
                date_val=date_stable,
                top_picks=[],
                accuracy=accuracy,
                uniform_note=None,
                dynamic_css=None,
                coverage_note=None,
                results_note=None,
                show_edges=True,
                coverage=coverage_summary,
                archive_dates=archive_dates,
                show_bootstrap=False,
                bootstrap_url=None,
                show_diag=False,
                diag_url=None,
                fused_bootstrap_url=None,
                refresh_odds_url=None,
            )
        except Exception as _stable_e:
            pipeline_stats['stable_display_error'] = str(_stable_e)[:160]
    team_variance_total: dict[str, float] | None = None
    team_variance_margin: dict[str, float] | None = None
    # Backtest metrics ingestion (totals/spread/moneyline) if precomputed JSON exists
    # Expected file: outputs/backtest_metrics_<date>.json produced by daily_backtest script.
    try:
        bt_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        bt_path = OUT / f'backtest_metrics_{bt_date}.json'
        if bt_path.exists():
            import json as _json
            bt_payload = _json.loads(bt_path.read_text(encoding='utf-8'))
            pipeline_stats['backtest_ingested'] = True
            pipeline_stats['backtest_date'] = bt_payload.get('date')
            pipeline_stats['backtest_generated_at'] = bt_payload.get('generated_at')
            def _lift(prefix: str, obj: Any):
                if isinstance(obj, dict):
                    for k,v in obj.items():
                        # Avoid extremely large nested structures; only primitive scalars retained
                        if isinstance(v, (int,float,str)) or v is None:
                            pipeline_stats[f'{prefix}_{k}'] = v
            for key in ('totals_closing','spread_closing','moneyline_closing'):
                if key in bt_payload:
                    _lift(key, bt_payload[key])
        else:
            pipeline_stats['backtest_ingested'] = False
    except Exception:
        pipeline_stats['backtest_error'] = True
    # Meta backtest ingestion (ATS/OU/ML) from outputs/logs/meta_backtest_summary.json
    try:
        mb_sum_path = OUT / 'logs' / 'meta_backtest_summary.json'
        if mb_sum_path.exists():
            import json as _json
            mb = _json.loads(mb_sum_path.read_text(encoding='utf-8'))
            g = mb.get('global', {})
            def _put_global(prefix: str, d: Any):
                if isinstance(d, dict):
                    for k, v in d.items():
                        if isinstance(v, (int, float, str)) or v is None:
                            pipeline_stats[f'meta_{prefix}_{k}'] = v
            _put_global('ats', g.get('ats', {}))
            _put_global('totals', g.get('totals', {}))
            _put_global('ml', g.get('ml', {}))
            pipeline_stats['meta_backtest_ingested'] = True
            # Lift by-date metrics for the selected/today date if present
            sel_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
            bd = mb.get('by_date', {})
            if isinstance(bd, dict) and sel_date in bd:
                for k, v in bd[sel_date].items():
                    if isinstance(v, (int, float, str)) or v is None:
                        pipeline_stats[f'meta_by_date_{k}'] = v
                pipeline_stats['meta_by_date'] = sel_date
        else:
            pipeline_stats['meta_backtest_ingested'] = False
    except Exception:
        pipeline_stats['meta_backtest_error'] = True
    # Proper scoring rules (CRPS / log-likelihood) ingestion if JSON exists
    try:
        scoring_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        scoring_path = OUT / f'scoring_{scoring_date}.json'
        if scoring_path.exists():
            import json as _json
            sc_payload = _json.loads(scoring_path.read_text(encoding='utf-8'))
            for k in [
                'totals_crps_mean','totals_loglik_mean','margins_crps_mean','margins_loglik_mean',
                'totals_rows','margins_rows','sigma_total_default_used','sigma_margin_default_used'
            ]:
                if k in sc_payload:
                    pipeline_stats[f'scoring_{k}'] = sc_payload[k]
            pipeline_stats['scoring_loaded'] = True
        else:
            pipeline_stats['scoring_loaded'] = False
    except Exception:
        pipeline_stats['scoring_error'] = True
    # Residual distribution ingestion (totals/margins) if JSON exists
    try:
        resid_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        resid_path = OUT / f'residuals_{resid_date}.json'
        if resid_path.exists():
            import json as _json
            r_payload = _json.loads(resid_path.read_text(encoding='utf-8'))
            pipeline_stats['residuals_ingested'] = True
            def _lift_res(prefix: str, obj: Any):
                if isinstance(obj, dict):
                    for k,v in obj.items():
                        if isinstance(v,(int,float,str)) or v is None:
                            pipeline_stats[f'{prefix}_{k}'] = v
            if 'total_stats' in r_payload: _lift_res('resid_total', r_payload['total_stats'])
            if 'margin_stats' in r_payload: _lift_res('resid_margin', r_payload['margin_stats'])
            for key in ('total_corr','margin_corr'):
                if key in r_payload and isinstance(r_payload[key], (int,float)):
                    pipeline_stats[key] = r_payload[key]
            if 'status' in r_payload:
                pipeline_stats['residuals_status'] = r_payload['status']
        else:
            pipeline_stats['residuals_ingested'] = False
    except Exception:
        pipeline_stats['residuals_error'] = True
    # Quantile residual model ingestion (totals/margins) from outputs/quantile_model.json
    try:
        qm_path = OUT / 'quantile_model.json'
        if qm_path.exists():
            import json as _json
            qm_payload = _json.loads(qm_path.read_text(encoding='utf-8'))
            rq = qm_payload.get('residual_quantiles', {}) if isinstance(qm_payload, dict) else {}
            tot = rq.get('total', {}) if isinstance(rq, dict) else {}
            mar = rq.get('margin', {}) if isinstance(rq, dict) else {}
            def _lift_quant(prefix: str, obj: Any):
                if isinstance(obj, dict):
                    for k, v in obj.items():
                        if isinstance(v, (int, float)):
                            pipeline_stats[f'{prefix}_{k}'] = float(v)
            _lift_quant('quantile_total', tot)
            _lift_quant('quantile_margin', mar)
            for k in ('n_total','n_margin','window_days','source'):
                if k in qm_payload:
                    pipeline_stats[f'quantile_model_{k}'] = qm_payload[k]
            pipeline_stats['quantile_model_loaded'] = True
        else:
            pipeline_stats['quantile_model_loaded'] = False
    except Exception:
        pipeline_stats['quantile_model_error'] = True
    # Team variance ingestion for adaptive sigma scaling
    try:
        tv_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        tv_path = OUT / f'team_variance_{tv_date}.json'
        if tv_path.exists():
            import json as _json
            tv_payload = _json.loads(tv_path.read_text(encoding='utf-8'))
            teams_block = tv_payload.get('teams', {}) if isinstance(tv_payload, dict) else {}
            # Build maps for later per-row application
            team_variance_total = {k: v.get('total_std') for k,v in teams_block.items() if isinstance(v, dict) and v.get('total_std') is not None}
            team_variance_margin = {k: v.get('margin_std') for k,v in teams_block.items() if isinstance(v, dict) and v.get('margin_std') is not None}
            pipeline_stats['team_variance_ingested'] = True
            pipeline_stats['team_variance_teams'] = len(teams_block)
            pipeline_stats['team_variance_total_std_median'] = tv_payload.get('global', {}).get('total_std_median')
            pipeline_stats['team_variance_margin_std_median'] = tv_payload.get('global', {}).get('margin_std_median')
        else:
            pipeline_stats['team_variance_ingested'] = False
    except Exception:
        pipeline_stats['team_variance_error'] = True
    # Recalibration trigger ingestion (evaluates drift/correlation/scoring vs thresholds)
    try:
        rc_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        rc_path = OUT / f'recalibration_{rc_date}.json'
        if rc_path.exists():
            import json as _json
            rc_payload = _json.loads(rc_path.read_text(encoding='utf-8'))
            pipeline_stats['recalibration_ingested'] = True
            pipeline_stats['recalibration_needed'] = rc_payload.get('recalibration_needed')
            pipeline_stats['recalibration_reasons'] = rc_payload.get('reasons')
            metrics_block = rc_payload.get('metrics') if isinstance(rc_payload, dict) else {}
            if isinstance(metrics_block, dict):
                for k,v in metrics_block.items():
                    if isinstance(v, (int,float,str)) or v is None:
                        pipeline_stats[f'recalib_{k}'] = v
        else:
            pipeline_stats['recalibration_ingested'] = False
    except Exception:
        pipeline_stats['recalibration_error'] = True
    # Leakage scan ingestion (suspicious feature columns)
    try:
        leak_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        leak_path = OUT / f'leakage_{leak_date}.json'
        if leak_path.exists():
            import json as _json
            leak_payload = _json.loads(leak_path.read_text(encoding='utf-8'))
            pipeline_stats['leakage_ingested'] = True
            pipeline_stats['leakage_suspicious_cols'] = int(len(leak_payload.get('suspicious_columns', [])))
            if 'summary' in leak_payload and isinstance(leak_payload['summary'], dict):
                for k,v in leak_payload['summary'].items():
                    if isinstance(v,(int,float,str)):
                        pipeline_stats[f'leakage_summary_{k}'] = v
        else:
            pipeline_stats['leakage_ingested'] = False
    except Exception:
        pipeline_stats['leakage_error'] = True
    # Conference fairness ingestion
    try:
        fair_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        fair_path = OUT / f'fairness_{fair_date}.json'
        if fair_path.exists():
            import json as _json
            fair_payload = _json.loads(fair_path.read_text(encoding='utf-8'))
            pipeline_stats['fairness_ingested'] = True
            if 'global' in fair_payload and isinstance(fair_payload['global'], dict):
                for k,v in fair_payload['global'].items():
                    if isinstance(v,(int,float,str)):
                        pipeline_stats[f'fairness_global_{k}'] = v
            records = fair_payload.get('records', [])
            pipeline_stats['fairness_conferences_evaluated'] = int(len(records)) if isinstance(records, list) else 0
            if isinstance(records, list) and records:
                # extreme disparities
                try:
                    max_disp = max((abs(r.get('disparity_z_total')) for r in records if isinstance(r, dict) and r.get('disparity_z_total') is not None), default=None)
                    pipeline_stats['fairness_disparity_max_abs'] = max_disp
                except Exception:
                    pass
                try:
                    max_bias = max((abs(r.get('mean_residual_total')) for r in records if isinstance(r, dict) and r.get('mean_residual_total') is not None), default=None)
                    pipeline_stats['fairness_bias_max_abs'] = max_bias
                except Exception:
                    pass
        else:
            pipeline_stats['fairness_ingested'] = False
    except Exception:
        pipeline_stats['fairness_error'] = True
    # Predictability evaluation ingestion
    try:
        pred_eval_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        pred_eval_path = OUT / f'predictability_{pred_eval_date}.json'
        if pred_eval_path.exists():
            import json as _json
            pe_payload = _json.loads(pred_eval_path.read_text(encoding='utf-8'))
            pipeline_stats['predictability_ingested'] = True
            keep_keys = [
                'residual_mean','residual_std','residual_mae','calibration_slope_total','calibration_intercept_total',
                'corr_pred_market_total','coverage_rows','predictability_score','trailing_residual_std','trailing_residual_mae','trailing_calibration_slope_total'
            ]
            for k in keep_keys:
                if k in pe_payload:
                    pipeline_stats[f'predictability_{k}'] = pe_payload.get(k)
        else:
            pipeline_stats['predictability_ingested'] = False
    except Exception:
        pipeline_stats['predictability_error'] = True
    # Feature importance ingestion
    try:
        imp_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        imp_path = OUT / f'importance_{imp_date}.json'
        if imp_path.exists():
            import json as _json
            imp_payload = _json.loads(imp_path.read_text(encoding='utf-8'))
            pipeline_stats['importance_ingested'] = True
            # Capture top 8 features for totals and margin if present
            if 'totals' in imp_payload and isinstance(imp_payload['totals'], dict):
                feats = imp_payload['totals'].get('features', [])
                if isinstance(feats, list) and feats:
                    top_totals = [f.get('feature') for f in feats[:8] if isinstance(f, dict)]
                    pipeline_stats['importance_totals_top'] = top_totals
            if 'margin' in imp_payload and isinstance(imp_payload['margin'], dict):
                feats = imp_payload['margin'].get('features', [])
                if isinstance(feats, list) and feats:
                    top_margin = [f.get('feature') for f in feats[:8] if isinstance(f, dict)]
                    pipeline_stats['importance_margin_top'] = top_margin
        else:
            pipeline_stats['importance_ingested'] = False
    except Exception:
        pipeline_stats['importance_error'] = True
    # Segment performance ingestion
    try:
        seg_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        seg_path = OUT / f'segment_{seg_date}.json'
        if seg_path.exists():
            import json as _json
            seg_payload = _json.loads(seg_path.read_text(encoding='utf-8'))
            pipeline_stats['segment_ingested'] = True
            segs = seg_payload.get('segments', {}) if isinstance(seg_payload, dict) else {}
            if isinstance(segs, dict):
                # Extract key residual means for representative buckets
                for bucket in ['tempo::Q1_slowest','tempo::Q4_fastest','spread::0_2','spread::9_plus']:
                    if bucket in segs and isinstance(segs[bucket], dict):
                        val = segs[bucket].get('mean_residual_total')
                        pipeline_stats[f"segment_{bucket.replace('::','_')}_mean_residual"] = val
        else:
            pipeline_stats['segment_ingested'] = False
    except Exception:
        pipeline_stats['segment_error'] = True
    # Reliability calibration ingestion
    try:
        rel_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        rel_path = OUT / f'reliability_{rel_date}.json'
        if rel_path.exists():
            import json as _json
            rel_payload = _json.loads(rel_path.read_text(encoding='utf-8'))
            pipeline_stats['reliability_ingested'] = True
            for k in ['calibration_slope','calibration_intercept','rows']:
                if k in rel_payload:
                    pipeline_stats[f'reliability_{k}'] = rel_payload.get(k)
        else:
            pipeline_stats['reliability_ingested'] = False
        # Season evaluation ingestion (SOT reliability badges: ECE, Brier, Log-loss)
        try:
            season_eval = {}
            p = OUT / 'backtest_reports' / 'season_eval_summary.json'
            if p.exists():
                import json
                with open(p, 'r', encoding='utf-8') as f:
                    season_eval = json.load(f) or {}
                pipeline_stats['season_eval_loaded'] = True
                ou = season_eval.get('probabilities', {}).get('over', {})
                ats = season_eval.get('probabilities', {}).get('cover', {})
                if ou:
                    pipeline_stats['ou_n'] = int(ou.get('n', 0))
                    pipeline_stats['ou_brier'] = float(ou.get('brier', float('nan')))
                    pipeline_stats['ou_log_loss'] = float(ou.get('log_loss', float('nan')))
                    if 'ece' in ou:
                        pipeline_stats['ou_ece'] = float(ou.get('ece', float('nan')))
                    pipeline_stats['ou_prob_column_used'] = str(ou.get('prob_column_used', ''))
                if ats:
                    pipeline_stats['ats_n'] = int(ats.get('n', 0))
                    pipeline_stats['ats_brier'] = float(ats.get('brier', float('nan')))
                    pipeline_stats['ats_log_loss'] = float(ats.get('log_loss', float('nan')))
                    if 'ece' in ats:
                        pipeline_stats['ats_ece'] = float(ats.get('ece', float('nan')))
                    pipeline_stats['ats_prob_column_used'] = str(ats.get('prob_column_used', ''))
        except Exception:
            pipeline_stats['season_eval_loaded_error'] = True
    except Exception:
        pipeline_stats['reliability_error'] = True
    # Reliability trend ingestion (rolling window metrics)
    try:
        trend_path = OUT / 'reliability_trend.json'
        if trend_path.exists():
            import json as _json_rt
            trend_payload = _json_rt.loads(trend_path.read_text(encoding='utf-8'))
            if isinstance(trend_payload, list) and trend_payload:
                last = trend_payload[-1]
                for k in ['ece_total_roll7','ece_margin_roll7','sharpness_total_roll7','sharpness_margin_roll7']:
                    v = last.get(k)
                    if isinstance(v, (int,float)):
                        pipeline_stats[k] = float(v)
                pipeline_stats['reliability_trend_rows'] = len(trend_payload)
        else:
            pipeline_stats['reliability_trend_missing'] = True
    except Exception:
        pipeline_stats['reliability_trend_error'] = True
    # Daily performance aggregation ingestion (composite health metrics)
    try:
        perf_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        perf_path = OUT / f'performance_{perf_date}.json'
        if perf_path.exists():
            import json as _json
            perf_payload = _json.loads(perf_path.read_text(encoding='utf-8'))
            pipeline_stats['performance_ingested'] = True
            for k in [
                'model_health','predictability_score','recalibration_needed','fairness_bias_flag','fairness_disparity_flag',
                'leakage_suspicious_cols','total_mean','margin_mean','total_corr','margin_corr','total_mean_z','margin_mean_z','total_corr_z','margin_corr_z'
            ]:
                if k in perf_payload:
                    pipeline_stats[f'perf_{k}'] = perf_payload.get(k)
            for k in ['predictability_score','residual_std','crps_total','total_mean','total_corr']:
                for w in ['7','14']:
                    keyname = f'{k}_trailing_{w}'
                    if keyname in perf_payload:
                        pipeline_stats[f'perf_{keyname}'] = perf_payload.get(keyname)
        else:
            pipeline_stats['performance_ingested'] = False
    except Exception:
        pipeline_stats['performance_error'] = True
    # Season metrics ingestion for rolling health context
    try:
        season_path = OUT / 'season_metrics.json'
        if season_path.exists():
            import json as _json
            season_payload = _json.loads(season_path.read_text(encoding='utf-8'))
            pipeline_stats['season_metrics_ingested'] = True
            summary_block = season_payload.get('summary', {}) if isinstance(season_payload, dict) else {}
            for k in [
                'residual_std_mean','predictability_score_mean','crps_mean_mean','totals_residual_std_mean','margin_residual_std_mean'
            ]:
                if k in summary_block:
                    pipeline_stats[f'season_{k}'] = summary_block.get(k)
        else:
            pipeline_stats['season_metrics_ingested'] = False
    except Exception:
        pipeline_stats['season_metrics_error'] = True
    # Meta probability stacking metrics ingestion (features used / performance)
    try:
        meta_metrics_path = OUT / 'meta_probs_metrics.json'
        if meta_metrics_path.exists():
            pipeline_stats['meta_probs_metrics'] = _json.loads(meta_metrics_path.read_text(encoding='utf-8'))
        else:
            pipeline_stats['meta_probs_metrics_missing'] = True
    except Exception:
        pipeline_stats['meta_probs_metrics_error'] = True
    # Probability stability divergence ingestion
    try:
        from datetime import datetime as _dt
        _today_iso = _dt.utcnow().strftime('%Y-%m-%d')
        stability_path = OUT / f'prob_stability_{_today_iso}.json'
        if not stability_path.exists():
            # fallback: latest file
            cand = sorted(OUT.glob('prob_stability_*.json'))[-1:] if list(OUT.glob('prob_stability_*.json')) else []
            stability_path = cand[0] if cand else stability_path
        if stability_path.exists():
            pipeline_stats['probability_stability'] = _json.loads(stability_path.read_text(encoding='utf-8'))
        else:
            pipeline_stats['probability_stability_missing'] = True
    except Exception:
        pipeline_stats['probability_stability_error'] = True
    # Auto calibration refresh artifact ingestion
    try:
        auto_cal_path = sorted(OUT.glob('auto_refresh_calibration_*.json'))[-1:] if list(OUT.glob('auto_refresh_calibration_*.json')) else []
        if auto_cal_path:
            pipeline_stats['auto_refresh_calibration'] = _json.loads(auto_cal_path[0].read_text(encoding='utf-8'))
        else:
            pipeline_stats['auto_refresh_calibration_missing'] = True
    except Exception:
        pipeline_stats['auto_refresh_calibration_error'] = True
    # Drift/Bias diagnostics integration (load precomputed JSON if present for today or selected date)
    try:
        drift_date = date_q if date_q else _today_local().strftime('%Y-%m-%d')
        drift_path = OUT / f'drift_bias_{drift_date}.json'
        if not drift_path.exists():
            # Fallback: if requesting past date before diagnostics existed, ignore silently
            # Attempt today's file when browsing without date param
            if not date_q:
                alt_path = OUT / f'drift_bias_{_today_local().strftime("%Y-%m-%d")}.json'
                if alt_path.exists():
                    drift_path = alt_path
        if drift_path.exists():
            import json as _json
            payload = _json.loads(drift_path.read_text(encoding='utf-8'))
            # Whitelist expected keys to avoid large blob
            for k in [
                'totals_bias','pace_drift','trailing_mean_pred_total','today_mean_pred_total',
                'conference_margin_bias','source_rows'
            ]:
                if k in payload:
                    pipeline_stats[f'drift_bias_{k}'] = payload[k]
            pipeline_stats['drift_bias_loaded'] = True
        else:
            pipeline_stats['drift_bias_loaded'] = False
    except Exception:
        pipeline_stats['drift_bias_error'] = True
    # Instrumentation: capture early model predictions stats after pipeline_stats exists
    try:
        if not model_preds.empty:
            pipeline_stats['model_preds_source'] = _MODEL_PREDICTIONS_SOURCE_PATH
            pipeline_stats['model_preds_cols'] = list(model_preds.columns)
            if 'pred_total_model' in model_preds.columns:
                ptm_early = pd.to_numeric(model_preds['pred_total_model'], errors='coerce')
                pipeline_stats['model_preds_total_stats_early'] = {
                    'count': int(ptm_early.notna().sum()),
                    'min': float(ptm_early.min()) if ptm_early.notna().any() else None,
                    'max': float(ptm_early.max()) if ptm_early.notna().any() else None,
                    'mean': float(ptm_early.mean()) if ptm_early.notna().any() else None,
                    'std': float(ptm_early.std()) if ptm_early.notna().any() else None,
                    'unique': int(ptm_early.nunique()) if ptm_early.notna().any() else 0
                }
                pipeline_stats['model_preds_total_head'] = ptm_early.head(10).tolist()
            if 'pred_total_calibrated' in model_preds.columns:
                ptc_early = pd.to_numeric(model_preds['pred_total_calibrated'], errors='coerce')
                pipeline_stats['model_preds_total_calibrated_head'] = ptc_early.head(10).tolist()
                pipeline_stats['model_preds_total_calibrated_stats_early'] = {
                    'count': int(ptc_early.notna().sum()),
                    'min': float(ptc_early.min()) if ptc_early.notna().any() else None,
                    'max': float(ptc_early.max()) if ptc_early.notna().any() else None,
                    'mean': float(ptc_early.mean()) if ptc_early.notna().any() else None,
                    'std': float(ptc_early.std()) if ptc_early.notna().any() else None,
                    'unique': int(ptc_early.nunique()) if ptc_early.notna().any() else 0
                }
                # Preferred initial source determination (calibrated when any non-NaN values present)
                if ptc_early.notna().any():
                    pipeline_stats['model_preds_preferred_initial_source'] = 'calibrated'
                else:
                    pipeline_stats['model_preds_preferred_initial_source'] = 'raw_model'
            else:
                pipeline_stats['model_preds_preferred_initial_source'] = 'raw_model'
            if 'pred_margin_model' in model_preds.columns:
                pmm_early = pd.to_numeric(model_preds['pred_margin_model'], errors='coerce')
                pipeline_stats['model_preds_margin_head'] = pmm_early.head(10).tolist()
    except Exception:
        pass
    # Attach source paths if available
    try:
        pipeline_stats["predictions_source"] = _PREDICTIONS_SOURCE_PATH
    except Exception:
        pipeline_stats["predictions_source"] = None
    try:
        pipeline_stats["model_predictions_source"] = _MODEL_PREDICTIONS_SOURCE_PATH
    except Exception:
        pipeline_stats["model_predictions_source"] = None
    try:
        # odds loader can expose attribute _ODDS_SOURCE_PATH similarly if implemented; attempt best-effort detection
        odds_source = None
        for cand in [OUT/"odds_today.csv", OUT/"odds_curr.csv", OUT/"odds_" + str(date_q) + ".csv", OUT/"games_with_last.csv"]:
            try:
                if hasattr(cand, "exists") and cand.exists():
                    # heuristic: if shape matches odds df row count within tolerance, pick
                    if not odds.empty:
                        # allow mismatch; we still record first existing for visibility
                        odds_source = str(cand)
                        break
            except Exception:
                continue
        pipeline_stats["odds_source_guess"] = odds_source
    except Exception:
        pipeline_stats["odds_source_guess"] = None
    logger.info("Index request start date=%s games_cols=%s preds_cols=%s odds_cols=%s", date_q, list(games.columns), list(preds.columns), list(odds.columns))
    # Initialize coverage summary; will refine after merges to reflect coalesced closing lines
    coverage_summary = {"full": 0, "partial": 0, "none": 0}
    edges = _load_edges()

    if not preds.empty:
        preds["game_id"] = preds["game_id"].astype(str)
    if not games.empty and "game_id" in games.columns:
        games["game_id"] = games["game_id"].astype(str)

    # Normalize date strings
    for df_ in (preds, games):
        if not df_.empty and "date" in df_.columns:
            try:
                df_["date"] = pd.to_datetime(df_["date"]).dt.strftime("%Y-%m-%d")
            except Exception:
                pass

    # If no date provided, prefer today if there are any games for today or any in-progress/live games
    if not date_q:
        today_str = _today_local().strftime("%Y-%m-%d")
        has_today = False
        has_live = False
        try:
            if "date" in games.columns:
                has_today = (games["date"].astype(str) == today_str).any()
            if "status" in games.columns:
                st = games["status"].astype(str).str.lower()
                has_live = st.str.contains("in").any() | st.str.contains("live").any()
        except Exception:
            pass
        # Consider odds presence as a signal that today has a slate even if games/preds are missing
        has_today_odds = False
        try:
            if not odds.empty:
                # commence_time preferred
                if "commence_time" in odds.columns:
                    ct = pd.to_datetime(odds["commence_time"], errors="coerce").dt.strftime("%Y-%m-%d")
                    has_today_odds = (ct == today_str).any()
                # fallback to date_line or _day_diff markers
                if not has_today_odds and "date_line" in odds.columns:
                    has_today_odds = (odds["date_line"].astype(str) == today_str).any()
                if not has_today_odds and "_day_diff" in odds.columns:
                    try:
                        has_today_odds = (pd.to_numeric(odds["_day_diff"], errors="coerce") == 0).any()
                    except Exception:
                        pass
        except Exception:
            has_today_odds = False
        if has_today or has_live or has_today_odds:
            date_q = today_str
        else:
            # Fallback to latest date seen in predictions or games
            if "date" in preds.columns and not preds.empty and preds["date"].notna().any():
                try:
                    date_q = pd.to_datetime(preds["date"]).max().strftime("%Y-%m-%d")
                except Exception:
                    date_q = (preds["date"].dropna().astype(str).max())
            elif "date" in games.columns and not games.empty and games["date"].notna().any():
                try:
                    date_q = pd.to_datetime(games["date"]).max().strftime("%Y-%m-%d")
                except Exception:
                    date_q = (games["date"].dropna().astype(str).max())
            else:
                # As a last resort (e.g., on Render with no games/preds), use the latest daily_results date if present
                try:
                    dr_dir = OUT / "daily_results"
                    if dr_dir.exists():
                        dates: list[str] = []
                        for p in dr_dir.glob("results_*.csv"):
                            stem = p.stem
                            if stem.startswith("results_"):
                                dates.append(stem.replace("results_", ""))
                        if dates:
                            # Pick max date string safely
                            try:
                                date_q = pd.to_datetime(pd.Series(dates)).max().strftime("%Y-%m-%d")
                            except Exception:
                                date_q = sorted(dates)[-1]
                except Exception:
                    pass

    # Apply date filter (prefer to filter predictions if they carry date; else games)
    # Keep originals for fallback when showing live/today slates
    games_all = games.copy()
    preds_all = preds.copy()
    if date_q:
        if "date" in preds.columns:
            preds = preds[preds["date"] == date_q]
        if "date" in games.columns:
            filtered = games[games["date"] == date_q]
            # If targeting today but no rows due to timezone skew, fallback to any live rows regardless of date
            if filtered.empty:
                try:
                    today_str = _today_local().strftime("%Y-%m-%d")
                    st = games_all.get("status")
                    if st is not None:
                        stl = st.astype(str).str.lower()
                        live_mask = stl.str.contains("in") | stl.str.contains("live")
                        live_df = games_all[live_mask]
                        if not live_df.empty and date_q == today_str:
                            games = live_df
                        else:
                            games = filtered
                    else:
                        games = filtered
                except Exception:
                    games = filtered
            else:
                games = filtered
        # Direct date-specific fallback: if games empty for requested date, attempt to load games_<date>.csv
        if games.empty:
            try:
                date_specific = OUT / f"games_{date_q}.csv"
                if date_specific.exists():
                    gdf = pd.read_csv(date_specific)
                    if not gdf.empty:
                        if "date" in gdf.columns:
                            try:
                                gdf["date"] = pd.to_datetime(gdf["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                            except Exception:
                                pass
                            gdf = gdf[gdf["date"].astype(str) == str(date_q)]
                        if "game_id" in gdf.columns:
                            gdf["game_id"] = gdf["game_id"].astype(str)
                        games = gdf
            except Exception:
                pass

    # If daily_results exist for the date, prefer it only if games have real scores (>0) or explicit predictions;
    # for future/upcoming slates daily_results may be a placeholder with 0 scores and missing preds.
    daily_df = _load_daily_results_for(date_q) if date_q else pd.DataFrame()
    pipeline_stats["games_after_date"] = len(games)
    pipeline_stats["preds_after_date"] = len(preds)
    pipeline_stats["daily_results_rows"] = len(daily_df)
    # Zero-games reason (pre-synthetic) recorded now; may be updated later if synthetic schedule built
    if pipeline_stats["games_after_date"] == 0:
        if pipeline_stats.get("games_load_rows", 0) == 0:
            pipeline_stats["zero_games_reason"] = "no_games_loaded"
        else:
            pipeline_stats["zero_games_reason"] = "date_filter_eliminated_all"
    daily_used = False
    results_note = None
    if not daily_df.empty:
        try:
            has_scores = False
            if {"home_score","away_score"}.issubset(daily_df.columns):
                sc_sum = pd.to_numeric(daily_df["home_score"], errors="coerce") + pd.to_numeric(daily_df["away_score"], errors="coerce")
                has_scores = bool((sc_sum > 0).any())
            has_preds = False
            for c in ("pred_total","pred_margin","market_total","closing_total"):
                if c in daily_df.columns and daily_df[c].notna().any():
                    has_preds = True
                    break
            date_obj: dt.date | None = None
            try:
                date_obj = dt.date.fromisoformat(date_q)
            except Exception:
                date_obj = None
            if not has_scores and not has_preds and not force_use_daily:
                # Relax discard: if the date is in the past retain placeholder rows so user can see slate even without preds/scores.
                if date_obj and date_obj < _today_local():
                    daily_used = True
                    results_note = f"No scores/preds captured for {date_q}; retaining placeholder daily slate."  # past date placeholder
                else:
                    daily_df = pd.DataFrame()
            else:
                daily_used = True
                if not has_scores:
                    results_note = f"Results pending for {date_q}: scores not yet ingested; showing schedule/predictions."
                elif has_scores and force_use_daily:
                    results_note = f"Showing daily results for {date_q}."
        except Exception:
            daily_df = pd.DataFrame()

    # ------------------------------------------------------------------
    # Synthetic games fallback: if we have ZERO games rows for the target date
    # but we do have predictions or odds, synthesize a minimal games frame so
    # that cards can render instead of showing an empty slate.
    # ------------------------------------------------------------------
    if games.empty:
        fallback_reason: list[str] = []
        synth_df = pd.DataFrame()
        # Prefer predictions for richer team naming
        try:
            if not preds.empty and "game_id" in preds.columns:
                # Candidate columns already present
                if {"home_team","away_team"}.issubset(preds.columns):
                    cols = [c for c in ["game_id","home_team","away_team","date","start_time"] if c in preds.columns]
                    synth_df = preds[cols].drop_duplicates("game_id")
                    fallback_reason.append("preds_home_away_direct")
                else:
                    # Attempt reconstruction from team/opponent + home_away flag
                    if {"team","opponent"}.issubset(preds.columns):
                        ha_col = None
                        for cand in ["home_away","homeaway","is_home"]:
                            if cand in preds.columns:
                                ha_col = cand
                                break
                        if ha_col:
                            ha = preds[ha_col].astype(str).str.lower()
                            home_mask = ha.isin(["home","h","1","true","t"])
                            away_mask = ha.isin(["away","a","0","false","f"])
                            home_side = preds[home_mask][[c for c in ["game_id","team"] if c in preds.columns]].rename(columns={"team":"home_team"})
                            away_side = preds[away_mask][[c for c in ["game_id","team"] if c in preds.columns]].rename(columns={"team":"away_team"})
                            if not home_side.empty and not away_side.empty:
                                merged = home_side.merge(away_side, on="game_id", how="inner")
                                if "date" in preds.columns:
                                    merged = merged.merge(preds[["game_id","date"]].drop_duplicates(), on="game_id", how="left")
                                synth_df = merged.drop_duplicates("game_id")
                                fallback_reason.append("preds_reconstructed_home_away")
                        # If still empty, last resort: pair team/opponent per row (may duplicate, but we dedupe game_id)
                        if synth_df.empty:
                            tmp = preds.copy()
                            if {"team","opponent"}.issubset(tmp.columns):
                                tmp = tmp.rename(columns={"team":"home_team","opponent":"away_team"})
                                cols = [c for c in ["game_id","home_team","away_team","date","start_time"] if c in tmp.columns]
                                synth_df = tmp[cols].drop_duplicates("game_id")
                                fallback_reason.append("preds_team_opponent_pairing")
        except Exception:
            pass
        # If predictions path failed, attempt odds-based synthesis
        if synth_df.empty:
            try:
                if not odds.empty and {"game_id","home_team","away_team"}.issubset(odds.columns):
                    cols = [c for c in ["game_id","home_team","away_team","date_line","commence_time"] if c in odds.columns]
                    o2 = odds[cols].drop_duplicates("game_id")
                    # Normalize date column name to "date"
                    if "date_line" in o2.columns:
                        o2 = o2.rename(columns={"date_line":"date"})
                    if "commence_time" in o2.columns and "start_time" not in o2.columns:
                        o2 = o2.rename(columns={"commence_time":"start_time"})
                    synth_df = o2
                    fallback_reason.append("odds_game_rows")
            except Exception:
                pass
        # Constrain to requested date if a date column exists
        if not synth_df.empty and date_q and "date" in synth_df.columns:
            try:
                synth_df["date"] = pd.to_datetime(synth_df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                synth_df = synth_df[synth_df["date"].astype(str) == str(date_q)]
            except Exception:
                pass
        if not synth_df.empty:
            try:
                synth_df["game_id"] = synth_df["game_id"].astype(str)
            except Exception:
                pass
            games = synth_df.copy()
            pipeline_stats["games_synthetic"] = len(games)
            pipeline_stats["games_fallback_reason"] = fallback_reason
            if results_note:
                results_note += " | Synthetic schedule constructed (" + ",".join(fallback_reason) + ")"
            else:
                results_note = "Synthetic schedule constructed (" + ",".join(fallback_reason) + ")"
        else:
            pipeline_stats["games_synthetic"] = 0
            if not fallback_reason:
                pipeline_stats["games_fallback_reason"] = ["none_available"]
            else:
                pipeline_stats["games_fallback_reason"] = fallback_reason or ["attempted_no_rows"]

    # Merge predictions with game metadata
    if not daily_df.empty:
        # Normalize shape to what the template expects
        df = daily_df.copy()
        # Backfill missing/zero scores from master games file for past dates
        try:
            date_obj = dt.date.fromisoformat(date_q) if date_q else None
        except Exception:
            date_obj = None
        if date_obj and date_obj < _today_local():
            try:
                master_path = OUT / "games_all.csv"
                if master_path.exists() and "game_id" in df.columns:
                    master = pd.read_csv(master_path, usecols=[c for c in [
                        "game_id","date","home_score","away_score","status",
                        "home_score_1h","away_score_1h","home_score_2h","away_score_2h"
                    ] if c])
                    master["game_id"] = master["game_id"].astype(str)
                    if "date" in master.columns:
                        master["date"] = pd.to_datetime(master["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                        master = master[master["date"] == str(date_q)]
                    df = df.merge(master.drop(columns=[c for c in ["date"] if c in master.columns]), on="game_id", how="left", suffixes=("", "_m"))
                    # Coalesce scores: prefer existing (>0) else master (>0)
                    for side in ("home","away"):
                        prov = pd.to_numeric(df.get(f"{side}_score"), errors="coerce")
                        mast = pd.to_numeric(df.get(f"{side}_score_m"), errors="coerce")
                        df[f"{side}_score"] = np.where((prov>0) | prov.notna(), df.get(f"{side}_score"), mast)
                    # Coalesce halftime and 2H scores when present in master
                    for half_col in ["home_score_1h","away_score_1h","home_score_2h","away_score_2h"]:
                        if half_col in df.columns or f"{half_col}_m" in df.columns:
                            cur = pd.to_numeric(df.get(half_col), errors="coerce") if half_col in df.columns else pd.Series(np.nan, index=df.index)
                            mval = pd.to_numeric(df.get(f"{half_col}_m"), errors="coerce") if f"{half_col}_m" in df.columns else pd.Series(np.nan, index=df.index)
                            df[half_col] = np.where(cur.notna(), cur, mval)
                    if "status_m" in df.columns:
                        df["status"] = df.get("status").where(df.get("status").notna(), df.get("status_m"))
            except Exception:
                pass
        # If fused historical file present, attempt to enrich missing start_time or scores from it
        try:
            fused_path = OUT / "games_hist_fused.csv"
            if fused_path.exists() and "game_id" in df.columns:
                fused_df = pd.read_csv(fused_path, usecols=["game_id","start_time"])
                fused_df["game_id"] = fused_df["game_id"].astype(str)
                df = df.merge(fused_df, on="game_id", how="left", suffixes=("", "_fused"))
                if "start_time" not in df.columns or df["start_time"].isna().all():
                    df["start_time"] = df.get("start_time_fused")
                df = df.drop(columns=[c for c in ["start_time_fused"] if c in df.columns])
        except Exception:
            pass
        # Enrich with games metadata for commence_time and venue/city/state if available
        try:
            if not games.empty and "game_id" in games.columns and "game_id" in df.columns:
                g2 = games.copy()
                g2["game_id"] = g2["game_id"].astype(str)
                keep = [c for c in [
                    "game_id", "start_time", "commence_time",
                    "venue", "venue_full", "arena", "stadium", "location", "site", "site_name", "city", "state"
                ] if c in g2.columns]
                if keep:
                    df = df.merge(g2[keep], on="game_id", how="left", suffixes=("", "_g"))
        except Exception:
            pass
        # Ensure actual_total is present/coalesced for all rows when scores are available
        try:
            if {"home_score","away_score"}.issubset(df.columns):
                hs = pd.to_numeric(df["home_score"], errors="coerce")
                aw = pd.to_numeric(df["away_score"], errors="coerce")
                if "actual_total" not in df.columns:
                    df["actual_total"] = np.where(hs.notna() & aw.notna(), hs + aw, np.nan)
                else:
                    mask = df["actual_total"].isna()
                    df.loc[mask, "actual_total"] = hs[mask] + aw[mask]
        except Exception:
            pass
        # Pick best available start_time among candidates (prefer one that contains a time component)
        try:
            cands = ["start_time", "start_time_g", "commence_time", "commence_time_g"]
            have = [c for c in cands if c in df.columns]
            if have:
                def _pick_start(row):
                    vals = []
                    for c in have:
                        v = row.get(c)
                        if pd.notna(v) and str(v).strip() != "":
                            vals.append(str(v).strip())
                    if not vals:
                        return None
                    # Prefer any with a time component
                    for v in vals:
                        if ":" in v:
                            return v
                    return vals[0]
                df["start_time"] = df.apply(_pick_start, axis=1)
        except Exception:
            pass
        if "game_id" in df.columns:
            try:
                df["game_id"] = df["game_id"].astype(str)
            except Exception:
                pass
        if "date" in df.columns:
            try:
                df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
            except Exception:
                pass
        # Market total: prefer closing_total when present; coalesce NaNs as well
        if "closing_total" in df.columns:
            if "market_total" not in df.columns:
                df["market_total"] = df["closing_total"]
            else:
                try:
                    df["market_total"] = df["market_total"].where(df["market_total"].notna(), df["closing_total"])
                except Exception:
                    pass
    else:
        # Build from preds + games (older behavior) to show upcoming slates
        if preds.empty and not games.empty:
            # No predictions for this date: show games-only slate
            base_cols = [c for c in ["game_id","date","home_team","away_team","home_score","away_score","start_time"] if c in games.columns]
            df = games[base_cols].copy()
            # Ensure required columns exist
            for c in ["pred_total","pred_margin"]:
                if c not in df.columns:
                    df[c] = None
        else:
            df = preds.copy()
            if not games.empty:
                # Try to pick best team columns available
                home_cols = [c for c in ["home_team", "home_team_name", "home"] if c in games.columns]
                away_cols = [c for c in ["away_team", "away_team_name", "away"] if c in games.columns]
                keep = [c for c in [
                    "game_id", "date", "status", "home_score", "away_score", "home_score_1h", "away_score_1h",
                    "start_time", "commence_time", "neutral_site",
                    # venue/location candidates
                    "venue", "venue_full", "arena", "stadium", "location", "site", "site_name", "city", "state"
                ] if c in games.columns]
                keep += (home_cols[:1] or []) + (away_cols[:1] or [])
                # Use suffixes so left (preds) keeps original names, right (games) gets _g on collisions
                df = df.merge(games[keep], on="game_id", how="left", suffixes=("", "_g"))

                # ------------------------------------------------------------------
                # Augment: ensure games without predictions are still shown.
                # Previous behavior dropped games lacking prediction rows because
                # we started from preds and performed a left merge. For partial
                # slates (some preds missing) important matchups disappeared.
                # We now append minimal rows for any games on the requested date
                # that are absent from the predictions set so the UI renders them.
                # ------------------------------------------------------------------
                try:
                    if "game_id" in games.columns and "game_id" in df.columns:
                        # Restrict to target date if date column present
                        if date_q and "date" in games.columns:
                            games_for_date = games[games["date"].astype(str) == str(date_q)]
                        else:
                            games_for_date = games
                        missing = games_for_date[~games_for_date["game_id"].astype(str).isin(df["game_id"].astype(str))]
                        if not missing.empty:
                            add_cols = ["game_id","date","home_team","away_team","home_score","away_score","start_time"]
                            add = missing[[c for c in add_cols if c in missing.columns]].copy()
                            # Guarantee prediction placeholder columns
                            for c in ["pred_total","pred_margin"]:
                                if c not in add.columns:
                                    add[c] = None
                            df = pd.concat([df, add], ignore_index=True)
                except Exception:
                    pass

            def _pick(df_, candidates: list[str]) -> str | None:
                for name in candidates:
                    if name in df_.columns:
                        return name
                return None

            # Standardize team fields: create/patch home_team and away_team
            if "home_team" not in df.columns:
                src = _pick(df, [
                    "home_team_g", "home_team", "home_team_name_g", "home_team_name", "home_g", "home"
                ])
                if src is not None:
                    df["home_team"] = df[src]
            if "away_team" not in df.columns:
                src = _pick(df, [
                    "away_team_g", "away_team", "away_team_name_g", "away_team_name", "away_g", "away"
                ])
                if src is not None:
                    df["away_team"] = df[src]
        # Ensure actual_total is present for upcoming/preds builds
        try:
            if {"home_score","away_score"}.issubset(df.columns):
                hs = pd.to_numeric(df["home_score"], errors="coerce")
                aw = pd.to_numeric(df["away_score"], errors="coerce")
                if "actual_total" not in df.columns:
                    df["actual_total"] = np.where(hs.notna() & aw.notna(), hs + aw, np.nan)
                else:
                    mask = df["actual_total"].isna()
                    df.loc[mask, "actual_total"] = hs[mask] + aw[mask]
        except Exception:
            pass

    # Odds-only fallback (index route): if df still empty but odds present for selected date, synthesize minimal slate
    try:
        if df.empty and not odds.empty and date_q:
            o = odds.copy()
            # Restrict to totals + full game period markers
            if "market" in o.columns:
                o = o[o["market"].astype(str).str.lower() == "totals"]
            if "period" in o.columns:
                vals = o["period"].astype(str).str.lower()
                o = o[vals.isin(["full_game","fg","full game","game","match"]) | vals.isna()]
            # Date filter via commence_time or date_line
            if "commence_time" in o.columns:
                try:
                    o["_commence_date"] = pd.to_datetime(o["commence_time"], errors="coerce").dt.strftime("%Y-%m-%d")
                    o = o[o["_commence_date"] == str(date_q)]
                except Exception:
                    pass
            elif "date_line" in o.columns:
                o = o[o["date_line"].astype(str) == str(date_q)]
            if not o.empty:
                rows: list[dict[str, Any]] = []
                o["game_id"] = o.get("game_id", pd.Series(range(len(o)))).astype(str)
                for gid, g in o.groupby("game_id"):
                    r = {
                        "game_id": str(gid),
                        "home_team": g.get("home_team").dropna().astype(str).iloc[0] if "home_team" in g.columns and g["home_team"].notna().any() else g.get("home_team_name").dropna().astype(str).iloc[0] if "home_team_name" in g.columns and g["home_team_name"].notna().any() else None,
                        "away_team": g.get("away_team").dropna().astype(str).iloc[0] if "away_team" in g.columns and g["away_team"].notna().any() else g.get("away_team_name").dropna().astype(str).iloc[0] if "away_team_name" in g.columns and g["away_team_name"].notna().any() else None,
                        "start_time": None,
                        "pred_total": None,
                        "pred_margin": None,
                        "market_total": pd.to_numeric(g.get("total"), errors="coerce").median() if "total" in g.columns else None,
                        "date": str(date_q),
                        "home_score": None,
                        "away_score": None,
                    }
                    if "commence_time" in g.columns:
                        try:
                            t = pd.to_datetime(g["commence_time"], errors="coerce").min()
                            r["start_time"] = t.strftime("%Y-%m-%d %H:%M") if pd.notna(t) else None
                        except Exception:
                            pass
                    rows.append(r)
                if rows:
                    df = pd.DataFrame(rows)
                    results_note = f"Odds-only slate for {date_q} (no games/predictions available)"
                    coverage_summary = {"full": 0, "partial": 0, "none": 0}
    except Exception:
        pass

    # Ensure game_id present: if missing attempt deterministic construction (only when teams available)
    if "game_id" not in df.columns:
        home_col = next((c for c in ["home_team","home"] if c in df.columns), None)
        away_col = next((c for c in ["away_team","away"] if c in df.columns), None)
        if home_col and away_col:
            try:
                # Build surrogate id: date + normalized team names
                date_part = (df.get("date") or pd.Series([date_q]*len(df))).astype(str)
                home_norm = df[home_col].astype(str).map(normalize_name)
                away_norm = df[away_col].astype(str).map(normalize_name)
                df["game_id"] = [f"{d}:{a}:{h}" for d,a,h in zip(date_part, away_norm, home_norm)]
                logger.warning("Constructed surrogate game_id for %d rows (date/team based)", len(df))
            except Exception:
                logger.exception("Failed to construct surrogate game_id")
    if "game_id" in df.columns:
        try:
            df["game_id"] = df["game_id"].astype(str)
        except Exception:
            logger.warning("Could not cast game_id to string")

    # Coalesce start_time from games merge (start_time_g) if primary start_time is missing/blank
    try:
        if "start_time_g" in df.columns:
            if "start_time" not in df.columns:
                df["start_time"] = df["start_time_g"]
            else:
                st_primary = df["start_time"].astype(str).str.strip()
                # Fill global if entirely missing/blank
                if (df["start_time"].isna().all()) or st_primary.eq("").all():
                    df["start_time"] = df["start_time_g"]
                else:
                    mask_missing = df["start_time"].isna() | st_primary.eq("")
                    if mask_missing.any():
                        df.loc[mask_missing, "start_time"] = df.loc[mask_missing, "start_time_g"]
    except Exception:
        pass
    # Time alignment diagnostics: quantify start_time vs commence_time diffs (minutes) with TZ normalization
    try:
        if 'start_time' in df.columns and ('commence_time' in df.columns or 'commence_time_g' in df.columns):
            import os
            from zoneinfo import ZoneInfo
            sched_tz_name = os.getenv('SCHEDULE_TZ') or os.getenv('NCAAB_SCHEDULE_TZ') or 'America/New_York'
            try:
                sched_tz = ZoneInfo(sched_tz_name)
            except Exception:
                sched_tz = None
            # Normalize start_time to UTC (treat naive as schedule tz)
            st_raw = df['start_time'].astype(str)
            st_has_off = st_raw.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | st_raw.str.endswith('Z')
            st_off = pd.to_datetime(st_raw.where(st_has_off, None), errors='coerce', utc=True)
            st_naive = pd.to_datetime(st_raw.where(~st_has_off, None), errors='coerce', utc=False)
            if st_naive.notna().any():
                def _loc_st(x):
                    try:
                        if pd.isna(x):
                            return x
                        if getattr(x, 'tzinfo', None) is None and sched_tz is not None:
                            return x.replace(tzinfo=sched_tz)
                        return x
                    except Exception:
                        return x
                st_naive = st_naive.map(_loc_st)
                try:
                    st_naive = st_naive.dt.tz_convert(dt.timezone.utc)
                except Exception:
                    pass
            st_utc = st_off.where(st_off.notna(), st_naive)

            # Normalize commence_time columns to UTC
            ct_combined = None
            # commence_time: treat naive as UTC
            if 'commence_time' in df.columns:
                ct_raw = df['commence_time'].astype(str)
                ct_has_off = ct_raw.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | ct_raw.str.endswith('Z')
                ct_off = pd.to_datetime(ct_raw.where(ct_has_off, None).str.replace('Z', '+00:00', regex=False), errors='coerce', utc=True)
                ct_naive = pd.to_datetime(ct_raw.where(~ct_has_off, None), errors='coerce', utc=False)
                if ct_naive.notna().any():
                    def _loc_ct_naive_utc(x):
                        try:
                            if pd.isna(x):
                                return x
                            return x.replace(tzinfo=dt.timezone.utc)
                        except Exception:
                            return x
                    ct_naive = ct_naive.map(_loc_ct_naive_utc)
                ct = ct_off.where(ct_off.notna(), ct_naive)
                ct_combined = ct
            # commence_time_g: treat naive as schedule tz then convert to UTC
            if 'commence_time_g' in df.columns:
                cg_raw = df['commence_time_g'].astype(str)
                cg_has_off = cg_raw.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | cg_raw.str.endswith('Z')
                cg_off = pd.to_datetime(cg_raw.where(cg_has_off, None).str.replace('Z', '+00:00', regex=False), errors='coerce', utc=True)
                cg_naive = pd.to_datetime(cg_raw.where(~cg_has_off, None), errors='coerce', utc=False)
                if cg_naive.notna().any():
                    def _loc_cg(x):
                        try:
                            if pd.isna(x):
                                return x
                            if getattr(x, 'tzinfo', None) is None and sched_tz is not None:
                                return x.replace(tzinfo=sched_tz)
                            return x
                        except Exception:
                            return x
                    cg_naive = cg_naive.map(_loc_cg)
                    try:
                        cg_naive = cg_naive.dt.tz_convert(dt.timezone.utc)
                    except Exception:
                        pass
                cg = cg_off.where(cg_off.notna(), cg_naive)
                ct_combined = cg if ct_combined is None else ct_combined.combine_first(cg)
            if ct_combined is not None:
                diff_min = (st_utc - ct_combined).dt.total_seconds() / 60.0
                valid = pd.to_numeric(diff_min, errors='coerce').dropna()
                if not valid.empty:
                    pipeline_stats['start_time_diff_minutes_mean'] = float(valid.mean())
                    pipeline_stats['start_time_diff_minutes_median'] = float(valid.median())
                    pipeline_stats['start_time_diff_minutes_p95'] = float(valid.quantile(0.95))
                    # Mismatch threshold: >5 minutes considered meaningful
                    try:
                        pipeline_stats['start_time_mismatch_count'] = int((valid.abs() > 5.0).sum())
                    except Exception:
                        pass
                    pipeline_stats['start_time_large_diff_count'] = int((valid.abs() > 60).sum())
                    pipeline_stats['start_time_diff_rows'] = int(valid.count())
    except Exception:
        pipeline_stats['start_time_alignment_error'] = True

    # Attach odds (line totals) when available, unless daily_df already provided market_total
    # Distinguish strict last odds vs heuristic closing: if source file name contains 'last', label later.
    source_last = False
    try:
        # crude detection: odds DataFrame path not retained; infer from columns typical to last_odds vs closing_lines
        # last_odds has no 'prio' or synthetic markers; rely on absence of 'prio' + presence of many individual book rows
        source_last = True if "event_id" in odds.columns and "total" in odds.columns and "closing_total" not in odds.columns else False
    except Exception:
        source_last = False
    # Skip odds enrichment entirely if base df is empty (nothing to merge onto)
    if not odds.empty and not df.empty:
        oddf = odds.copy()
        if "market" in oddf.columns:
            oddf = oddf[oddf["market"].astype(str).str.lower() == "totals"]
        # Disabled period-based filtering to avoid accidental loss of usable odds rows; retain all rows regardless of period.
        if "game_id" in oddf.columns:
            oddf["game_id"] = oddf["game_id"].astype(str)
            # Direct merge of already-aggregated columns if present on odds frame
            try:
                direct_cols = {}
                if "market_total" in odds.columns:
                    direct_cols["market_total"] = "market_total"
                if "closing_total" in odds.columns:
                    direct_cols["closing_total"] = "closing_total"
                # Spread and ML flexible source names on odds dataframe
                if "spread_home" in odds.columns:
                    direct_cols["spread_home"] = "spread_home"
                elif "home_spread" in odds.columns:
                    direct_cols["home_spread"] = "spread_home"
                if "ml_home" in odds.columns:
                    direct_cols["ml_home"] = "ml_home"
                elif "moneyline_home" in odds.columns:
                    direct_cols["moneyline_home"] = "ml_home"
                if direct_cols:
                    sub = odds.copy()
                    sub["game_id"] = sub["game_id"].astype(str)
                    keep = ["game_id"] + list(direct_cols.keys())
                    sub = sub[[c for c in keep if c in sub.columns]]
                    if len(sub.columns) > 1:
                        # Aggregate by median in case of multiple rows per game
                        agg = sub.groupby("game_id").median(numeric_only=True).reset_index()
                        # Rename to canonical output names
                        agg = agg.rename(columns=direct_cols)
                        for c in ["market_total","closing_total","spread_home","ml_home"]:
                            if c in agg.columns:
                                if c in df.columns:
                                    try:
                                        df[c] = df[c].where(df[c].notna(), df["game_id"].map(agg.set_index("game_id")[c]))
                                    except Exception:
                                        pass
                                else:
                                    df = df.merge(agg[["game_id", c]], on="game_id", how="left")
            except Exception:
                pass
            if "total" in oddf.columns:
                # Ensure groupby result is a DataFrame with an explicit game_id column for safe merge
                line_by_game = (
                    oddf.groupby("game_id", as_index=False)["total"]
                        .median()
                        .rename(columns={"total": "_market_total_from_odds"})
                )
                if not line_by_game.empty:
                    if "game_id" in df.columns:
                        try:
                            df = df.merge(line_by_game, on="game_id", how="left")
                        except Exception:
                            logger.exception("Odds total merge failed; proceeding without _market_total_from_odds")
                    else:
                        # Attempt to synthesize game_id on the fly if teams present
                        home_col = next((c for c in ["home_team","home"] if c in df.columns), None)
                        away_col = next((c for c in ["away_team","away"] if c in df.columns), None)
                        date_col = "date" if "date" in df.columns else None
                        if home_col and away_col:
                            try:
                                date_part = (df.get(date_col) or pd.Series([None]*len(df))).astype(str)
                                home_norm = df[home_col].astype(str).map(normalize_name)
                                away_norm = df[away_col].astype(str).map(normalize_name)
                                df["game_id"] = [f"{d}:{a}:{h}" for d,a,h in zip(date_part, away_norm, home_norm)]
                                logger.warning("Synthesized game_id for odds total merge (%d rows)", len(df))
                                df = df.merge(line_by_game, on="game_id", how="left")
                            except Exception:
                                logger.exception("Failed synthesizing game_id for odds total merge; skipping")
                        else:
                            logger.warning("Skipping odds total merge: df missing game_id and team columns")
                    # Coalesce per-row to fill missing market_total only if helper column now exists
                if "_market_total_from_odds" in df.columns:
                    if "market_total" in df.columns:
                        try:
                            df["market_total"] = df["market_total"].where(df["market_total"].notna(), df["_market_total_from_odds"])
                        except Exception:
                            pass
                    else:
                        # Create market_total only when we actually have odds-derived values
                        try:
                            df["market_total"] = df["_market_total_from_odds"]
                        except Exception:
                            logger.warning("Could not assign market_total from _market_total_from_odds despite column presence")
            # Build per-game odds list and start time from commence_time when present
            odds_map: dict[str, list[dict[str, Any]]] = {}
            start_map: dict[str, str] = {}
            commence_map_odds: dict[str, Any] = {}
            # Parse commence_time from the full odds feed (all markets), not just totals
            odds_commence = odds.copy()
            if "game_id" in odds_commence.columns:
                odds_commence["game_id"] = odds_commence["game_id"].astype(str)
            if "commence_time" in odds_commence.columns:
                try:
                    odds_commence["_commence"] = pd.to_datetime(odds_commence["commence_time"].astype(str).str.replace("Z", "+00:00", regex=False), errors="coerce", utc=True)
                except Exception:
                    odds_commence["_commence"] = pd.NaT
            else:
                odds_commence["_commence"] = pd.NaT
            # Select a small subset of columns for the odds list
            for gid, g in oddf.groupby("game_id"):
                # Sort by book name for determinism
                g2 = g.sort_values(["book"]) if "book" in g.columns else g
                items: list[dict[str, Any]] = []
                for _, r in g2.iterrows():
                    item: dict[str, Any] = {
                        "book": r.get("book"),
                        "total": r.get("total"),
                    }
                    # Optional prices/lines
                    for k in ("price_over", "price_under", "over", "under", "line_over", "line_under"):
                        if k in g2.columns:
                            item[k] = r.get(k)
                    items.append(item)
                    if len(items) >= 6:
                        break
                odds_map[str(gid)] = items
                # Earliest commence time for display
                # Use earliest commence from the full odds feed for the game_id
                try:
                    cg = odds_commence[odds_commence["game_id"].astype(str) == str(gid)]
                    if "_commence" in cg.columns and cg["_commence"].notna().any():
                        t = cg["_commence"].min()
                        start_map[str(gid)] = t.strftime("%Y-%m-%d %H:%M")
                        commence_map_odds[str(gid)] = t
                except Exception:
                    pass
            if odds_map and "game_id" in df.columns:
                df["_odds_list"] = df["game_id"].map(lambda x: odds_map.get(str(x), []))
            if start_map and "game_id" in df.columns:
                # Only fill missing start_time from odds commence_time; don't overwrite existing game times
                mapped = df["game_id"].map(lambda x: start_map.get(str(x)))
                if "start_time" in df.columns:
                    try:
                        df["start_time"] = df["start_time"].where(df["start_time"].notna(), mapped)
                    except Exception:
                        df["start_time"] = mapped
                else:
                    df["start_time"] = mapped
            # Also expose authoritative odds commence as a dedicated column for downstream time authority
            if commence_map_odds and "game_id" in df.columns:
                try:
                    df["commence_time_odds"] = df["game_id"].map(lambda x: commence_map_odds.get(str(x)))
                except Exception:
                    pass
            # Fallback: fill commence_time_odds by unordered team pair if game_id mapping misses
            try:
                if "commence_time_odds" not in df.columns or df["commence_time_odds"].isna().any():
                    src = oddf.copy()
                    # Resolve team columns from odds
                    hn = "home_team_name" if "home_team_name" in src.columns else ("home_team" if "home_team" in src.columns else None)
                    an = "away_team_name" if "away_team_name" in src.columns else ("away_team" if "away_team" in src.columns else None)
                    if hn and an and "_commence" in src.columns:
                        def _slug(x):
                            try:
                                return _canon_slug(str(x))
                            except Exception:
                                return normalize_name(str(x))
                        src["_hn"] = src[hn].astype(str).map(_slug)
                        src["_an"] = src[an].astype(str).map(_slug)
                        src["_pair"] = src.apply(lambda r: "::".join(sorted([str(r.get("_hn") or ""), str(r.get("_an") or "")])), axis=1)
                        ct_pair = src.groupby("_pair")["_commence"].min()
                        if "_pair_key" not in df.columns and {"home_team","away_team"}.issubset(df.columns):
                            df["_home_norm"] = df["home_team"].astype(str).map(_slug)
                            df["_away_norm"] = df["away_team"].astype(str).map(_slug)
                            df["_pair_key"] = df.apply(lambda r: "::".join(sorted([str(r.get("_home_norm") or ""), str(r.get("_away_norm") or "")])), axis=1)
                        if "_pair_key" in df.columns:
                            pair_map_vals = df["_pair_key"].map(ct_pair)
                            if "commence_time_odds" in df.columns:
                                miss = df["commence_time_odds"].isna() | df["commence_time_odds"].astype(str).str.strip().eq("")
                                if isinstance(miss, pd.Series) and miss.any():
                                    df.loc[miss, "commence_time_odds"] = pair_map_vals[miss]
                            else:
                                df["commence_time_odds"] = pair_map_vals
            except Exception:
                pass
            # Instrumentation: count non-null commence_time_odds
            try:
                if "commence_time_odds" in df.columns:
                    # Normalize to UTC-aware ISO strings to ensure consistent parsing downstream
                    try:
                        ct_raw = df["commence_time_odds"].astype(str).str.replace("Z", "+00:00", regex=False)
                        # Prefer explicit ISO format when possible; fall back to dateutil per-element if needed
                        try:
                            ct_parsed = pd.to_datetime(ct_raw, format="%Y-%m-%dT%H:%M:%S%z", utc=True, errors="coerce")
                        except Exception:
                            ct_parsed = pd.to_datetime(ct_raw, errors="coerce", utc=True)
                        df["commence_time_odds"] = ct_parsed.dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                    except Exception:
                        pass
                    nn = int(pd.to_datetime(df["commence_time_odds"], errors="coerce").notna().sum())
                    pipeline_stats["commence_time_odds_nonnull"] = nn
            except Exception:
                pipeline_stats["commence_time_odds_metric_error"] = True

        # Apply robust odds backfill across the merged frame to ensure per-game coverage
        try:
            df = apply_odds_backfill(df)
        except Exception:
            pipeline_stats["apply_odds_backfill_error"] = True

    # Time alignment diagnostics (post odds + commence_time enrichment) with TZ normalization
    try:
        if 'start_time' in df.columns and ('commence_time' in df.columns or '_commence' in df.columns):
            import os
            from zoneinfo import ZoneInfo
            sched_tz_name = os.getenv('SCHEDULE_TZ') or os.getenv('NCAAB_SCHEDULE_TZ') or 'America/New_York'
            try:
                sched_tz = ZoneInfo(sched_tz_name)
            except Exception:
                sched_tz = None
            # Normalize start_time to UTC
            st_raw = df['start_time'].astype(str)
            st_has_off = st_raw.str.contains(r"[+-]\\d{2}:\\d{2}$", regex=True) | st_raw.str.endswith('Z')
            st_off = pd.to_datetime(st_raw.where(st_has_off, None), errors='coerce', utc=True)
            st_naive = pd.to_datetime(st_raw.where(~st_has_off, None), errors='coerce', utc=False)
            if st_naive.notna().any():
                def _loc_st2(x):
                    try:
                        if pd.isna(x):
                            return x
                        if getattr(x, 'tzinfo', None) is None and sched_tz is not None:
                            return x.replace(tzinfo=sched_tz)
                        return x
                    except Exception:
                        return x
                st_naive = st_naive.map(_loc_st2)
                try:
                    st_naive = st_naive.dt.tz_convert(dt.timezone.utc)
                except Exception:
                    pass
            st_utc = st_off.where(st_off.notna(), st_naive)
            # Normalize commence_time/_commence to UTC
            ct = None
            for src in ['commence_time','_commence']:
                if src in df.columns:
                    cr = df[src].astype(str)
                    ch = cr.str.contains(r"[+-]\\d{2}:\\d{2}$", regex=True) | cr.str.endswith('Z')
                    co = pd.to_datetime(cr.where(ch, None).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                    cn = pd.to_datetime(cr.where(~ch, None), errors='coerce', utc=False)
                    if cn.notna().any():
                        def _loc_ct2(x):
                            try:
                                if pd.isna(x):
                                    return x
                                if getattr(x, 'tzinfo', None) is None and sched_tz is not None:
                                    return x.replace(tzinfo=sched_tz)
                                return x
                            except Exception:
                                return x
                        cn = cn.map(_loc_ct2)
                        try:
                            cn = cn.dt.tz_convert(dt.timezone.utc)
                        except Exception:
                            pass
                    cur = co.where(co.notna(), cn)
                    ct = cur if ct is None else ct.combine_first(cur)
            if ct is not None:
                diff_min = (st_utc - ct).dt.total_seconds() / 60.0
                valid = pd.to_numeric(diff_min, errors='coerce').dropna()
                if not valid.empty:
                    pipeline_stats['start_time_diff_minutes_mean'] = float(valid.mean())
                    pipeline_stats['start_time_diff_minutes_median'] = float(valid.median())
                    pipeline_stats['start_time_diff_minutes_p95'] = float(valid.quantile(0.95))
                    try:
                        pipeline_stats['start_time_mismatch_count'] = int((valid.abs() > 5.0).sum())
                    except Exception:
                        pass
                    pipeline_stats['start_time_large_diff_count'] = int((valid.abs() > 60).sum())
                    pipeline_stats['start_time_diff_rows'] = int(valid.count())
                    # UTC-only mismatch examples (post-odds) for debugging
                    try:
                        mask_post = (diff_min.abs() > 1.0)
                        if mask_post.any():
                            ex_df = pd.DataFrame({
                                'game_id': df.get('game_id'),
                                'home_team': df.get('home_team'),
                                'away_team': df.get('away_team'),
                                'schedule_start_iso': st_utc.astype(str),
                                'commence_start_iso': ct.astype(str),
                                'diff_minutes': diff_min
                            }).dropna(subset=['diff_minutes'])
                            pipeline_stats['time_mismatch_examples_post'] = ex_df.head(8).to_dict('records')
                            pipeline_stats['time_mismatch_count_post'] = int(mask_post.sum())
                    except Exception:
                        pipeline_stats['time_mismatch_examples_post_error'] = True
    except Exception:
        pipeline_stats['start_time_alignment_post_error'] = True

    # Fallback odds merge: if market_total still missing for many rows, derive from raw odds feed by team name matching.
    try:
        if "market_total" in df.columns:
            missing_mt_mask = df["market_total"].isna() | (df["market_total"].astype(str).str.lower().isin(["nan","none","null","nat"]))
        else:
            missing_mt_mask = pd.Series([True]*len(df))
        # Threshold: only run fallback if more than 50% missing or all missing
        if len(df) and missing_mt_mask.sum() > 0 and missing_mt_mask.sum() >= int(0.5*len(df)):
            # Determine raw odds file path per date (supports today and historical) with pattern fallback for prefetch variants.
            primary_raw = OUT / ("odds_today.csv" if (not date_q or (today_str and date_q == today_str)) else f"odds_{date_q}.csv")
            # Also support historical snapshots under outputs/odds_history/odds_<date>.csv
            history_raw = OUT / "odds_history" / (f"odds_{date_q}.csv" if date_q else "")
            raw = pd.DataFrame()
            if primary_raw.exists():
                try:
                    raw = pd.read_csv(primary_raw)
                except Exception:
                    raw = pd.DataFrame()
            # Fallback to odds_history for past dates
            if raw.empty and date_q and history_raw.exists():
                try:
                    raw = pd.read_csv(history_raw)
                except Exception:
                    raw = pd.DataFrame()
            if raw.empty and date_q:
                try:
                    # e.g. odds_YYYY-MM-DD_prefetch.csv or other suffixes
                    for p in sorted(OUT.glob(f"odds_{date_q}_*.csv")):
                        tmp = _safe_read_csv(p)
                        if not tmp.empty:
                            raw = tmp
                            break
                except Exception:
                    pass
            if not raw.empty:
                # Constrain by date if commence_time present and date_q provided (avoid cross-day contamination)
                if date_q and "commence_time" in raw.columns:
                    try:
                        raw["_commence_dt"] = pd.to_datetime(raw["commence_time"], errors="coerce")
                        raw_date_mask = raw["_commence_dt"].dt.strftime("%Y-%m-%d") == str(date_q)
                        raw = raw[raw_date_mask | raw["_commence_dt"].isna()]
                    except Exception:
                        pass
                # Resolve team column names flexibly
                home_col = next((c for c in ["home_team_name","home_team","home"] if c in raw.columns), None)
                away_col = next((c for c in ["away_team_name","away_team","away"] if c in raw.columns), None)
                if home_col and away_col:
                    # Initial canonical slug pass
                    raw["_home_norm"] = raw[home_col].astype(str).map(_canon_slug)
                    raw["_away_norm"] = raw[away_col].astype(str).map(_canon_slug)
                    # Build canonical universe from current df (games/preds) for improved matching
                    try:
                        canon_universe: set[str] = set()
                        if {"home_team","away_team"}.issubset(df.columns):
                            canon_universe.update(_canon_slug(t) for t in df["home_team"].astype(str))
                            canon_universe.update(_canon_slug(t) for t in df["away_team"].astype(str))
                        # Fallback simple cleanup for odds names that include mascots not present in internal canonical names.
                        mascot_drop = {"falcons","wildcats","tigers","bulldogs","eagles","hawks","knights","raiders","rams","spartans","vikings","aggies","cardinals","broncos","panthers","lions","gators","longhorns","buckeyes","sooners","rebels","cougars","mountaineers","bearcats","bears","wolfpack","cowboys","dolphins","gaels","miners","pilots","dons","jaguars","gamecocks","hurricanes","gophers","badgers","illini","hoosiers","seminoles","noles","heels","priests","demons","deacons","billikens","salukis","anteaters","horned","hornedfrogs","hornets","hornets","owls","trojans","spiders","catamounts","bluejays","musketeers","skyhawks","lancers","warriors","titans","matadors","bison","lopes","lopers","vikings","golden","goldeneagles","bluehens","seahawks","sea","sea hawks","blackbirds","bananas","peacocks","peacocks","stags","quakers","dragons","dragons","foxfes"}
                        try:
                            from rapidfuzz import process, fuzz  # type: ignore
                            use_fuzzy = True
                        except Exception:
                            use_fuzzy = False
                        def _refine(name: str) -> str:
                            base = _canon_slug(name)
                            if base in canon_universe:
                                return base
                            parts = [p for p in re.sub(r"[^A-Za-z0-9 ]+"," ", name).lower().split() if p]
                            # Drop trailing mascot tokens progressively
                            for k in range(len(parts), 0, -1):
                                cand = _canon_slug(" ".join(p for p in parts[:k] if p not in mascot_drop))
                                if cand in canon_universe:
                                    return cand
                            if use_fuzzy and canon_universe:
                                # Fuzzy match against universe
                                best = process.extractOne(base, list(canon_universe), scorer=fuzz.token_set_ratio)
                                if best and best[1] >= 85:
                                    return best[0]
                            return base
                        raw["_home_norm"] = raw[home_col].astype(str).map(_refine)
                        raw["_away_norm"] = raw[away_col].astype(str).map(_refine)
                    except Exception:
                        pass
                    agg_rows: dict[str, dict[str, Any]] = {}
                    for _, r in raw.iterrows():
                        hn = r.get("_home_norm"); an = r.get("_away_norm")
                        if not hn or not an:
                            continue
                        pair_key = "::".join(sorted([str(hn), str(an)]))
                        dct = agg_rows.setdefault(pair_key, {"totals": [], "spreads": [], "ml": []})
                        # Flexible column resolution
                        tot = next((r.get(tc) for tc in ["total","over_under","market_total","line_total"] if tc in raw.columns and pd.notna(r.get(tc))), None)
                        spr = next((r.get(sc) for sc in ["spread","home_spread","spread_home","handicap_home"] if sc in raw.columns and pd.notna(r.get(sc))), None)
                        mlh = next((r.get(mc) for mc in ["moneyline_home","ml_home","price_home","h2h_home"] if mc in raw.columns and pd.notna(r.get(mc))), None)
                        if pd.notna(tot):
                            dct["totals"].append(tot)
                        if pd.notna(spr):
                            dct["spreads"].append(spr)
                        if pd.notna(mlh):
                            dct["ml"].append(mlh)
                    # Apply aggregates to df rows
                    logger.info("Raw odds fallback aggregator pairs=%d", len(agg_rows))
                    for idx, row in df.iterrows():
                        h = _canon_slug(str(row.get("home_team") or ""))
                        a = _canon_slug(str(row.get("away_team") or ""))
                        if not h or not a:
                            continue
                        pkey = "::".join(sorted([h, a]))
                        ag = agg_rows.get(pkey)
                        if not ag:
                            continue
                        if ("market_total" not in df.columns) or pd.isna(df.loc[idx, "market_total"]) or str(df.loc[idx, "market_total"]).lower() in ("nan","none","null"):
                            if ag["totals"]:
                                df.loc[idx, "market_total"] = float(pd.to_numeric(pd.Series(ag["totals"]), errors="coerce").median())
                        if ("spread_home" not in df.columns or pd.isna(df.loc[idx, "spread_home"]) or str(df.loc[idx, "spread_home"]).lower() in ("nan","none","null")) and ag["spreads"]:
                            df.loc[idx, "spread_home"] = float(pd.to_numeric(pd.Series(ag["spreads"]), errors="coerce").median())
                        if ("ml_home" not in df.columns or pd.isna(df.loc[idx, "ml_home"]) or str(df.loc[idx, "ml_home"]).lower() in ("nan","none","null")) and ag["ml"]:
                            df.loc[idx, "ml_home"] = float(pd.to_numeric(pd.Series(ag["ml"]), errors="coerce").median())
    except Exception:
        pass

    # Additional start_time fallback from games' commence_time (post-merge) and odds_today style columns
    # Secondary odds coverage fallback: if majority of market_total still missing, derive directly from raw odds totals without any period filtering.
    try:
        if not odds.empty and "game_id" in odds.columns:
            if ("market_total" not in df.columns) or (df["market_total"].isna().sum() > 0.6 * len(df)):
                o3 = odds.copy()
                o3["game_id"] = o3["game_id"].astype(str)
                if "total" in o3.columns:
                    line_by_game2 = (
                        o3.groupby("game_id", as_index=False)["total"]
                          .median()
                          .rename(columns={"total": "_market_total_from_odds_fallback"})
                    )
                    logger.info("Secondary odds fallback: %d games aggregated; pre-missing=%d", len(line_by_game2), int(df["market_total"].isna().sum() if "market_total" in df.columns else len(df)))
                    # Pair-based aggregation for rows lacking game_id alignment (use canonical slugs)
                    try:
                        o3["_home_norm"] = o3.get("home_team_name", o3.get("home_team", "")).astype(str).map(_canon_slug)
                        o3["_away_norm"] = o3.get("away_team_name", o3.get("away_team", "")).astype(str).map(_canon_slug)
                        o3["_pair_key"] = o3.apply(lambda r: "::".join(sorted([str(r.get("_home_norm")), str(r.get("_away_norm"))])), axis=1)
                        pair_med = (
                            o3.groupby("_pair_key", as_index=False)["total"]
                              .median()
                              .rename(columns={"total": "_market_total_pair_med"})
                        )
                    except Exception:
                        pair_med = pd.DataFrame()
                    if not line_by_game2.empty and "game_id" in df.columns:
                        df = df.merge(line_by_game2, on="game_id", how="left")
                        if "market_total" in df.columns:
                            df["market_total"] = df["market_total"].where(df["market_total"].notna(), df["_market_total_from_odds_fallback"])
                        else:
                            df["market_total"] = df["_market_total_from_odds_fallback"]
                    # Apply pair-based fill if still missing
                    if pair_med is not None and not pair_med.empty and ("home_team" in df.columns and "away_team" in df.columns):
                        try:
                            df["_pair_key"] = df.apply(lambda r: "::".join(sorted([_canon_slug(str(r.get("home_team") or "")), _canon_slug(str(r.get("away_team") or ""))])), axis=1)
                            df = df.merge(pair_med, on="_pair_key", how="left")
                            if "market_total" in df.columns:
                                df["market_total"] = df["market_total"].where(df["market_total"].notna(), df["_market_total_pair_med"])
                            else:
                                df["market_total"] = df["_market_total_pair_med"]
                        except Exception:
                            pass
                        logger.info("Secondary odds fallback applied; post-missing=%d", int(df["market_total"].isna().sum()))
    except Exception:
        pass
    try:
        for cname in ("commence_time", "commence_time_g"):
            if cname in df.columns:
                # Normalize commence_time to user's local timezone and standard display format
                try:
                    import os
                    from zoneinfo import ZoneInfo
                    # Prefer end-user provided timezone (query/cookie), then env, then default
                    try:
                        disp_tz_name = _get_display_tz_name()
                    except Exception:
                        disp_tz_name = os.getenv("DISPLAY_TZ") or os.getenv("SCHEDULE_TZ") or "America/New_York"
                    try:
                        _local_tz = ZoneInfo(disp_tz_name)
                    except Exception:
                        _local_tz = dt.datetime.now().astimezone().tzinfo
                    # Also get schedule tz for naive commence_time strings
                    try:
                        sched_tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                        _sched_tz = ZoneInfo(sched_tz_name)
                    except Exception:
                        _sched_tz = None
                except Exception:
                    _local_tz = dt.datetime.now().astimezone().tzinfo
                    _sched_tz = None
                try:
                    raw_series = df[cname].astype(str).str.strip()
                    has_offset = raw_series.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | raw_series.str.endswith("Z")
                    # Offset-aware parse as UTC
                    s_off = pd.to_datetime(raw_series.where(has_offset, None).str.replace("Z", "+00:00", regex=False), errors="coerce", utc=True)
                    # Naive handling per source:
                    # - commence_time (odds): treat naive strings as UTC
                    # - commence_time_g (games schedule): treat naive strings as schedule timezone, then convert to UTC
                    s_naive = pd.to_datetime(raw_series.where(~has_offset, None), errors="coerce", utc=False)
                    if s_naive.notna().any():
                        if cname == "commence_time":
                            # Assume UTC for naive odds commence_time
                            s_naive = s_naive.map(lambda x: x.replace(tzinfo=dt.timezone.utc) if pd.notna(x) and getattr(x, 'tzinfo', None) is None else x)
                        else:
                            def _loc_naive2(x):
                                try:
                                    if pd.isna(x):
                                        return x
                                    if getattr(x, 'tzinfo', None) is None and _sched_tz is not None:
                                        return x.replace(tzinfo=_sched_tz)
                                    return x
                                except Exception:
                                    return x
                            s_naive = s_naive.map(_loc_naive2)
                            try:
                                s_naive = s_naive.dt.tz_convert(dt.timezone.utc)
                            except Exception:
                                pass
                    s = s_off.where(s_off.notna(), s_naive)
                    # Convert to user's display tz
                    if _local_tz is not None:
                        s = s.dt.tz_convert(_local_tz)
                    mapped = s.dt.strftime("%Y-%m-%d %H:%M")
                except Exception:
                    # Fallback naive parse/format without tz conversion
                    mapped = pd.to_datetime(df[cname], errors="coerce").dt.strftime("%Y-%m-%d %H:%M")
                if "start_time" in df.columns:
                    df["start_time"] = df["start_time"].where(df["start_time"].notna(), mapped)
                else:
                    df["start_time"] = mapped
                break
        # odds_today format may have start_time under different columns like game_date+commence_time or missing market column entirely.
        if ("market_total" not in df.columns or df["market_total"].isna().all()) and "total" in df.columns and "_market_total_from_odds" not in df.columns:
            try:
                # If totals aggregated missing, compute median from raw total column.
                if "game_id" in df.columns:
                    df["_market_total_from_odds"] = df.groupby("game_id")["total"].transform(lambda s: pd.to_numeric(s, errors="coerce").median())
                    df["market_total"] = df["market_total"].where(df["market_total"].notna(), df["_market_total_from_odds"]) if "market_total" in df.columns else df["_market_total_from_odds"]
                else:
                    logger.warning("Skipping transform for market_total: df missing game_id column")
            except Exception:
                pass
    except Exception:
        pass

    # Also attach spreads and moneyline medians (full game) for ATS/ML displays and half lines for 1H/2H totals
    try:
        if not odds.empty and "game_id" in odds.columns:
            o2 = odds.copy()
            o2["game_id"] = o2["game_id"].astype(str)
            def _agg_market(market: str, val_col: str, out_col: str) -> pd.Series | None:
                sub = o2[o2["market"].astype(str).str.lower() == market]
                if "period" in sub.columns:
                    sub = sub[sub["period"].astype(str).str.lower().isin(["full_game","fg","full game"])]
                if val_col not in sub.columns or sub.empty:
                    return None
                return sub.groupby("game_id")[val_col].median().rename(out_col)
            s_fg = _agg_market("spreads", "home_spread", "spread_home")
            m_fg = _agg_market("h2h", "moneyline_home", "ml_home")
            for srs in [s_fg, m_fg]:
                if srs is not None and not srs.empty:
                    # Convert Series to DataFrame with explicit key for merge
                    if isinstance(srs, pd.Series):
                        srs = srs.reset_index()  # columns: ["game_id", <named_col>]
                    if "game_id" in df.columns:
                        df = df.merge(srs, on="game_id", how="left")
                    else:
                        logger.warning("Skipping spread/ML merge: df missing game_id column")

            # 1H/2H medians with robust period normalization (spaces/hyphens/variants)
            def _norm_period(v: Any) -> str:
                try:
                    s = str(v).lower().strip()
                except Exception:
                    return ""
                s = re.sub(r"[^a-z0-9]+", "_", s).strip("_")
                return s
            def _agg_market_period(market: str, val_col: str, period_keys: set[str], out_col: str) -> pd.Series | None:
                sub = o2[o2["market"].astype(str).str.lower() == market]
                if "period" in sub.columns:
                    vals = sub["period"].map(_norm_period)
                    sub = sub[vals.isin(period_keys)]
                if val_col not in sub.columns or sub.empty:
                    return None
                return sub.groupby("game_id")[val_col].median().rename(out_col)
            # Accept many variants for half labels
            PERIOD_1H = {"1h","1st_half","first_half","h1","half_1","1_h","1sthalf","firsthalf"}
            PERIOD_2H = {"2h","2nd_half","second_half","h2","half_2","2_h","2ndhalf","secondhalf"}
            t_1h = _agg_market_period("totals", "total", PERIOD_1H, "market_total_1h")
            t_2h = _agg_market_period("totals", "total", PERIOD_2H, "market_total_2h")
            s_1h = _agg_market_period("spreads", "home_spread", PERIOD_1H, "spread_home_1h")
            s_2h = _agg_market_period("spreads", "home_spread", PERIOD_2H, "spread_home_2h")
            # Half moneylines (when available)
            m_1h_h = _agg_market_period("h2h", "moneyline_home", PERIOD_1H, "ml_home_1h")
            m_2h_h = _agg_market_period("h2h", "moneyline_home", PERIOD_2H, "ml_home_2h")
            m_1h_a = _agg_market_period("h2h", "moneyline_away", PERIOD_1H, "ml_away_1h")
            m_2h_a = _agg_market_period("h2h", "moneyline_away", PERIOD_2H, "ml_away_2h")
            for srs in [t_1h, t_2h, s_1h, s_2h, m_1h_h, m_2h_h, m_1h_a, m_2h_a]:
                if srs is not None and not srs.empty:
                    if isinstance(srs, pd.Series):
                        srs = srs.reset_index()
                    if "game_id" in df.columns:
                        df = df.merge(srs, on="game_id", how="left")
                    else:
                        logger.warning("Skipping half-line merge: df missing game_id column")

            # Fallback: derive half spreads from full-game spread if provider 1H/2H spreads missing
            try:
                if "spread_home" in df.columns:
                    sh_full = pd.to_numeric(df["spread_home"], errors="coerce")
                    # Only derive if 1H spread column absent OR all NaN
                    need_1h = ("spread_home_1h" not in df.columns) or (df.get("spread_home_1h").isna().all())
                    if need_1h:
                        df["spread_home_1h"] = np.where(sh_full.notna(), sh_full * 0.5, np.nan)
                        df["spread_home_1h_basis"] = np.where(sh_full.notna(), "derived", None)
                    else:
                        # Mark basis real for existing values
                        df["spread_home_1h_basis"] = np.where(df["spread_home_1h"].notna(), "provider", None)
                    need_2h = ("spread_home_2h" not in df.columns) or (df.get("spread_home_2h").isna().all())
                    if need_2h:
                        # Assume symmetric halves; second half spread equals first half derived
                        # If we derived 1H above, re-use; else derive independently.
                        base_1h = df.get("spread_home_1h") if "spread_home_1h" in df.columns else (sh_full * 0.5)
                        df["spread_home_2h"] = np.where(sh_full.notna(), sh_full - base_1h, np.nan)
                        df["spread_home_2h_basis"] = np.where(sh_full.notna(), "derived", None)
                    else:
                        df["spread_home_2h_basis"] = np.where(df["spread_home_2h"].notna(), "provider", None)
            except Exception:
                pass

        # Coalesce halftime columns into canonical names even if base exists but is empty
        # Prefer non-NaN across [base, base_x, base_y, base_m, base_bs, base_sec, base_ref, base_fused]
        for base in ["home_score_1h","away_score_1h","home_score_2h","away_score_2h"]:
            cands = [c for c in df.columns if c == base or c.startswith(base + "_")]
            if cands:
                vals = None
                for c in [base] + [c for c in cands if c != base]:
                    if c not in df.columns:
                        continue
                    try:
                        s = pd.to_numeric(df[c], errors="coerce")
                    except Exception:
                        s = pd.Series(np.nan, index=df.index)
                    vals = s if vals is None else vals.where(vals.notna(), s)
                if vals is not None:
                    df[base] = vals
    except Exception:
        pass

    # Hard enrichment: inject start_time/commence_time and venue from games_curr for the selected date as authoritative source
    try:
        gm = _safe_read_csv(OUT / "games_curr.csv")
        if (gm.empty or (date_q and "date" in gm.columns and not (gm["date"].astype(str) == str(date_q)).any())) and date_q:
            alt = OUT / f"games_{date_q}.csv"
            if alt.exists():
                g2 = _safe_read_csv(alt)
                if not g2.empty:
                    gm = g2
        if not gm.empty and "game_id" in gm.columns:
            gm["game_id"] = gm["game_id"].astype(str)
            if date_q and "date" in gm.columns:
                try:
                    gm["date"] = pd.to_datetime(gm["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                    gm = gm[gm["date"] == str(date_q)]
                except Exception:
                    pass
            # Build maps
            m_commence = gm.set_index("game_id")["commence_time"] if "commence_time" in gm.columns else None
            m_start = gm.set_index("game_id")["start_time"] if "start_time" in gm.columns else None
            m_venue = gm.set_index("game_id")["venue"] if "venue" in gm.columns else None
            m_neutral = gm.set_index("game_id")["neutral_site"] if "neutral_site" in gm.columns else None
            m_city = gm.set_index("game_id")["city"] if "city" in gm.columns else None
            m_state = gm.set_index("game_id")["state"] if "state" in gm.columns else None
            if "game_id" in df.columns:
                # Ensure string type
                df["game_id"] = df["game_id"].astype(str)
                # First, prefer commence_time by game_id to drive slate/time normalization later
                if m_commence is not None:
                    if "commence_time" in df.columns:
                        cm_str = df["commence_time"].astype(str)
                        mask_missing_cm = df["commence_time"].isna() | cm_str.str.strip().eq("")
                        if mask_missing_cm.any():
                            df.loc[mask_missing_cm, "commence_time"] = df.loc[mask_missing_cm, "game_id"].map(m_commence)
                    else:
                        df["commence_time"] = df["game_id"].map(m_commence)
                if m_start is not None:
                    if "start_time" in df.columns:
                        st_str = df["start_time"].astype(str)
                        mask_missing = df["start_time"].isna() | st_str.str.strip().eq("")
                        if mask_missing.any():
                            df.loc[mask_missing, "start_time"] = df.loc[mask_missing, "game_id"].map(m_start)
                    else:
                        df["start_time"] = df["game_id"].map(m_start)
                if m_venue is not None:
                    if "venue" in df.columns:
                        v_str = df["venue"].astype(str)
                        mask_mv = df["venue"].isna() | v_str.str.strip().eq("") | v_str.str.lower().isin(["nan","none","null","nat"]) 
                        if mask_mv.any():
                            df.loc[mask_mv, "venue"] = df.loc[mask_mv, "game_id"].map(m_venue)
                    else:
                        df["venue"] = df["game_id"].map(m_venue)
                if m_neutral is not None and "neutral_site" not in df.columns:
                    df["neutral_site"] = df["game_id"].map(m_neutral)
                # If venue still missing, try City, State
                if "venue" in df.columns and df["venue"].isna().any() and (m_city is not None or m_state is not None):
                    def _city_state(gid):
                        ci = m_city.get(gid) if m_city is not None else None
                        st = m_state.get(gid) if m_state is not None else None
                        if pd.notna(ci) and pd.notna(st):
                            return f"{ci}, {st}"
                        return ci if pd.notna(ci) else (st if pd.notna(st) else None)
                    mask_missing_v = df["venue"].isna()
                    if mask_missing_v.any():
                        df.loc[mask_missing_v, "venue"] = df.loc[mask_missing_v, "game_id"].map(_city_state)

            # Pair-key enrichment: also fill commence_time/start_time/venue via unordered (neutral-site safe) team pairs
            try:
                have_names_df = {"home_team","away_team"}.issubset(df.columns)
                have_names_gm = {"home_team","away_team"}.issubset(gm.columns)
                if have_names_df and have_names_gm:
                    # Build pair keys
                    if "_pair_key" not in df.columns:
                        df["_home_norm"] = df["home_team"].astype(str).map(_canon_slug)
                        df["_away_norm"] = df["away_team"].astype(str).map(_canon_slug)
                        df["_pair_key"] = df.apply(lambda r: "::".join(sorted([str(r.get("_home_norm")), str(r.get("_away_norm"))])), axis=1)
                    gm = gm.copy()
                    gm["_home_norm"] = gm["home_team"].astype(str).map(_canon_slug)
                    gm["_away_norm"] = gm["away_team"].astype(str).map(_canon_slug)
                    gm["_pair_key"] = gm.apply(lambda r: "::".join(sorted([str(r.get("_home_norm")), str(r.get("_away_norm"))])), axis=1)
                    # Build pair-based maps (dropna to avoid overwriting with blanks)
                    def _series_map(gm_df, col):
                        if col in gm_df.columns:
                            s = gm_df[["_pair_key", col]].copy()
                            s[col] = s[col].replace({"": np.nan})
                            s = s.dropna(subset=[col])
                            return s.set_index("_pair_key")[col]
                        return None
                    mp_commence = _series_map(gm, "commence_time")
                    mp_start = _series_map(gm, "start_time")
                    mp_venue = _series_map(gm, "venue")
                    mp_neutral = _series_map(gm, "neutral_site")
                    # Compute a schedule datetime (UTC) per pair, preferring commence_time; else parse start_time respecting offsets
                    try:
                        gm_dt = gm.copy()
                        # Parse commence_time to UTC
                        ct_parsed = None
                        if "commence_time" in gm_dt.columns:
                            try:
                                ct_parsed = pd.to_datetime(gm_dt["commence_time"].astype(str).str.replace("Z","+00:00", regex=False), errors="coerce", utc=True)
                            except Exception:
                                ct_parsed = None
                        st_parsed = None
                        if "start_time" in gm_dt.columns:
                            try:
                                st_raw = gm_dt["start_time"].astype(str).str.strip().str.replace("Z","+00:00", regex=False)
                                has_off = st_raw.str.contains(r"[+-]\\d{2}:\\d{2}$", regex=True) | st_raw.str.endswith("Z")
                                st_off = pd.to_datetime(st_raw.where(has_off, None), errors="coerce", utc=True)
                                st_naive = pd.to_datetime(st_raw.where(~has_off, None), errors="coerce", utc=False)
                                # Localize naive to schedule tz for start_time
                                try:
                                    import os
                                    from zoneinfo import ZoneInfo
                                    tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                                    try:
                                        sched_tz2 = ZoneInfo(tz_name)
                                    except Exception:
                                        sched_tz2 = None
                                except Exception:
                                    sched_tz2 = None
                                if sched_tz2 is not None and isinstance(st_naive, pd.Series) and st_naive.notna().any():
                                    st_naive = st_naive.map(lambda x: x.replace(tzinfo=sched_tz2) if pd.notna(x) else x)
                                st_parsed = st_off.where(st_off.notna(), st_naive)
                            except Exception:
                                st_parsed = None
                        # Build schedule dt and source label per row
                        if isinstance(ct_parsed, pd.Series):
                            gm_dt["_sch_dt"] = ct_parsed
                            gm_dt["_sch_src"] = np.where(ct_parsed.notna(), "commence", None)
                            if isinstance(st_parsed, pd.Series):
                                # fill where commence missing
                                need_fill = gm_dt["_sch_dt"].isna() & st_parsed.notna()
                                gm_dt.loc[need_fill, "_sch_dt"] = st_parsed[need_fill]
                                gm_dt.loc[need_fill, "_sch_src"] = "start"
                        else:
                            gm_dt["_sch_dt"] = st_parsed if isinstance(st_parsed, pd.Series) else pd.NaT
                            gm_dt["_sch_src"] = np.where((isinstance(st_parsed, pd.Series) and st_parsed.notna()), "start", None)
                        # Build map by pair_key (choose last non-null per pair)
                        mp_dt = None
                        mp_src = None
                        try:
                            tmp_dt = gm_dt.dropna(subset=["_pair_key"]).dropna(subset=["_sch_dt"])
                            mp_dt = tmp_dt.set_index("_pair_key")["_sch_dt"]
                            try:
                                mp_src = tmp_dt.set_index("_pair_key")["_sch_src"]
                            except Exception:
                                mp_src = None
                        except Exception:
                            mp_dt = None
                            mp_src = None
                    except Exception:
                        mp_dt = None
                        mp_src = None

                    # Fill commence_time first
                    if mp_commence is not None:
                        if "commence_time" in df.columns:
                            cm_str = df["commence_time"].astype(str)
                            mask_missing_cm = df["commence_time"].isna() | cm_str.str.strip().eq("")
                            if mask_missing_cm.any():
                                df.loc[mask_missing_cm, "commence_time"] = df.loc[mask_missing_cm, "_pair_key"].map(mp_commence)
                                pipeline_stats["pair_enrichment_commence_applied"] = int(mask_missing_cm.sum())
                        else:
                            df["commence_time"] = df["_pair_key"].map(mp_commence)
                            pipeline_stats["pair_enrichment_commence_applied"] = int(df["commence_time"].notna().sum())
                    # Then start_time
                    if mp_start is not None:
                        if "start_time" in df.columns:
                            st_str = df["start_time"].astype(str)
                            mask_missing_st = df["start_time"].isna() | st_str.str.strip().eq("")
                            if mask_missing_st.any():
                                df.loc[mask_missing_st, "start_time"] = df.loc[mask_missing_st, "_pair_key"].map(mp_start)
                                pipeline_stats["pair_enrichment_start_applied"] = int(mask_missing_st.sum())
                        else:
                            df["start_time"] = df["_pair_key"].map(mp_start)
                            pipeline_stats["pair_enrichment_start_applied"] = int(df["start_time"].notna().sum())
                    # Venue and neutral
                    if mp_venue is not None:
                        if "venue" in df.columns:
                            v_str = df["venue"].astype(str)
                            mask_mv2 = df["venue"].isna() | v_str.str.strip().eq("") | v_str.str.lower().isin(["nan","none","null","nat"]) 
                            if mask_mv2.any():
                                df.loc[mask_mv2, "venue"] = df.loc[mask_mv2, "_pair_key"].map(mp_venue)
                        else:
                            df["venue"] = df["_pair_key"].map(mp_venue)
                    if mp_neutral is not None and "neutral_site" not in df.columns:
                        df["neutral_site"] = df["_pair_key"].map(mp_neutral)
                    # Align _start_dt to schedule only when missing, or when schedule provides a commence_time and differs materially
                    try:
                        if mp_dt is not None:
                            sch_map = mp_dt
                            sch_src_map = mp_src
                            # Ensure _start_dt exists
                            if "_start_dt" not in df.columns:
                                df["_start_dt"] = pd.NaT
                            # Vectorized compare is awkward with mixed tz; apply row-wise safely
                            def _to_utc(x):
                                try:
                                    ts = pd.to_datetime(x, errors='coerce')
                                    if pd.isna(ts):
                                        return pd.NaT
                                    # If tz-aware, convert; else localize as UTC
                                    if getattr(ts, 'tzinfo', None) is None:
                                        return ts.tz_localize('UTC')
                                    return ts.tz_convert('UTC')
                                except Exception:
                                    return pd.NaT
                            def _align_dt(idx):
                                try:
                                    pk = df.at[idx, "_pair_key"]
                                    sch = sch_map.get(pk, None)
                                    sch_src = None
                                    try:
                                        if sch_src_map is not None:
                                            sch_src = sch_src_map.get(pk, None)
                                    except Exception:
                                        sch_src = None
                                    cur = df.at[idx, "_start_dt"]
                                    if sch is None:
                                        return cur
                                    if pd.isna(cur):
                                        return sch
                                    try:
                                        # Normalize both to UTC for diff
                                        cur_utc = _to_utc(cur)
                                        sch_utc = _to_utc(sch)
                                        if pd.isna(cur_utc) or pd.isna(sch_utc):
                                            return cur
                                        delta = abs((cur_utc - sch_utc).total_seconds())
                                        # Only override if schedule source is commence; keep commence-based cur over start_time
                                        if (sch_src == 'commence') and (delta >= 30 * 60):  # 30 minutes
                                            return sch
                                        return cur
                                    except Exception:
                                        return cur
                                except Exception:
                                    return df.at[idx, "_start_dt"]
                            df["_start_dt"] = df.index.to_series().map(_align_dt)
                    except Exception:
                        pipeline_stats["pair_enrichment_dt_align_error"] = True
            except Exception:
                pipeline_stats["pair_enrichment_error"] = True
    except Exception:
        pass

    try:
        if 'start_time' in df.columns and len(df):
            st_raw = df['start_time'].astype(str)
            # Detect timezone offset pattern (e.g. +00:00)
            tz_mask = st_raw.str.contains(r'\+\d{2}:\d{2}')
            # Use end-user driven display timezone (query/cookie/env), fallback to system local
            try:
                import os
                from zoneinfo import ZoneInfo
                try:
                    disp_tz_name = _get_display_tz_name()
                except Exception:
                    disp_tz_name = os.getenv("DISPLAY_TZ") or os.getenv("SCHEDULE_TZ") or "America/New_York"
                try:
                    local_tz = ZoneInfo(disp_tz_name)
                except Exception:
                    local_tz = dt.datetime.now().astimezone().tzinfo
            except Exception:
                local_tz = dt.datetime.now().astimezone().tzinfo
            def _norm_start(v: str) -> str:
                # Attempt offset/UTC-aware parse first; convert to local
                try:
                    ts = pd.to_datetime(v, errors='coerce', utc=True)
                    if ts is not None and not pd.isna(ts):
                        return ts.astimezone(local_tz).strftime('%Y-%m-%d %H:%M')
                except Exception:
                    pass
                # Fallback: naive parse interpreted as LOCAL (previously UTC leading to -offset shift)
                try:
                    ts2 = pd.to_datetime(v, errors='coerce', utc=False)
                    if ts2 is not None and not pd.isna(ts2):
                        if ts2.tzinfo is None:
                            try:
                                ts2 = ts2.replace(tzinfo=local_tz)
                            except Exception:
                                pass
                        else:
                            try:
                                ts2 = ts2.astimezone(local_tz)
                            except Exception:
                                pass
                        return ts2.strftime('%Y-%m-%d %H:%M')
                except Exception:
                    pass
                return v

    # (CAL resolve block moved below CAL enforcement)
                # Attempt offset/UTC-aware parse first; convert to local
                try:
                    ts = pd.to_datetime(v, errors='coerce', utc=True)
                    if ts is not None and not pd.isna(ts):
                        return ts.astimezone(local_tz).strftime('%Y-%m-%d %H:%M')
                except Exception:
                    pass
                # Fallback: naive parse interpreted as LOCAL (previously UTC leading to -offset shift)
                try:
                    ts2 = pd.to_datetime(v, errors='coerce', utc=False)
                    if ts2 is not None and not pd.isna(ts2):
                        if ts2.tzinfo is None:
                            try:
                                ts2 = ts2.replace(tzinfo=local_tz)
                            except Exception:
                                pass
                        else:
                            try:
                                ts2 = ts2.astimezone(local_tz)
                            except Exception:
                                pass
                        return ts2.strftime('%Y-%m-%d %H:%M')
                except Exception:
                    pass
                return v
            # Rows needing normalization (timezone pattern OR non-standard format)
            need_fix_mask = tz_mask | (~st_raw.str.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}$'))
            if need_fix_mask.any():
                df.loc[need_fix_mask, 'start_time_normalized'] = st_raw[need_fix_mask].map(_norm_start)
                norm_vals = df['start_time_normalized']
                good_norm = norm_vals.notna() & norm_vals.astype(str).str.match(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}$')
                df['start_time'] = np.where(good_norm, norm_vals, df['start_time'])
            # Instrumentation
            pipeline_stats['start_time_rows'] = int(len(df))
            pipeline_stats['start_time_tz_pattern_count'] = int(tz_mask.sum())
            pipeline_stats['start_time_normalized_count'] = int(need_fix_mask.sum())
            if 'game_id' in df.columns:
                mmask = df['game_id'].astype(str) == '401827130'
                if mmask.any():
                    pipeline_stats['marshall_start_time_raw'] = st_raw[mmask].iloc[0]
                    pipeline_stats['marshall_start_time_final'] = df['start_time'][mmask].iloc[0]
    except Exception:
        pass

    # ESPN TBD auto-resolution: replace placeholder team names using cached ESPN scoreboard
    # Uses data/cache/espn/<date>.json if available; avoids network calls during request.
    try:
        if not df.empty and {'game_id','home_team','away_team'}.issubset(df.columns):
            bads = {"tbd","t.b.d","tba","t.b.a","to be determined","to-be-determined","to be announced","to-be-announced","unknown","na","n/a","","none","null"}
            def _is_bad_val(x):
                try:
                    return str(x).strip().lower() in bads
                except Exception:
                    return True
            ht_bad0 = df['home_team'].map(_is_bad_val)
            at_bad0 = df['away_team'].map(_is_bad_val)
            if ht_bad0.any() or at_bad0.any():
                # Determine date for cache lookup
                probe_date = str(date_q) if date_q else (
                    df['date'].astype(str).mode().iloc[0] if ('date' in df.columns and not df.empty) else _today_local().strftime('%Y-%m-%d')
                )
                from pathlib import Path as _Path
                cache_json = _Path(__file__).resolve().parents[0] / 'data' / 'cache' / 'espn' / f'{probe_date}.json'
                if cache_json.exists():
                    import json as _json
                    try:
                        payload = _json.loads(cache_json.read_text(encoding='utf-8'))
                    except Exception:
                        payload = None
                    if isinstance(payload, dict):
                        events = payload.get('events') or []
                        team_map = {}
                        for ev in events:
                            try:
                                gid = str(ev.get('id')) if ev.get('id') is not None else None
                                comps = (ev.get('competitions') or [{}])[0]
                                comps_list = comps.get('competitors') or []
                                h = next((c for c in comps_list if c.get('homeAway') == 'home'), None)
                                a = next((c for c in comps_list if c.get('homeAway') == 'away'), None)
                                if gid and h and a:
                                    hname = (h.get('team') or {}).get('displayName') or (h.get('team') or {}).get('shortDisplayName')
                                    aname = (a.get('team') or {}).get('displayName') or (a.get('team') or {}).get('shortDisplayName')
                                    if hname or aname:
                                        team_map[gid] = {'home': hname, 'away': aname}
                            except Exception:
                                continue
                        if team_map:
                            df['game_id'] = df['game_id'].astype(str)
                            replaced = 0
                            def _fill_team(row, side):
                                nonlocal replaced
                                try:
                                    cur = row[side + '_team']
                                    if not _is_bad_val(cur):
                                        return cur
                                    gid = str(row.get('game_id'))
                                    tm = team_map.get(gid)
                                    if tm and tm.get('home') and tm.get('away'):
                                        # Only replace when both names known to avoid partial blanks
                                        newv = tm['home'] if side == 'home' else tm['away']
                                        if newv:
                                            replaced += 1
                                            return newv
                                except Exception:
                                    return row.get(side + '_team')
                                return row.get(side + '_team')
                            df['home_team'] = df.apply(lambda r: _fill_team(r, 'home'), axis=1)
                            df['away_team'] = df.apply(lambda r: _fill_team(r, 'away'), axis=1)
                            pipeline_stats['tbd_resolved_from_espn'] = int(replaced)
                else:
                    pipeline_stats['tbd_resolve_cache_missing'] = True
    except Exception:
        pipeline_stats['tbd_autoresolve_error'] = True

    # Final cleanup: refined placeholder filtering & dedup
    # Drop rows only when BOTH teams are placeholders OR a single placeholder team has no preds/odds and non-digit game_id.
    try:
        if not df.empty:
            bads = {"tbd","t.b.d","tba","t.b.a","to be determined","to-be-determined","to be announced","to-be-announced","unknown","na","n/a","","none","null"}
            def _is_bad_val(x):
                try:
                    return str(x).strip().lower() in bads
                except Exception:
                    return True
            if {"home_team","away_team"}.issubset(df.columns):
                ht_bad = df["home_team"].map(_is_bad_val)
                at_bad = df["away_team"].map(_is_bad_val)
                both_bad = ht_bad & at_bad
                no_pred = (~df.get('pred_total').notna() if 'pred_total' in df.columns else True) & (~df.get('pred_margin').notna() if 'pred_margin' in df.columns else True)
                no_odds = (~df.get('market_total').notna() if 'market_total' in df.columns else True) & (~df.get('spread_home').notna() if 'spread_home' in df.columns else True)
                gid_series = df.get('game_id') if 'game_id' in df.columns else pd.Series(['']*len(df))
                gid_digit = gid_series.astype(str).str.isdigit()
                low_info_placeholder = (ht_bad ^ at_bad) & no_pred & no_odds & (~gid_digit)
                drop_mask = both_bad | low_info_placeholder
                if drop_mask.any():
                    pipeline_stats['placeholder_rows_both'] = int(both_bad.sum())
                    pipeline_stats['placeholder_rows_lowinfo_single'] = int(low_info_placeholder.sum())
                    pipeline_stats['placeholder_rows_dropped_total'] = int(drop_mask.sum())
                    df = df[~drop_mask].copy()
                pipeline_stats['placeholder_logic_refined'] = True
            # If date_q provided, ensure rows align by slate date derived from canonical instant in schedule tz,
            # but keep "date" available for later user-display conversion.
            if date_q:
                try:
                    # Resolve schedule tz
                    try:
                        import os
                        from zoneinfo import ZoneInfo  # py>=3.9
                        tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                        try:
                            sched_tz = ZoneInfo(tz_name)
                        except Exception:
                            sched_tz = None
                    except Exception:
                        sched_tz = None
                    # Derive slate date using multiple timestamp columns (ISO -> commence_time -> commence_time_g -> start_time -> start_time_g)
                    def _col_to_sched_date(series_name: str) -> pd.Series:
                        try:
                            if series_name not in df.columns:
                                return pd.Series([None]*len(df))
                            ser = df[series_name].astype(str).str.strip().str.replace("Z", "+00:00", regex=False)
                            has_offset = ser.str.contains(r"[+-]\\d{2}:\\d{2}$", regex=True) | ser.str.endswith("Z")
                            parsed_off = pd.to_datetime(ser.where(has_offset, None), errors="coerce", utc=True)
                            if sched_tz is not None and isinstance(parsed_off, pd.Series) and parsed_off.notna().any():
                                try:
                                    parsed_off = parsed_off.dt.tz_convert(sched_tz)
                                except Exception:
                                    pass
                            parsed_naive = pd.to_datetime(ser.where(~has_offset, None), errors="coerce", utc=False)
                            if sched_tz is not None and isinstance(parsed_naive, pd.Series) and parsed_naive.notna().any():
                                parsed_naive = parsed_naive.map(lambda x: x.replace(tzinfo=sched_tz) if pd.notna(x) else x)
                            combined_ts = parsed_off.where(parsed_off.notna(), parsed_naive)
                            try:
                                return combined_ts.dt.strftime("%Y-%m-%d")
                            except Exception:
                                return pd.Series([None]*len(df))
                        except Exception:
                            return pd.Series([None]*len(df))

                    candidates = []
                    for cname in ("start_time_iso","commence_time","commence_time_g","start_time","start_time_g"):
                        candidates.append(_col_to_sched_date(cname))
                    # Coalesce first non-null candidate across all series
                    st_date_sched = pd.Series([None]*len(df))
                    for ser in candidates:
                        try:
                            st_date_sched = st_date_sched.where(st_date_sched.notna(), ser)
                        except Exception:
                            pass
                    # Fallback to explicit 'date' column where start_time wasn't usable
                    date_col = df["date"].astype(str) if "date" in df.columns else pd.Series([None]*len(df))
                    slate_date = st_date_sched.where(st_date_sched.notna(), date_col)
                    # Persist for diagnostics; keep separate from final user display date
                    df["_slate_date"] = slate_date
                    try:
                        if "date" in df.columns:
                            mismatch = (slate_date.astype(str) != df["date"].astype(str)) & slate_date.notna() & df["date"].notna()
                            pipeline_stats["slate_date_mismatch_count"] = int(mismatch.sum())
                    except Exception:
                        pass
                    before = len(df)
                    # Prefer canonical UTC start window or Central-local comparison when available
                    sel = slate_date == str(date_q)
                    try:
                        # If canonical start present, accept rows within +/- 12 hours of _start_dt
                        if "_start_dt" in df.columns:
                            start_dt = pd.to_datetime(df["_start_dt"], errors="coerce", utc=True)
                            # Build candidate commence/start columns to compare
                            cand = None
                            for cname in ("commence_time","start_time_iso","start_time"):
                                if cname in df.columns:
                                    try:
                                        cser = pd.to_datetime(df[cname].astype(str).str.replace("Z","+00:00"), errors="coerce", utc=True)
                                        cand = cser if cand is None else cand.where(cand.notna(), cser)
                                    except Exception:
                                        pass
                            if cand is not None:
                                hours = (cand - start_dt).dt.total_seconds().abs() / 3600.0
                                sel = sel | hours.le(12)
                        else:
                            # As a fallback, compare Central-local date for commence/start
                            try:
                                import pytz
                                central = pytz.timezone("America/Chicago")
                                cands = []
                                for cname in ("commence_time","start_time_iso","start_time"):
                                    if cname in df.columns:
                                        try:
                                            cser = pd.to_datetime(df[cname].astype(str).str.replace("Z","+00:00"), errors="coerce", utc=True)
                                            cser = cser.dt.tz_convert(central)
                                            cands.append(cser.dt.strftime("%Y-%m-%d"))
                                        except Exception:
                                            pass
                                if cands:
                                    local_date = pd.Series([None]*len(df))
                                    for s in cands:
                                        local_date = local_date.where(local_date.notna(), s)
                                    sel = sel | (local_date == str(date_q))
                            except Exception:
                                pass
                    except Exception:
                        pass
                    try:
                        sel = sel.reindex(df.index, fill_value=False)
                    except Exception:
                        pass
                    df = df[sel].copy()
                    pipeline_stats["post_time_date_filter_dropped"] = int(before - len(df))
                except Exception:
                    pass
            # Deduplicate by game_id when present, else by teams+start_time
            try:
                before = len(df)
                if "game_id" in df.columns:
                    df = df.drop_duplicates(subset=["game_id"], keep="last")
                elif {"home_team","away_team","start_time"}.issubset(df.columns):
                    df = df.drop_duplicates(subset=["home_team","away_team","start_time"], keep="last")
                pipeline_stats["post_cleanup_dedup_dropped"] = int(before - len(df))
            except Exception:
                pass

            # Secondary deduplication: collapse reversed Home/Away duplicates using unordered pair key per slate day.
            try:
                if {"home_team","away_team"}.issubset(df.columns):
                    # Build unordered pair key and use slate_date when available (fallback to 'date')
                    def _pair_unordered(ht, at):
                        try:
                            a = _canon_slug(str(ht or "").strip())
                            b = _canon_slug(str(at or "").strip())
                            return "::".join(sorted([a, b]))
                        except Exception:
                            return None
                    df["_pair_unordered"] = [
                        _pair_unordered(ht, at) for ht, at in zip(df.get("home_team"), df.get("away_team"))
                    ]
                    sd = df.get("_slate_date") if "_slate_date" in df.columns else (df.get("date") if "date" in df.columns else pd.Series([None]*len(df)))
                    df["_pair_slate_key"] = df["_pair_unordered"].astype(str) + "@" + sd.astype(str)
                    # Row quality score to keep the best representative per matchup/day
                    def _score_row(r):
                        s = 0.0
                        gid = str(r.get("game_id") or "")
                        if gid and gid.isdigit():
                            s += 4.0
                        if gid and not gid.startswith("odds:"):
                            s += 2.0
                        if pd.notna(pd.to_numeric(r.get("market_total"), errors="coerce")) or pd.notna(pd.to_numeric(r.get("spread_home"), errors="coerce")):
                            s += 1.0
                        if pd.notna(pd.to_numeric(r.get("pred_total"), errors="coerce")) or pd.notna(pd.to_numeric(r.get("pred_margin"), errors="coerce")):
                            s += 1.0
                        # Prefer rows with proper commence_time (UTC) or normalized start_time
                        if pd.notna(pd.to_datetime(r.get("commence_time"), errors="coerce", utc=True)):
                            s += 0.35
                        if pd.notna(pd.to_datetime(r.get("start_time"), errors="coerce")):
                            s += 0.2
                        return s
                    try:
                        df["_row_score"] = df.apply(_score_row, axis=1)
                        before2 = len(df)
                        df = df.sort_values(["_pair_slate_key","_row_score"], ascending=[True, False])
                        df = df.drop_duplicates(subset=["_pair_slate_key"], keep="first")
                        pipeline_stats["post_pair_dedup_dropped"] = int(before2 - len(df))
                    except Exception:
                        pass
            except Exception:
                pass
    except Exception:
        pass

    # Post-clean enrichment persistence: record remaining full placeholders (should be zero after refined logic)
    try:
        if not df.empty and {'home_team','away_team'}.issubset(df.columns):
            bads = {"tbd","t.b.d","tba","t.b.a","to be determined","to-be-determined","to be announced","to-be-announced","unknown","na","n/a","","none","null"}
            ht_lower = df['home_team'].astype(str).str.strip().str.lower()
            at_lower = df['away_team'].astype(str).str.strip().str.lower()
            both_bad_post = ht_lower.isin(bads) & at_lower.isin(bads)
            if both_bad_post.any():
                pipeline_stats['postclean_remaining_full_placeholder_rows'] = int(both_bad_post.sum())
        # Schedule parity diagnostics
        try:
            if 'game_id' in df.columns and 'game_id' in games_all.columns:
                if date_q and 'date' in games_all.columns:
                    sched_mask = games_all['date'].astype(str) == str(date_q)
                    scheduled_ids = set(games_all.loc[sched_mask, 'game_id'].astype(str))
                else:
                    scheduled_ids = set(games_all['game_id'].astype(str))
                final_ids = set(df['game_id'].astype(str))
                missing_ids = sorted(list(scheduled_ids - final_ids))
                pipeline_stats['scheduled_game_count'] = int(len(scheduled_ids))
                pipeline_stats['final_game_count'] = int(len(final_ids))
                pipeline_stats['scheduled_missing_game_ids'] = missing_ids
                pipeline_stats['scheduled_missing_count'] = int(len(missing_ids))
                if missing_ids:
                    pipeline_stats['coverage_escalation_missing_games'] = True
                # ESPN parity probe ingestion: attach detailed alignment diagnostics if probe JSON exists
                try:
                    # Derive probe date (prefer explicit query date)
                    probe_date = str(date_q) if date_q else (
                        df['date'].astype(str).mode().iloc[0] if ('date' in df.columns and not df.empty) else _today_local().strftime('%Y-%m-%d')
                    )
                    probe_path = OUT / f'schedule_alignment_probe_{probe_date}.json'
                    if probe_path.exists():
                        import json as _json
                        _probe = _json.loads(probe_path.read_text())
                        pipeline_stats['espn_parity_ok'] = bool(_probe.get('parity_ok'))
                        pipeline_stats['espn_parity_placeholder_rows'] = int(_probe.get('placeholder_rows', 0) or 0)
                        pipeline_stats['espn_parity_missing_ids'] = _probe.get('espn_missing_ids', [])
                        pipeline_stats['espn_parity_local_extra_ids'] = _probe.get('local_extra_ids', [])
                        pipeline_stats['espn_parity_espn_ids_count'] = int(_probe.get('espn_ids_count', 0) or 0)
                        pipeline_stats['espn_parity_games_curr_rows'] = int(_probe.get('games_curr_rows', 0) or 0)
                        pipeline_stats['espn_parity_enriched_rows'] = int(_probe.get('enriched_rows', 0) or 0)
                        pipeline_stats['espn_parity_probe_path'] = str(probe_path.name)
                        if not _probe.get('parity_ok'):
                            # Escalation flag specific to ESPN parity failures
                            pipeline_stats['coverage_escalation_parity'] = True
                except Exception:
                    pipeline_stats['espn_parity_probe_error'] = True
        except Exception:
            pipeline_stats['schedule_parity_error'] = True
        # Determine target date for write
        if date_q:
            t_date3 = str(date_q)
        else:
            if 'date' in df.columns and not df.empty:
                try:
                    t_date3 = df['date'].astype(str).mode().iloc[0]
                except Exception:
                    t_date3 = _today_local().strftime('%Y-%m-%d')
            else:
                t_date3 = _today_local().strftime('%Y-%m-%d')
        enriched_path3 = OUT / f'predictions_unified_enriched_{t_date3}.csv'
        df.to_csv(enriched_path3, index=False)
        pipeline_stats['enriched_unified_final_rows'] = int(len(df))
        pipeline_stats['enriched_unified_final'] = str(enriched_path3.name)
        # Export missing real coverage rows
        try:
            if 'game_id' in df.columns:
                miss_mask_cov = (
                    (df.get('pred_total').isna() if 'pred_total' in df.columns else False) |
                    (df.get('pred_margin').isna() if 'pred_margin' in df.columns else False) |
                    (df.get('market_total').isna() if 'market_total' in df.columns else False) |
                    (df.get('spread_home').isna() if 'spread_home' in df.columns else False)
                )
                if isinstance(miss_mask_cov, pd.Series) and miss_mask_cov.any():
                    need_cols = [c for c in ['game_id','home_team','away_team','pred_total','pred_margin','market_total','spread_home','pred_total_model','pred_margin_model'] if c in df.columns]
                    miss_df = df.loc[miss_mask_cov, need_cols].copy()
                    miss_out = OUT / f'missing_real_coverage_{t_date3}.csv'
                    miss_df.to_csv(miss_out, index=False)
                    pipeline_stats['missing_real_coverage_rows'] = int(len(miss_df))
                    pipeline_stats['missing_real_coverage_path'] = miss_out.name
        except Exception:
            pipeline_stats['missing_real_coverage_export_error'] = True
    except Exception:
        pipeline_stats['enriched_unified_final_error'] = True

    # Fallback: attach closing lines from games_with_closing_<date>.csv (preferred) or games_with_closing.csv if missing
    if ("game_id" in df.columns):
        try:
            # Prefer date-specific artifact when browsing a specific slate; else fallback to undated file
            closing_path = None
            if date_q:
                cand = OUT / f"games_with_closing_{date_q}.csv"
                if cand.exists():
                    closing_path = cand
            if closing_path is None:
                closing_path = OUT / "games_with_closing.csv"
            if closing_path.exists():
                # Stabilize dtypes to avoid mixed-type warnings during downstream joins
                try:
                    cl = pd.read_csv(closing_path, low_memory=False)
                except Exception:
                    cl = pd.read_csv(closing_path)
                if not cl.empty and "game_id" in cl.columns:
                    cl["game_id"] = cl["game_id"].astype(str)
                    # Build canonical pair keys to enable fallback joins when game_id mismatches
                    try:
                        ch_col = next((c for c in ["home_team","home_team_name","home"] if c in cl.columns), None)
                        ca_col = next((c for c in ["away_team","away_team_name","away"] if c in cl.columns), None)
                        if ch_col and ca_col:
                            cl["_home_norm"] = cl[ch_col].astype(str).map(_canon_slug)
                            cl["_away_norm"] = cl[ca_col].astype(str).map(_canon_slug)
                            cl["_pair_key"] = cl.apply(lambda r: "::".join(sorted([r.get("_home_norm"), r.get("_away_norm")])), axis=1)
                            if {"home_team","away_team"}.issubset(df.columns):
                                df["_home_norm"] = df["home_team"].astype(str).map(_canon_slug)
                                df["_away_norm"] = df["away_team"].astype(str).map(_canon_slug)
                                df["_pair_key"] = df.apply(lambda r: "::".join(sorted([r.get("_home_norm"), r.get("_away_norm")])), axis=1)
                    except Exception:
                        pass
                    # closing_total may not be precomputed; derive from totals market median per game
                    if "closing_total" not in cl.columns and {"market","total"}.issubset(cl.columns):
                        try:
                            totals_fg = cl[(cl["market"].astype(str).str.lower()=="totals") & (cl["period"].astype(str).str.lower().isin(["full_game","full game","fg"]))]
                            if not totals_fg.empty:
                                med_tot = totals_fg.groupby("game_id")["total"].median().rename("closing_total")
                                cl = cl.merge(med_tot, on="game_id", how="left")
                        except Exception:
                            pass
                    # closing spread median from spreads market
                    closing_spread_series = None
                    try:
                        if {"market","home_spread"}.issubset(cl.columns):
                            spreads_fg = cl[(cl["market"].astype(str).str.lower()=="spreads") & (cl["period"].astype(str).str.lower().isin(["full_game","full game","fg"]))]
                            if not spreads_fg.empty:
                                closing_spread_series = spreads_fg.groupby("game_id")["home_spread"].median().rename("closing_spread_home")
                    except Exception:
                        closing_spread_series = None
                    # Merge closing_total if column absent or entirely NaN; also coalesce market_total NaNs from closing_total
                    if "closing_total" in cl.columns:
                        med_ct = cl.groupby("game_id")["closing_total"].median().rename("closing_total")
                        if "closing_total" not in df.columns or df["closing_total"].isna().all():
                            df = df.merge(med_ct, on="game_id", how="left") if "closing_total" not in df.columns else df
                        # Coalesce market_total from closing_total when market_total exists but is NaN (common for past dates)
                        if "market_total" in df.columns:
                            try:
                                df["market_total"] = df["market_total"].where(df["market_total"].notna(), df.get("closing_total"))
                            except Exception:
                                pass
                        else:
                            df["market_total"] = df.get("closing_total")
                    # Merge closing spread
                    if closing_spread_series is not None and not closing_spread_series.empty and "closing_spread_home" not in df.columns:
                        df = df.merge(closing_spread_series, on="game_id", how="left")
                    # Coalesce spread_home from closing_spread_home when missing
                    if "closing_spread_home" in df.columns:
                        if "spread_home" in df.columns:
                            try:
                                df["spread_home"] = df["spread_home"].where(df["spread_home"].notna(), df.get("closing_spread_home"))
                            except Exception:
                                pass
                        else:
                            df["spread_home"] = df.get("closing_spread_home")
                        # Pair-key based coalesce for spreads + totals if still missing
                        try:
                            if "_pair_key" in df.columns and "_pair_key" in cl.columns:
                                # Totals
                                if "market_total" in df.columns and df["market_total"].isna().sum() > 0 and "closing_total" in cl.columns:
                                    pair_tot = cl.groupby("_pair_key")["closing_total"].median().rename("_closing_total_pair")
                                    df = df.merge(pair_tot, on="_pair_key", how="left")
                                    df["market_total"] = df["market_total"].where(df["market_total"].notna(), df.get("_closing_total_pair"))
                                    if "closing_total" in df.columns:
                                        df["closing_total"] = df["closing_total"].where(df["closing_total"].notna(), df.get("_closing_total_pair"))
                                # Spreads
                                if df["spread_home"].isna().sum() > 0 and "home_spread" in cl.columns:
                                    pair_sp = cl.groupby("_pair_key")["home_spread"].median().rename("_closing_spread_pair")
                                    df = df.merge(pair_sp, on="_pair_key", how="left")
                                    df["spread_home"] = df["spread_home"].where(df["spread_home"].notna(), df.get("_closing_spread_pair"))
                                    if "closing_spread_home" in df.columns:
                                        df["closing_spread_home"] = df["closing_spread_home"].where(df["closing_spread_home"].notna(), df.get("_closing_spread_pair"))
                        except Exception:
                            pass
        except Exception:
            pass

    # Ingest spread logistic calibration constant if available
    try:
        cal_path = OUT / 'calibration_spread_logistic.json'
        if cal_path.exists():
            cal_payload = _json.loads(cal_path.read_text(encoding='utf-8'))
            if isinstance(cal_payload, dict):
                pipeline_stats['spread_logistic_K'] = cal_payload.get('best_K')
                pipeline_stats['spread_logistic_rows'] = cal_payload.get('n_rows')
                pipeline_stats['spread_logistic_generated_at'] = cal_payload.get('generated_at')
    except Exception:
        pipeline_stats['spread_logistic_error'] = True

    # Kelly suggestion (spread): compute fraction using calibrated K (if available) and current spread vs model margin
    try:
        if 'pred_margin' in df.columns and ('spread_home' in df.columns or 'closing_spread_home' in df.columns):
            # Prefer closing spread when available
            if 'spread_home' in df.columns:
                base_spread = df['spread_home']
            else:
                base_spread = df['closing_spread_home']
            pm = pd.to_numeric(df['pred_margin'], errors='coerce')
            sh = pd.to_numeric(base_spread, errors='coerce')
            K = pipeline_stats.get('spread_logistic_K', None)
            if K is None or (isinstance(K, float) and (K <= 0 or pd.isna(K))):
                K = 0.115
            def _kelly(p: float, b: float = 0.909) -> float:
                try:
                    return max(0.0, min(1.0, ((p*(b+1) - 1)/b)))
                except Exception:
                    return 0.0
            # Compute cover probability and kelly fraction per row
            diff = pm - sh
            # Guard where values missing
            diff = diff.where(~(pm.isna() | sh.isna()), np.nan)
            p_cover = 1.0/(1.0 + np.exp(-(diff.astype(float) / float(K))))
            df['kelly_frac_spread'] = p_cover.map(lambda p: _kelly(p) if pd.notna(p) else np.nan)
            # Also expose direction suggestion: HOME if model favors home w.r.t line, else AWAY
            df['kelly_side_spread'] = np.where(diff > 0, 'HOME', np.where(diff < 0, 'AWAY', None))
            pipeline_stats['kelly_spread_populated'] = int(df['kelly_frac_spread'].notna().sum())
    except Exception:
        pipeline_stats['kelly_spread_error'] = True

    # Prediction fallback enrichment: ensure we rarely render a card with odds but no predictions.
    # Removes previous logic that hid odds when predictions were missing; instead we synthesize a baseline prediction.
    try:
        # Ensure prediction columns always exist so fallback logic executes even when model merge failed entirely.
        if "pred_total" not in df.columns:
            df["pred_total"] = np.nan
            pipeline_stats["pred_total_column_injected"] = True
        else:
            pipeline_stats["pred_total_column_injected"] = False
        if "pred_margin" not in df.columns:
            df["pred_margin"] = np.nan
            pipeline_stats["pred_margin_column_injected"] = True
        else:
            pipeline_stats["pred_margin_column_injected"] = False
        if "pred_total" in df.columns:
            # Coerce textual placeholders ('nan','None','') to actual NaN before missing mask
            try:
                df["pred_total"] = pd.to_numeric(df["pred_total"], errors="coerce")
            except Exception:
                pass
            if "pred_margin" in df.columns:
                try:
                    df["pred_margin"] = pd.to_numeric(df["pred_margin"], errors="coerce")
                except Exception:
                    pass
            mt_series = pd.to_numeric(df.get("market_total"), errors="coerce") if "market_total" in df.columns else None
            pm_series = pd.to_numeric(df.get("pred_margin"), errors="coerce") if "pred_margin" in df.columns else None
            missing_pred = df["pred_total"].isna()
            pipeline_stats["missing_pred_total_rows_initial"] = int(missing_pred.sum())
            # Instrumentation: capture column presence and a small sample of missing prediction rows pre-fill
            try:
                cols_needed = [
                    "market_total","home_tempo_rating","away_tempo_rating","home_rtg","away_rtg",
                    "spread_home","closing_spread_home","home_team","away_team","pred_margin","closing_total"
                ]
                pipeline_stats["pred_pipeline_columns_present"] = [c for c in cols_needed if c in df.columns]
                pipeline_stats["pred_total_missing_initial"] = int(missing_pred.sum())
                if missing_pred.any():
                    sample_cols = [
                        "home_team","away_team","market_total","closing_total","pred_total","pred_total_basis",
                        "home_tempo_rating","away_tempo_rating","home_rtg","away_rtg","spread_home","pred_margin"
                    ]
                    sample_view_cols = [c for c in sample_cols if c in df.columns]
                    pipeline_stats["pred_total_missing_sample"] = (
                        df.loc[missing_pred, sample_view_cols].head(5).to_dict(orient="records")
                    )
            except Exception:
                pass
            # If market_total column itself missing, create it from closing_total or leave None so downstream logic can still inspect.
            if mt_series is None:
                if "closing_total" in df.columns:
                    df["market_total"] = pd.to_numeric(df["closing_total"], errors="coerce")
                else:
                    df["market_total"] = np.nan
                mt_series = pd.to_numeric(df.get("market_total"), errors="coerce")
            # If entire market_total is NaN, we'll still attempt a tempo-based synthetic baseline.
            all_market_nan = mt_series.isna().all() if mt_series is not None else True
            # Fallback 1 (revised): synthetic baseline when pred_total missing.
            # Older behavior copied market_total verbatim, collapsing edge variance.
            # New: blend league average, optional tempo, partial market anchor, deterministic noise.
            if missing_pred.any():
                can_fill = missing_pred & mt_series.notna()
                if can_fill.any():
                    import zlib
                    # Reduce reliance on a fixed league baseline to avoid uniform predictions
                    baseline_league_avg = None
                    if {"home_tempo_rating","away_tempo_rating"}.issubset(df.columns):
                        ht = pd.to_numeric(df.get("home_tempo_rating"), errors="coerce")
                        at = pd.to_numeric(df.get("away_tempo_rating"), errors="coerce")
                        tempo_avg_series = np.where(ht.notna() & at.notna(), (ht+at)/2.0, np.nan)
                    else:
                        tempo_avg_series = np.full(len(df), np.nan)
                    def _stable_noise(home, away):
                        try:
                            key = f"{str(home)}::{str(away)}"
                            code = zlib.adler32(key.encode())
                            return ((code % 1000)/1000.0 - 0.5) * 3.2
                        except Exception:
                            return 0.0
                    for idx in df.index[can_fill]:
                        mt_val = mt_series.loc[idx]
                        h = df.at[idx, "home_team"] if "home_team" in df.columns else ""
                        a = df.at[idx, "away_team"] if "away_team" in df.columns else ""
                        noise = _stable_noise(h, a)
                        tempo_avg = tempo_avg_series[idx] if not (isinstance(tempo_avg_series, float) or pd.isna(tempo_avg_series[idx])) else np.nan
                        tempo_component = ((tempo_avg - 70.0) * 0.80) if (not pd.isna(tempo_avg)) else 0.0
                        # Anchor primarily to market with tempo adjustment and small deterministic noise
                        val = float(mt_val) + tempo_component + noise
                        if abs(val - mt_val) < 0.4:
                            val = val + (1.15 if val <= mt_val else -1.15)
                        # Guardrail: prevent implausibly low totals. Clamp to at least 110.
                        pre_clip = val
                        val = float(np.clip(val, 110, 188))
                        if val > pre_clip and pre_clip < 110:
                            try:
                                pipeline_stats.setdefault('low_total_guard_applied', 0)
                                pipeline_stats['low_total_guard_applied'] += 1
                            except Exception:
                                pass
                        try:
                            pipeline_stats.setdefault('synthetic_baseline_fills', 0)
                            pipeline_stats['synthetic_baseline_fills'] += 1
                            pipeline_stats.setdefault('synthetic_baseline_vals', []).append(val)
                        except Exception:
                            pass
                        df.at[idx, "pred_total"] = val
                        # Ensure basis dtype is object to avoid FutureWarning when assigning strings
                        if "pred_total_basis" in df.columns and df["pred_total_basis"].dtype != object:
                            try:
                                df["pred_total_basis"] = df["pred_total_basis"].astype("object")
                            except Exception:
                                pass
                        if "pred_total_basis" in df.columns and pd.isna(df.at[idx, "pred_total_basis"]):
                            df.at[idx, "pred_total_basis"] = "synthetic_baseline"
                        elif "pred_total_basis" not in df.columns:
                            df.loc[idx, "pred_total_basis"] = "synthetic_baseline"
                # Secondary path: fill rows where market_total is NaN (or entire column NaN) using pure league avg + tempo/noise.
                if (all_market_nan or (missing_pred & mt_series.isna()).any()):
                    import zlib
                    baseline_league_avg = 141.5
                    if {"home_tempo_rating","away_tempo_rating"}.issubset(df.columns):
                        ht = pd.to_numeric(df.get("home_tempo_rating"), errors="coerce")
                        at = pd.to_numeric(df.get("away_tempo_rating"), errors="coerce")
                        tempo_avg_series = np.where(ht.notna() & at.notna(), (ht+at)/2.0, np.nan)
                    else:
                        tempo_avg_series = np.full(len(df), np.nan)
                    def _stable_noise2(home, away):
                        try:
                            key = f"{str(home)}::{str(away)}"
                            code = zlib.adler32(key.encode())
                            return ((code % 1000)/1000.0 - 0.5) * 5.0
                        except Exception:
                            return 0.0
                    can_fill2 = missing_pred & df["pred_total"].isna()
                    for idx in df.index[can_fill2]:
                        h = df.at[idx, "home_team"] if "home_team" in df.columns else ""
                        a = df.at[idx, "away_team"] if "away_team" in df.columns else ""
                        noise = _stable_noise2(h, a)
                        tempo_avg = tempo_avg_series[idx] if not (isinstance(tempo_avg_series, float) or pd.isna(tempo_avg_series[idx])) else np.nan
                        tempo_component = ((tempo_avg - 70.0) * 0.85) if (not pd.isna(tempo_avg)) else 0.0
                        # No market anchor: use league baseline with stronger tempo and noise
                        val = baseline_league_avg + tempo_component + noise
                        # Guardrail for no-market synthetic baseline
                        pre_clip2 = val
                        val = float(np.clip(val, 110, 192))
                        if val > pre_clip2 and pre_clip2 < 110:
                            try:
                                pipeline_stats.setdefault('low_total_guard_applied', 0)
                                pipeline_stats['low_total_guard_applied'] += 1
                            except Exception:
                                pass
                        df.at[idx, "pred_total"] = val
                        if "pred_total_basis" in df.columns and df["pred_total_basis"].dtype != object:
                            try:
                                df["pred_total_basis"] = df["pred_total_basis"].astype("object")
                            except Exception:
                                pass
                        try:
                            pipeline_stats.setdefault("synthetic_baseline_fills_no_market", 0)
                            pipeline_stats["synthetic_baseline_fills_no_market"] += 1
                        except Exception:
                            pass
                        if "pred_total_basis" in df.columns and pd.isna(df.at[idx, "pred_total_basis"]):
                            df.at[idx, "pred_total_basis"] = "synthetic_baseline_nomkt"
                        elif "pred_total_basis" not in df.columns:
                            df.loc[idx, "pred_total_basis"] = "synthetic_baseline_nomkt"
            # Post synthetic fill instrumentation
            try:
                post_missing = df["pred_total"].isna()
                pipeline_stats["pred_total_missing_post_fill"] = int(post_missing.sum())
                if (pipeline_stats.get("synthetic_baseline_fills", 0) + pipeline_stats.get("synthetic_baseline_fills_no_market", 0)) > 0:
                    sample_cols2 = [
                        "home_team","away_team","market_total","pred_total","pred_total_basis",
                        "home_tempo_rating","away_tempo_rating","home_rtg","away_rtg","spread_home","pred_margin"
                    ]
                    view_cols2 = [c for c in sample_cols2 if c in df.columns]
                    pipeline_stats["pred_total_filled_sample"] = (
                        df.loc[~post_missing, view_cols2].head(5).to_dict(orient="records")
                    )
            except Exception:
                pass
            # Fallback 2: derive team projections if absent using pred_total & pred_margin
            if {"pred_total","pred_margin"}.issubset(df.columns):
                pt = pd.to_numeric(df["pred_total"], errors="coerce")
                pm2 = pd.to_numeric(df["pred_margin"], errors="coerce")
                need_proj_home = ("proj_home" not in df.columns) or df["proj_home"].isna().all()
                need_proj_away = ("proj_away" not in df.columns) or df["proj_away"].isna().all()
                if need_proj_home:
                    df["proj_home"] = np.where(pt.notna() & pm2.notna(), (pt/2) + (pm2/2), df.get("proj_home"))
                if need_proj_away:
                    # Use proj_home if just created
                    if "proj_home" in df.columns:
                        ph = pd.to_numeric(df["proj_home"], errors="coerce")
                        df["proj_away"] = np.where(pt.notna() & ph.notna(), pt - ph, df.get("proj_away"))
                # Mark adjusted flag when basis is copied from market (for template optional badge)
                if "pred_total_basis" in df.columns:
                    df["pred_total_adjusted"] = np.where(df["pred_total_basis"].isin(["market_copy","blended_low"]), True, df.get("pred_total_adjusted"))
            # Fallback 3: create pred_margin from spread or rating differentials if entirely missing
            if "pred_margin" in df.columns and df["pred_margin"].isna().all():
                spread_src = None
                for cand in ["spread_home","closing_spread_home"]:
                    if cand in df.columns:
                        spread_src = pd.to_numeric(df[cand], errors="coerce")
                        break
                if spread_src is not None and spread_src.notna().any():
                    df["pred_margin"] = spread_src.map(lambda x: -x if pd.notna(x) else x)
                    df["pred_margin_basis"] = df.get("pred_margin_basis")
                    if "pred_margin_basis" in df.columns:
                        df["pred_margin_basis"] = df["pred_margin_basis"].where(df["pred_margin_basis"].notna(), "synthetic_from_spread")
                else:
                    # Rating differential approach (offensive/defensive or power ratings)
                    if {"home_rtg","away_rtg"}.issubset(df.columns):
                        hr = pd.to_numeric(df["home_rtg"], errors="coerce")
                        ar = pd.to_numeric(df["away_rtg"], errors="coerce")
                        diff = hr - ar
                        if diff.notna().any():
                            df["pred_margin"] = diff
                            if "pred_margin_basis" in df.columns:
                                df["pred_margin_basis"] = df["pred_margin_basis"].where(df["pred_margin_basis"].notna(), "synthetic_rating_diff")
            # Fallback 3b: if still missing pred_margin (no spread, no ratings), force even margin to enable projections
            try:
                if "pred_margin" in df.columns:
                    remaining_pm_nan = df["pred_margin"].isna()
                    if remaining_pm_nan.any():
                        df.loc[remaining_pm_nan, "pred_margin"] = 0.0
                        if "pred_margin_basis" not in df.columns:
                            df["pred_margin_basis"] = None
                        df.loc[remaining_pm_nan, "pred_margin_basis"] = df.loc[remaining_pm_nan, "pred_margin_basis"].where(df.loc[remaining_pm_nan, "pred_margin_basis"].notna(), "synthetic_even")
                        pipeline_stats["pred_margin_even_fills"] = int(remaining_pm_nan.sum())
            except Exception:
                pass
            # Projection population (unconditional): build proj_home/proj_away wherever both pred_total & pred_margin exist
            try:
                if {"pred_total","pred_margin"}.issubset(df.columns):
                    pt_num = pd.to_numeric(df["pred_total"], errors="coerce")
                    pm_num = pd.to_numeric(df["pred_margin"], errors="coerce")
                    if "proj_home" not in df.columns:
                        df["proj_home"] = np.nan
                    if "proj_away" not in df.columns:
                        df["proj_away"] = np.nan
                    mask_proj = pt_num.notna() & pm_num.notna()
                    df.loc[mask_proj, "proj_home"] = (pt_num[mask_proj] / 2.0) + (pm_num[mask_proj] / 2.0)
                    df.loc[mask_proj, "proj_away"] = pt_num[mask_proj] - df.loc[mask_proj, "proj_home"]
                    pipeline_stats["proj_home_rows"] = int(df["proj_home"].notna().sum())
                    pipeline_stats["proj_away_rows"] = int(df["proj_away"].notna().sum())
            except Exception:
                pipeline_stats["proj_population_error"] = True
            # Instrument missing prediction counts
            try:
                pipeline_stats["missing_pred_total_rows"] = int(df["pred_total"].isna().sum())
                pipeline_stats["missing_pred_margin_rows"] = int(df["pred_margin"].isna().sum())
                # Uniform prediction diagnostic (detect collapse like constant 112)
                pt_diag = pd.to_numeric(df.get("pred_total"), errors="coerce")
                if pt_diag.notna().sum() > 8:
                    vc = pt_diag.value_counts()
                    top_frac = vc.iloc[0] / pt_diag.notna().sum() if len(vc) else 0
                    pipeline_stats["pred_total_unique_count"] = int(pt_diag.nunique())
                    pipeline_stats["pred_total_top_value"] = float(vc.index[0]) if len(vc) else None
                    pipeline_stats["pred_total_top_fraction"] = float(top_frac)
                    if top_frac > 0.90:
                        pipeline_stats["pred_total_uniform_flag"] = True
                        pipeline_stats.setdefault("warnings", []).append(
                            f"pred_total uniform flag: {vc.index[0]} appears in {top_frac:.2%} of rows"
                        )
                        # Capture small sample for debugging
                        sample_u_cols = [c for c in ["game_id","home_team","away_team","pred_total","pred_total_basis","market_total","derived_total"] if c in df.columns]
                        pipeline_stats["pred_total_uniform_sample"] = df[sample_u_cols].head(10).to_dict(orient="records")
                        # Default mitigation: treat uniform synthetic totals as pending unless explicitly disabled.
                        # Disable by setting NCAAB_UNIFORM_NULL to 0/false/no.
                        try:
                            flag = os.getenv("NCAAB_UNIFORM_NULL", "").strip().lower()
                            enable_uniform_null = flag not in ("0","false","no")
                        except Exception:
                            enable_uniform_null = True
                        if enable_uniform_null:
                            pipeline_stats["uniform_null_applied"] = True
                            uniform_val = vc.index[0]
                            mask_uniform = pd.to_numeric(df.get("pred_total"), errors="coerce") == float(uniform_val)
                            # Null only rows entirely uniform to keep any legitimately distinct future inserts.
                            df.loc[mask_uniform, "pred_total"] = np.nan
                            if "pred_total_basis" in df.columns:
                                df.loc[mask_uniform, "pred_total_basis"] = df.loc[mask_uniform, "pred_total_basis"].where(df.loc[mask_uniform, "pred_total_basis"].notna(), "uniform_null")
                            if "pred_margin" in df.columns:
                                # If margin shows little variance as well, null it; else keep.
                                pmv = pd.to_numeric(df.get("pred_margin"), errors="coerce")
                                if pmv.notna().sum() and (pmv.nunique() <= 2):
                                    df.loc[mask_uniform, "pred_margin"] = np.nan
                                    if "pred_margin_basis" in df.columns:
                                        df.loc[mask_uniform, "pred_margin_basis"] = df.loc[mask_uniform, "pred_margin_basis"].where(df.loc[mask_uniform, "pred_margin_basis"].notna(), "uniform_null")
                        # Strict remediation path: if NCAAB_UNIFORM_STRICT=1 and the uniform value looks like a synthetic fallback
                        # (e.g., classic 112 collapse or variance < 1.0 across all non-null totals), aggressively null synthetic rows
                        # but retain any legitimately differing future inserts. This prevents the UI from displaying a misleading
                        # constant total while still allowing later promotion of real model outputs.
                        try:
                            if os.getenv("NCAAB_UNIFORM_STRICT", "").strip().lower() in ("1","true","yes"):
                                ptv_all = pd.to_numeric(df.get("pred_total"), errors="coerce")
                                if ptv_all.notna().any():
                                    variance = float(ptv_all.max() - ptv_all.min()) if ptv_all.notna().sum() else 0.0
                                    uniform_val = float(vc.index[0])
                                    looks_synthetic = (uniform_val == 112.0) or (variance < 1.0)
                                    if looks_synthetic:
                                        # Only null rows whose basis is synthetic to avoid removing any legit early distinct predictions.
                                        if "pred_total_basis" in df.columns:
                                            basis_ser = df.get("pred_total_basis").astype(str).str.lower()
                                            mask_synth = basis_ser.isin(["synthetic_baseline","synthetic_baseline_nomkt","market_copy","blended_low"])
                                        else:
                                            mask_synth = pd.Series([True] * len(df))
                                        if mask_synth.any():
                                            df.loc[mask_synth, "pred_total"] = np.nan
                                            if "pred_total_basis" in df.columns:
                                                df.loc[mask_synth, "pred_total_basis"] = df.loc[mask_synth, "pred_total_basis"].where(df.loc[mask_synth, "pred_total_basis"].notna(), "uniform_strict_null")
                                            if "pred_margin" in df.columns:
                                                pmv_all = pd.to_numeric(df.get("pred_margin"), errors="coerce")
                                                if pmv_all.notna().sum() and (pmv_all.nunique() <= 2):
                                                    df.loc[mask_synth, "pred_margin"] = np.nan
                                                    if "pred_margin_basis" in df.columns:
                                                        df.loc[mask_synth, "pred_margin_basis"] = df.loc[mask_synth, "pred_margin_basis"].where(df.loc[mask_synth, "pred_margin_basis"].notna(), "uniform_strict_null")
                                            pipeline_stats["uniform_strict_null_applied"] = True
                                            pipeline_stats.setdefault("warnings", []).append("uniform strict null applied")
                        except Exception:
                            pass
            except Exception:
                pass
            # Synthetic line fallback: if market_total still missing, populate from pred_total or derived_total
            try:
                if "market_total" in df.columns:
                    miss_mt = df["market_total"].isna() | df["market_total"].astype(str).str.lower().isin(["nan","none","null",""])
                    if miss_mt.any():
                        if "pred_total" in df.columns:
                            pmiss = miss_mt & df["pred_total"].notna()
                            if pmiss.any():
                                df.loc[pmiss, "market_total"] = df.loc[pmiss, "pred_total"]
                                df.loc[pmiss, "market_total_basis"] = "synthetic_pred"
                        if "derived_total" in df.columns:
                            rem = miss_mt & df["market_total"].isna() & df["derived_total"].notna()
                            if rem.any():
                                df.loc[rem, "market_total"] = df.loc[rem, "derived_total"]
                                df.loc[rem, "market_total_basis"] = "synthetic_derived"
                # Synthetic spread fallback
                if {"spread_home","pred_margin"}.issubset(df.columns):
                    miss_sp = df["spread_home"].isna() | df["spread_home"].astype(str).str.lower().isin(["nan","none","null",""])
                    if miss_sp.any():
                        pmv = pd.to_numeric(df["pred_margin"], errors="coerce")
                        fill_mask = miss_sp & pmv.notna()
                        if fill_mask.any():
                            # Home favored => negative spread
                            df.loc[fill_mask, "spread_home"] = -pmv[fill_mask]
                            df.loc[fill_mask, "spread_home_basis"] = "synthetic_pred_margin"
            except Exception:
                pass
    except Exception:
        pass

    # Targeted per-row fuzzy odds fill for residual missing lines (row-level, regardless of global missing share)
    try:
        from rapidfuzz import process, fuzz  # type: ignore
        if date_q:
            # Resolve a raw odds file for the selected date. Prefer odds_today.csv for today, else odds_<date>.csv; always also consider odds_history.
            candidates = []
            if today_str and date_q == today_str:
                candidates.append(OUT / "odds_today.csv")
            candidates.append(OUT / f"odds_{date_q}.csv")
            candidates.append(OUT / "odds_history" / f"odds_{date_q}.csv")
            # As a last resort, glob any variant odds_<date>_*.csv under outputs and odds_history
            try:
                for p in sorted(OUT.glob(f"odds_{date_q}_*.csv")):
                    candidates.append(p)
                hist_dir = OUT / "odds_history"
                if hist_dir.exists():
                    for p in sorted(hist_dir.glob(f"odds_{date_q}_*.csv")):
                        candidates.append(p)
            except Exception:
                pass
            raw_file = next((p for p in candidates if p.exists()), None)
            if raw_file.exists() and not df.empty and {"home_team","away_team"}.issubset(df.columns):
                # Only attempt for rows still missing critical odds
                miss_mask = df["market_total"].isna() if "market_total" in df.columns else pd.Series([True]*len(df))
                miss_mask |= (df["spread_home"].isna() if "spread_home" in df.columns else False)
                if miss_mask.any():
                    raw = pd.read_csv(raw_file)
                    if not raw.empty:
                        # Resolve team columns flexibly
                        home_col = next((c for c in ["home_team_name","home_team","home"] if c in raw.columns), None)
                        away_col = next((c for c in ["away_team_name","away_team","away"] if c in raw.columns), None)
                        if not (home_col and away_col):
                            raise Exception("raw odds missing team columns")
                        # Constrain by date if commence_time present
                        if "commence_time" in raw.columns:
                            try:
                                _dt = pd.to_datetime(raw["commence_time"], errors="coerce")
                                raw = raw[_dt.dt.strftime("%Y-%m-%d") == str(date_q)]
                            except Exception:
                                pass
                        raw["_home_norm"] = raw[home_col].astype(str).map(_canon_slug)
                        raw["_away_norm"] = raw[away_col].astype(str).map(_canon_slug)
                        # Pre-build list of pair variants for fuzzy search
                        raw_pairs = []  # (pair_key, index)
                        for i, r in raw.iterrows():
                            hn = r.get("_home_norm"); an = r.get("_away_norm")
                            if not hn or not an:
                                continue
                            pair_key = f"{hn}::{an}"  # keep order
                            raw_pairs.append((pair_key, i))
                        pair_keys = [rk for rk,_ in raw_pairs]
                        # Helper to extract median metrics from subset of matching raw rows
                        def _apply_fill(idx):
                            row = df.loc[idx]
                            hn = _canon_slug(str(row.get("home_team") or ""))
                            an = _canon_slug(str(row.get("away_team") or ""))
                            if not hn or not an:
                                return
                            target1 = f"{hn}::{an}"; target2 = f"{an}::{hn}"  # order unknown in raw
                            # Fuzzy match both orientations
                            best1 = process.extractOne(target1, pair_keys, scorer=fuzz.token_set_ratio)
                            best2 = process.extractOne(target2, pair_keys, scorer=fuzz.token_set_ratio)
                            best = None
                            for cand in [best1, best2]:
                                if cand and (best is None or cand[1] > best[1]):
                                    best = cand
                            if not best or best[1] < 90:  # threshold
                                return
                            # Gather all raw rows with same unordered team set
                            sel = []
                            for (pk, i_raw) in raw_pairs:
                                parts = pk.split("::")
                                if set(parts) == {hn, an}:
                                    sel.append(i_raw)
                            if not sel:
                                return
                            sub = raw.loc[sel]
                            # Compute medians
                            if ("market_total" not in df.columns) or pd.isna(df.at[idx, "market_total"]):
                                for tc in ["total","over_under","market_total","line_total"]:
                                    if tc in sub.columns and sub[tc].notna().any():
                                        df.at[idx, "market_total"] = float(pd.to_numeric(sub[tc], errors="coerce").median())
                                        break
                            if ("spread_home" not in df.columns) or pd.isna(df.at[idx, "spread_home"]):
                                for sc in ["spread","home_spread","spread_home","handicap_home"]:
                                    if sc in sub.columns and sub[sc].notna().any():
                                        df.at[idx, "spread_home"] = float(pd.to_numeric(sub[sc], errors="coerce").median())
                                        break
                            if ("ml_home" not in df.columns) or pd.isna(df.at[idx, "ml_home"]):
                                for mc in ["moneyline_home","ml_home","price_home","h2h_home"]:
                                    if mc in sub.columns and sub[mc].notna().any():
                                        df.at[idx, "ml_home"] = float(pd.to_numeric(sub[mc], errors="coerce").median())
                                        break
                        for idx in df[miss_mask].index:
                            _apply_fill(idx)
                # Half-lines fallback fill (1H/2H) using raw odds period labels when joined odds lack halves
                need_1h = ("market_total_1h" not in df.columns) or df["market_total_1h"].isna().sum() >= int(0.5*len(df))
                need_2h = ("market_total_2h" not in df.columns) or df["market_total_2h"].isna().sum() >= int(0.5*len(df))
                if (need_1h or need_2h) and raw_file.exists():
                    raw = pd.read_csv(raw_file)
                    if not raw.empty:
                        # Normalize period in raw
                        per = raw.get("period")
                        if per is not None:
                            raw["_period_norm"] = per.map(lambda v: re.sub(r"[^a-z0-9]+","_", str(v).lower()).strip("_"))
                        else:
                            raw["_period_norm"] = ""
                        # Prepare normalized team keys
                        home_col = next((c for c in ["home_team_name","home_team","home"] if c in raw.columns), None)
                        away_col = next((c for c in ["away_team_name","away_team","away"] if c in raw.columns), None)
                        if home_col and away_col:
                            raw["_home_norm"] = raw[home_col].astype(str).map(_canon_slug)
                            raw["_away_norm"] = raw[away_col].astype(str).map(_canon_slug)
                            # Helper to aggregate from a subset filtered by period and unordered team set
                            def _fill_half_for_idx(idx, period_keys: set[str], tgt_prefix: str):
                                row = df.loc[idx]
                                hn = _canon_slug(str(row.get("home_team") or ""))
                                an = _canon_slug(str(row.get("away_team") or ""))
                                if not hn or not an:
                                    return
                                sub = raw[raw["_period_norm"].isin(period_keys)] if "_period_norm" in raw.columns else raw
                                if sub.empty:
                                    return
                                # Filter to unordered pair
                                sub = sub[((sub["_home_norm"]==hn) & (sub["_away_norm"]==an)) | ((sub["_home_norm"]==an) & (sub["_away_norm"]==hn))]
                                if sub.empty:
                                    return
                                # Totals
                                if (tgt_prefix+"total") in ["market_total_1h","market_total_2h"]:
                                    if (df.get(tgt_prefix+"total") is None) or pd.isna(df.at[idx, tgt_prefix+"total"]):
                                        for tc in ["total","over_under","market_total","line_total"]:
                                            if tc in sub.columns and sub[tc].notna().any():
                                                df.at[idx, tgt_prefix+"total"] = float(pd.to_numeric(sub[tc], errors="coerce").median())
                                                break
                                # Spreads
                                if (tgt_prefix+"spread_home") in ["spread_home_1h","spread_home_2h"]:
                                    if (df.get(tgt_prefix+"spread_home") is None) or pd.isna(df.at[idx, tgt_prefix+"spread_home"]):
                                        for sc in ["home_spread","spread","handicap_home","spread_home"]:
                                            if sc in sub.columns and sub[sc].notna().any():
                                                df.at[idx, tgt_prefix+"spread_home"] = float(pd.to_numeric(sub[sc], errors="coerce").median())
                                                break
                                # Moneyline
                                if (tgt_prefix+"ml_home") in ["ml_home_1h","ml_home_2h"]:
                                    if (df.get(tgt_prefix+"ml_home") is None) or pd.isna(df.at[idx, tgt_prefix+"ml_home"]):
                                        for mc in ["moneyline_home","ml_home","price_home","h2h_home"]:
                                            if mc in sub.columns and sub[mc].notna().any():
                                                df.at[idx, tgt_prefix+"ml_home"] = float(pd.to_numeric(sub[mc], errors="coerce").median())
                                                break
                            # Apply per row
                            PERIOD_1H = {"1h","1st_half","first_half","h1","half_1","1_h","1sthalf","firsthalf"}
                            PERIOD_2H = {"2h","2nd_half","second_half","h2","half_2","2_h","2ndhalf","secondhalf"}
                            for idx in df.index:
                                if need_1h:
                                    _fill_half_for_idx(idx, PERIOD_1H, "market_total_1h"[:-6])  # prefix 'market_total_'
                                    _fill_half_for_idx(idx, PERIOD_1H, "spread_home_1h"[:-3])   # prefix 'spread_home_'
                                    _fill_half_for_idx(idx, PERIOD_1H, "ml_home_1h"[:-3])       # prefix 'ml_home_'
                                if need_2h:
                                    _fill_half_for_idx(idx, PERIOD_2H, "market_total_2h"[:-6])
                                    _fill_half_for_idx(idx, PERIOD_2H, "spread_home_2h"[:-3])
                                    _fill_half_for_idx(idx, PERIOD_2H, "ml_home_2h"[:-3])
    except Exception:
        pass

    # Hard odds coverage fallback: ensure market_total populated by game_id or synthesized
    try:
        if 'game_id' in df.columns:
            df['game_id'] = df['game_id'].astype(str)
            # Attempt direct totals by game_id from raw odds files if missing
            need_mt = ('market_total' not in df.columns) or df['market_total'].isna().any()
            if need_mt:
                candidates = []
                if date_q:
                    candidates.append(OUT / f"odds_{date_q}.csv")
                    candidates.append(OUT / "odds_history" / f"odds_{date_q}.csv")
                if today_str and (not date_q or date_q == today_str):
                    candidates.append(OUT / "odds_today.csv")
                # Pick first existing odds file and build a game_id -> total map
                raw_odds = None
                for p in candidates:
                    try:
                        if p.exists():
                            tmp = pd.read_csv(p)
                            if not tmp.empty:
                                raw_odds = tmp
                                break
                    except Exception:
                        continue
                if raw_odds is not None:
                    try:
                        ro = raw_odds.copy()
                        # Normalize columns
                        if 'game_id' in ro.columns:
                            ro['game_id'] = ro['game_id'].astype(str)
                        # Identify a total column
                        total_col = None
                        for c in ('market_total','total','closing_total','last_total','line_total'):
                            if c in ro.columns:
                                total_col = c
                                break
                        if total_col and 'game_id' in ro.columns:
                            mt_map = ro.set_index('game_id')[total_col]
                            mt_map = pd.to_numeric(mt_map, errors='coerce')
                            df['_mt_from_raw'] = df['game_id'].map(mt_map)
                            # Fill only missing
                            if 'market_total' not in df.columns:
                                df['market_total'] = df['_mt_from_raw']
                            else:
                                miss_mask = df['market_total'].isna()
                                if miss_mask.any():
                                    df.loc[miss_mask, 'market_total'] = df.loc[miss_mask, '_mt_from_raw']
                            try:
                                 df.drop(columns=['_mt_from_raw'], inplace=True)
                            except Exception:
                                pass
                    except Exception:
                        pass
            # Removed synthetic market_total synthesis: retain missing odds as missing (real data only)
            if 'market_total' in df.columns:
                mt_missing_ct = int(pd.to_numeric(df['market_total'], errors='coerce').isna().sum())
                pipeline_stats['market_total_missing_after_raw_merge'] = mt_missing_ct
            # Backfill commence_time from raw odds by game_id (earliest commence per game)
            try:
                candidates_c = []
                if date_q:
                    candidates_c.append(OUT / f"odds_{date_q}.csv")
                    candidates_c.append(OUT / "odds_history" / f"odds_{date_q}.csv")
                if today_str and (not date_q or date_q == today_str):
                    candidates_c.append(OUT / "odds_today.csv")
                raw_odds_c = None
                for p in candidates_c:
                    try:
                        if p.exists():
                            tmp = pd.read_csv(p)
                            if not tmp.empty:
                                raw_odds_c = tmp
                                break
                    except Exception:
                        continue
                if raw_odds_c is not None and 'game_id' in raw_odds_c.columns:
                    ro2 = raw_odds_c.copy()
                    ro2['game_id'] = ro2['game_id'].astype(str)
                    # Identify commence column
                    comm_col = None
                    for c in ('commence_time','start_time','commence'):
                        if c in ro2.columns:
                            comm_col = c
                            break
                    if comm_col:
                        ro2['_commence_dt'] = pd.to_datetime(ro2[comm_col], errors='coerce', utc=True)
                        # Earliest commence per game_id
                        cm_map = ro2.sort_values('_commence_dt').dropna(subset=['_commence_dt']).groupby('game_id')['_commence_dt'].first()
                        # Populate in df only where missing
                        df['_commence_from_odds'] = df['game_id'].map(cm_map)
                        # Prefer writing to 'commence_time_odds' canonical attachment
                        if 'commence_time_odds' not in df.columns:
                            df['commence_time_odds'] = df['_commence_from_odds']
                        else:
                            miss_c = pd.to_datetime(df['commence_time_odds'], errors='coerce', utc=True).isna()
                            if miss_c.any():
                                df.loc[miss_c, 'commence_time_odds'] = df.loc[miss_c, '_commence_from_odds']
                        # Also set generic '_commence' if absent
                        if '_commence' not in df.columns:
                            df['_commence'] = df['_commence_from_odds']
                        else:
                            miss_c2 = pd.to_datetime(df['_commence'], errors='coerce', utc=True).isna()
                            if miss_c2.any():
                                df.loc[miss_c2, '_commence'] = df.loc[miss_c2, '_commence_from_odds']
                        try:
                            df.drop(columns=['_commence_from_odds'], inplace=True)
                        except Exception:
                            pass
                        # Instrument count
                        _cto_ct = int(pd.to_datetime(df.get('commence_time_odds'), errors='coerce', utc=True).notna().sum()) if 'commence_time_odds' in df.columns else 0
                        _comm_ct = int(pd.to_datetime(df.get('_commence'), errors='coerce', utc=True).notna().sum()) if '_commence' in df.columns else 0
                        pipeline_stats['commence_time_odds_nonnull'] = int(max(_cto_ct, _comm_ct))
            except Exception:
                pipeline_stats['commence_time_backfill_error'] = True
    except Exception:
        pass

    # Unordered team-pair commence fallback: attach commence from raw odds using normalized team names
    try:
        if not df.empty and {'home_team','away_team'}.issubset(df.columns):
            # Build df pair key
            try:
                df['_home_norm'] = df['home_team'].astype(str).map(_canon_slug)
                df['_away_norm'] = df['away_team'].astype(str).map(_canon_slug)
                df['_pair_key'] = df.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
            except Exception:
                pass
            # Load raw odds candidates
            candidates_p = []
            if date_q:
                candidates_p.append(OUT / f"odds_{date_q}.csv")
                candidates_p.append(OUT / 'odds_history' / f"odds_{date_q}.csv")
            if today_str and (not date_q or date_q == today_str):
                candidates_p.append(OUT / 'odds_today.csv')
            raw_p = None
            for p in candidates_p:
                try:
                    if p.exists():
                        tmp = pd.read_csv(p)
                        if not tmp.empty:
                            raw_p = tmp
                            break
                except Exception:
                    continue
            if raw_p is not None and {'home_team','away_team'}.issubset(raw_p.columns):
                ro = raw_p.copy()
                ro['_home_norm'] = ro['home_team'].astype(str).map(_canon_slug)
                ro['_away_norm'] = ro['away_team'].astype(str).map(_canon_slug)
                try:
                    ro['_pair_key'] = ro.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
                except Exception:
                    ro['_pair_key'] = None
                # Identify commence-like column
                comm_col = None
                for c in ('commence_time','start_time','commence'):
                    if c in ro.columns:
                        comm_col = c
                        break
                if comm_col:
                    ro['_commence_dt'] = pd.to_datetime(ro[comm_col], errors='coerce', utc=True)
                    pair_cm = ro.sort_values('_commence_dt').dropna(subset=['_commence_dt']).groupby('_pair_key')['_commence_dt'].first()
                    # Map onto df where missing
                    df['_commence_pair'] = df.get('_pair_key').map(pair_cm) if '_pair_key' in df.columns else pd.NaT
                    # Fill commence_time_odds and _commence only when missing
                    try:
                        miss_cto = pd.to_datetime(df.get('commence_time_odds'), errors='coerce', utc=True).isna() if 'commence_time_odds' in df.columns else pd.Series([True]*len(df))
                    except Exception:
                        miss_cto = pd.Series([True]*len(df))
                    if miss_cto.any():
                        if 'commence_time_odds' not in df.columns:
                            df['commence_time_odds'] = df['_commence_pair']
                        else:
                            df.loc[miss_cto, 'commence_time_odds'] = df.loc[miss_cto, '_commence_pair']
                    try:
                        miss_comm = pd.to_datetime(df.get('_commence'), errors='coerce', utc=True).isna() if '_commence' in df.columns else pd.Series([True]*len(df))
                    except Exception:
                        # Correct mask construction: ensure proper pandas Series usage
                        miss_comm = pd.Series([True] * len(df))
                    if miss_comm.any():
                        if '_commence' not in df.columns:
                            df['_commence'] = df['_commence_pair']
                        else:
                            df.loc[miss_comm, '_commence'] = df.loc[miss_comm, '_commence_pair']
                    # Instrument
                    _cto_ct = int(pd.to_datetime(df.get('commence_time_odds'), errors='coerce', utc=True).notna().sum()) if 'commence_time_odds' in df.columns else 0
                    _comm_ct = int(pd.to_datetime(df.get('_commence'), errors='coerce', utc=True).notna().sum()) if '_commence' in df.columns else 0
                    pipeline_stats['commence_time_odds_nonnull'] = int(max(_cto_ct, _comm_ct))
                    try:
                        df.drop(columns=['_commence_pair'], inplace=True)
                    except Exception:
                        pass
            # If still zero commence attached, use schedule games_curr as proxy via unordered team pair
            try:
                if int(pipeline_stats.get('commence_time_odds_nonnull', 0)) == 0:
                    gm = _safe_read_csv(OUT / 'games_curr.csv')
                    if isinstance(gm, pd.DataFrame) and not gm.empty and {'home_team','away_team'}.issubset(gm.columns):
                        ga = gm.copy()
                        ga['_home_norm'] = ga['home_team'].astype(str).map(_canon_slug)
                        ga['_away_norm'] = ga['away_team'].astype(str).map(_canon_slug)
                        try:
                            ga['_pair_key'] = ga.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
                        except Exception:
                            ga['_pair_key'] = None
                        # Prefer schedule commence_time; fallback to start_time
                        sch_comm = None
                        if 'commence_time' in ga.columns:
                            sch_comm = pd.to_datetime(ga['commence_time'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                        if (sch_comm is None or (isinstance(sch_comm, pd.Series) and sch_comm.isna().all())) and 'start_time' in ga.columns:
                            st_raw = ga['start_time'].astype(str).str.replace('Z','+00:00', regex=False)
                            sch_comm = pd.to_datetime(st_raw, errors='coerce', utc=True)
                        ga['_commence_dt'] = sch_comm
                        pair_comm_map = ga.sort_values('_commence_dt').dropna(subset=['_commence_dt']).groupby('_pair_key')['_commence_dt'].first()
                        # Map onto df
                        df['_commence_pair_sched'] = df.get('_pair_key').map(pair_comm_map) if '_pair_key' in df.columns else pd.NaT
                        # Fill only missing
                        try:
                            miss_cto2 = pd.to_datetime(df.get('commence_time_odds'), errors='coerce', utc=True).isna() if 'commence_time_odds' in df.columns else pd.Series([True]*len(df))
                        except Exception:
                            miss_cto2 = pd.Series([True]*len(df))
                        if miss_cto2.any():
                            if 'commence_time_odds' not in df.columns:
                                df['commence_time_odds'] = df['_commence_pair_sched']
                            else:
                                df.loc[miss_cto2, 'commence_time_odds'] = df.loc[miss_cto2, '_commence_pair_sched']
                        try:
                            miss_comm2 = pd.to_datetime(df.get('_commence'), errors='coerce', utc=True).isna() if '_commence' in df.columns else pd.Series([True]*len(df))
                        except Exception:
                            miss_comm2 = pd.Series([True]*len(df))
                        if miss_comm2.any():
                            if '_commence' not in df.columns:
                                df['_commence'] = df['_commence_pair_sched']
                            else:
                                df.loc[miss_comm2, '_commence'] = df.loc[miss_comm2, '_commence_pair_sched']
                        _cto_ct = int(pd.to_datetime(df.get('commence_time_odds'), errors='coerce', utc=True).notna().sum()) if 'commence_time_odds' in df.columns else 0
                        _comm_ct = int(pd.to_datetime(df.get('_commence'), errors='coerce', utc=True).notna().sum()) if '_commence' in df.columns else 0
                        pipeline_stats['commence_time_odds_nonnull'] = int(max(_cto_ct, _comm_ct))
                        try:
                            df.drop(columns=['_commence_pair_sched'], inplace=True)
                        except Exception:
                            pass
            except Exception:
                pipeline_stats['commence_pair_schedule_proxy_error'] = True
    except Exception:
        pipeline_stats['commence_pair_backfill_error'] = True

    # Synthetic odds suggestions (non-invasive): propose candidate lines without altering real odds columns.
    try:
        if not df.empty and 'game_id' in df.columns:
            # Ensure base columns exist; do NOT fill missing with synthetic values
            if 'market_total' not in df.columns:
                df['market_total'] = np.nan
            if 'spread_home' not in df.columns:
                df['spread_home'] = np.nan
            mt_missing_mask = df['market_total'].isna()
            sh_missing_mask = df['spread_home'].isna()
            # Candidate total suggestion
            if mt_missing_mask.any():
                src_series = None
                for cand in ['pred_total_model_unified','pred_total_calibrated','pred_total_model','pred_total','derived_total']:
                    if cand in df.columns:
                        ser = pd.to_numeric(df[cand], errors='coerce')
                        if ser.notna().any():
                            src_series = ser
                            break
                if src_series is not None:
                    def _round_half_safe(x):
                        try:
                            return round(float(x) * 2.0) / 2.0
                        except Exception:
                            return np.nan
                    sugg_vals = src_series.map(_round_half_safe).clip(lower=110, upper=188)
                    df['market_total_synthetic'] = sugg_vals
                    pipeline_stats['synthetic_market_total_candidates'] = int(sugg_vals.notna().sum())
            # Candidate spread suggestion
            if sh_missing_mask.any():
                margin_src = None
                for cand in ['pred_margin_calibrated','pred_margin_model','pred_margin']:
                    if cand in df.columns:
                        ms = pd.to_numeric(df[cand], errors='coerce')
                        if ms.notna().any():
                            margin_src = ms
                            break
                if margin_src is not None:
                    def _round_spread_safe(x):
                        try:
                            return - (round(float(x) * 2.0) / 2.0)
                        except Exception:
                            return np.nan
                    sugg_spreads = margin_src.map(_round_spread_safe).clip(lower=-40, upper=40)
                    df['spread_home_synthetic'] = sugg_spreads
                    pipeline_stats['synthetic_spread_home_candidates'] = int(sugg_spreads.notna().sum())
            pipeline_stats['residual_market_total_missing'] = int(df['market_total'].isna().sum())
            pipeline_stats['residual_spread_home_missing'] = int(df['spread_home'].isna().sum())
            if df['market_total'].isna().any() or df['spread_home'].isna().any():
                pipeline_stats['odds_residual_incomplete'] = True
            else:
                pipeline_stats['odds_full_coverage'] = True
            # Record missing prediction coverage (real model predictions only) for escalation diagnostics
            if 'pred_total' in df.columns:
                miss_pred = df[df['pred_total'].isna()]
                pipeline_stats['missing_pred_rows'] = int(len(miss_pred))
                if 'game_id' in miss_pred.columns:
                    pipeline_stats['missing_pred_game_ids'] = miss_pred['game_id'].astype(str).tolist()
            if 'pred_margin' in df.columns:
                miss_margin = df[df['pred_margin'].isna()]
                pipeline_stats['missing_margin_rows'] = int(len(miss_margin))
            # Real data coverage escalation flag: any missing odds or predictions after all real merges
            if pipeline_stats.get('missing_pred_rows',0) > 0 or pipeline_stats.get('residual_market_total_missing',0) > 0 or pipeline_stats.get('residual_spread_home_missing',0) > 0:
                pipeline_stats['coverage_escalation_real_missing'] = True
    except Exception:
        pipeline_stats['synthetic_odds_error'] = True

    # Prefer calibrated predictions over blended/base when available; then blended as secondary.
    # Ensures UI basis shows 'CAL' instead of 'BLEN' when both artifacts exist.
    try:
        if not df.empty:
            # Enforcement flag: calibrated-only display. When True we never substitute blended predictions
            # into pred_total/pred_margin. We will surface raw model values if calibration missing.
            enforce_cal_only = True
            used_blend = False
            used_cal = False
            # Calibrated preference (total) – override only non-NaN calibrated rows; preserve originals for diagnostics.
            if "pred_total_calibrated" in df.columns:
                cal_series = pd.to_numeric(df["pred_total_calibrated"], errors="coerce")
                if cal_series.notna().any():
                    # Auto-prefer calibrated when available unless user explicitly forced otherwise
                    if not prefer_cal_eff:
                        prefer_cal_eff = True
                        try:
                            pipeline_stats["prefer_cal_auto"] = True
                            pipeline_stats["prefer_cal_effective"] = True
                        except Exception:
                            pass
                    if "pred_total_orig" not in df.columns:
                        df["pred_total_orig"] = df.get("pred_total")
                    if "pred_total_basis" in df.columns and "pred_total_basis_orig" not in df.columns:
                        df["pred_total_basis_orig"] = df["pred_total_basis"].astype(str)
                    # Include additional blend variants so calibrated values supersede any blended/model mixes
                    lower_precedence = {"blend","blended","blended_model_baseline","blend_model_market","synthetic_baseline_final","synthetic","model_raw","baseline","none"}
                    current_basis = df.get("pred_total_basis").astype(str) if "pred_total_basis" in df.columns else pd.Series(["none"]*len(df))
                    # Treat absence ('none') or empty string as override candidate
                    override_candidates = current_basis.isin(lower_precedence) | current_basis.isna() | (current_basis.str.strip() == "")
                    # Current total predictions for diff gating
                    pt_curr = pd.to_numeric(df.get("pred_total"), errors="coerce") if "pred_total" in df.columns else pd.Series([np.nan]*len(df))
                    # When prefer_cal query flag set, allow override even for tiny diffs (use 0 threshold)
                    _thr_t = 0.0 if prefer_cal_eff else 0.01
                    diff_mask_total = pt_curr.isna() | (cal_series.notna() & pt_curr.notna() & (cal_series.sub(pt_curr).abs() > _thr_t))
                    override_mask_cal = cal_series.notna() & override_candidates & diff_mask_total
                    if override_mask_cal.any():
                        df.loc[override_mask_cal, "pred_total"] = cal_series[override_mask_cal]
                        if "pred_total_basis" in df.columns:
                            df.loc[override_mask_cal, "pred_total_basis"] = "cal"
                        else:
                            df["pred_total_basis"] = ["cal" if m else None for m in override_mask_cal]
                        pipeline_stats["calibration_precedence_overrides_total"] = int(override_mask_cal.sum())
                        pipeline_stats["calibration_precedence_total_candidates"] = int(cal_series.notna().sum())
                        used_cal = True
                        # Recompute closing edge if applicable
                        try:
                            if "closing_total" in df.columns:
                                ct_series = pd.to_numeric(df["closing_total"], errors="coerce")
                                pt_series2 = pd.to_numeric(df["pred_total"], errors="coerce")
                                df.loc[override_mask_cal, "edge_closing"] = pt_series2[override_mask_cal] - ct_series[override_mask_cal]
                                pipeline_stats["calibration_recomputed_edge_closing_total"] = int(override_mask_cal.sum())
                        except Exception:
                            pipeline_stats["calibration_recompute_edge_closing_total_error"] = True
            # Calibrated preference via unified model (when pred_total_calibrated absent but unified source is calibrated)
            if not used_cal and "pred_total_model_unified" in df.columns and "pred_total_basis" in df.columns:
                try:
                    src = pipeline_stats.get("pred_total_model_unified_stats", {}).get("source") if 'pipeline_stats' in locals() else None
                except Exception:
                    src = None
                if src == "calibrated":
                    unified_series = pd.to_numeric(df["pred_total_model_unified"], errors="coerce")
                    if unified_series.notna().any():
                        current_basis = df["pred_total_basis"].astype(str)
                        lower_precedence = {"blend","blended","blended_model_baseline","synthetic_baseline_final","synthetic","model_raw","baseline"}
                        override_mask_unified = unified_series.notna() & current_basis.isin(lower_precedence)
                        if override_mask_unified.any():
                            if "pred_total_orig" not in df.columns:
                                df["pred_total_orig"] = df.get("pred_total")
                            if "pred_total_basis_orig" not in df.columns:
                                df["pred_total_basis_orig"] = current_basis
                            df.loc[override_mask_unified, "pred_total"] = unified_series[override_mask_unified]
                            df.loc[override_mask_unified, "pred_total_basis"] = "cal"
                            pipeline_stats["calibration_precedence_overrides_total_unified"] = int(override_mask_unified.sum())
                            used_cal = True
            # Calibrated preference (margin) – similar selective override
            if "pred_margin_calibrated" in df.columns:
                margin_cal_series = pd.to_numeric(df["pred_margin_calibrated"], errors="coerce")
                if margin_cal_series.notna().any():
                    if not prefer_cal_eff:
                        prefer_cal_eff = True
                        try:
                            pipeline_stats["prefer_cal_auto"] = True
                            pipeline_stats["prefer_cal_effective"] = True
                        except Exception:
                            pass
                    if "pred_margin_orig" not in df.columns:
                        df["pred_margin_orig"] = df.get("pred_margin")
                    if "pred_margin_basis" in df.columns and "pred_margin_basis_orig" not in df.columns:
                        df["pred_margin_basis_orig"] = df["pred_margin_basis"].astype(str)
                    current_mb = df.get("pred_margin_basis").astype(str) if "pred_margin_basis" in df.columns else pd.Series(["none"]*len(df))
                    lower_precedence_m = {"blend","blended","blend_model_market","synthetic_from_spread_final","synthetic_even_final","model_raw","baseline","none"}
                    override_candidates_m = current_mb.isin(lower_precedence_m) | current_mb.isna() | (current_mb.str.strip() == "")
                    pm_curr = pd.to_numeric(df.get("pred_margin"), errors="coerce") if "pred_margin" in df.columns else pd.Series([np.nan]*len(df))
                    _thr_m = 0.0 if prefer_cal_eff else 0.01
                    diff_mask_margin = pm_curr.isna() | (margin_cal_series.notna() & pm_curr.notna() & (margin_cal_series.sub(pm_curr).abs() > _thr_m))
                    override_mask_margin_cal = margin_cal_series.notna() & override_candidates_m & diff_mask_margin
                    if override_mask_margin_cal.any():
                        df.loc[override_mask_margin_cal, "pred_margin"] = margin_cal_series[override_mask_margin_cal]
                        if "pred_margin_basis" in df.columns:
                            df.loc[override_mask_margin_cal, "pred_margin_basis"] = "cal"
                        else:
                            df["pred_margin_basis"] = ["cal" if m else None for m in override_mask_margin_cal]
                        pipeline_stats["calibration_precedence_overrides_margin"] = int(override_mask_margin_cal.sum())
                        pipeline_stats["calibration_precedence_margin_candidates"] = int(margin_cal_series.notna().sum())
                        used_cal = True
                        # Recompute closing ATS edge if closing spread present
                        try:
                            if "closing_spread_home" in df.columns:
                                cs_series = pd.to_numeric(df["closing_spread_home"], errors="coerce")
                                pm_series2 = pd.to_numeric(df["pred_margin"], errors="coerce")
                                # Closing ATS edge: predicted home spread minus closing spread
                                df.loc[override_mask_margin_cal, "edge_closing_ats"] = (-pm_series2[override_mask_margin_cal]) - cs_series[override_mask_margin_cal]
                                pipeline_stats["calibration_recomputed_edge_closing_margin"] = int(override_mask_margin_cal.sum())
                        except Exception:
                            pipeline_stats["calibration_recompute_edge_closing_margin_error"] = True
            # Calibrated missing: attach raw model predictions with explicit missing calibration basis
            if not used_cal:
                # Totals
                if "pred_total_model" in df.columns and "pred_total" in df.columns:
                    ptm = pd.to_numeric(df["pred_total_model"], errors="coerce")
                    pt_curr = pd.to_numeric(df["pred_total"], errors="coerce")
                    # Replace only where displayed pred_total is NaN or synthetic/blend basis and model exists
                    basis_series = df.get("pred_total_basis").astype(str) if "pred_total_basis" in df.columns else pd.Series(["none"]*len(df))
                    synthetic_like = basis_series.str.startswith("synthetic") | basis_series.isin(["blend","blended","blended_low","baseline","none","cal_est","derived_full","derived_remainder","market_copy"]) | basis_series.eq("")
                    replace_mask = ptm.notna() & (pt_curr.isna() | synthetic_like)
                    if replace_mask.any():
                        df.loc[replace_mask, "pred_total"] = ptm[replace_mask]
                        if "pred_total_basis" in df.columns:
                            df.loc[replace_mask, "pred_total_basis"] = "model_raw_missing_cal"
                        else:
                            df["pred_total_basis"] = ["model_raw_missing_cal" if m else None for m in replace_mask]
                        pipeline_stats["calibration_missing_total_rows_model_promoted"] = int(replace_mask.sum())
                # Margins
                if "pred_margin_model" in df.columns and "pred_margin" in df.columns:
                    pmm = pd.to_numeric(df["pred_margin_model"], errors="coerce")
                    pm_curr = pd.to_numeric(df["pred_margin"], errors="coerce")
                    basis_m_series = df.get("pred_margin_basis").astype(str) if "pred_margin_basis" in df.columns else pd.Series(["none"]*len(df))
                    synthetic_like_m = basis_m_series.str.startswith("synthetic") | basis_m_series.isin(["blend","blended","baseline","none","cal_est","synthetic_from_spread_final","synthetic_even_final","derived_full","derived_remainder","market_copy"]) | basis_m_series.eq("")
                    replace_mask_m = pmm.notna() & (pm_curr.isna() | synthetic_like_m)
                    if replace_mask_m.any():
                        df.loc[replace_mask_m, "pred_margin"] = pmm[replace_mask_m]
                        if "pred_margin_basis" in df.columns:
                            df.loc[replace_mask_m, "pred_margin_basis"] = "model_raw_missing_cal"
                        else:
                            df["pred_margin_basis"] = ["model_raw_missing_cal" if m else None for m in replace_mask_m]
                        pipeline_stats["calibration_missing_margin_rows_model_promoted"] = int(replace_mask_m.sum())
                # Instrument counts of missing calibration artifacts
                if "pred_total_calibrated" in df.columns:
                    pipeline_stats["calibration_total_rows"] = int(pd.to_numeric(df["pred_total_calibrated"], errors="coerce").notna().sum())
                if "pred_margin_calibrated" in df.columns:
                    pipeline_stats["calibration_margin_rows"] = int(pd.to_numeric(df["pred_margin_calibrated"], errors="coerce").notna().sum())
                # Missing model rows for games present
                if "game_id" in df.columns:
                    try:
                        if "pred_total_model" in df.columns:
                            pipeline_stats["missing_model_total_rows"] = int(pd.to_numeric(df["pred_total_model"], errors="coerce").isna().sum())
                        if "pred_margin_model" in df.columns:
                            pipeline_stats["missing_model_margin_rows"] = int(pd.to_numeric(df["pred_margin_model"], errors="coerce").isna().sum())
                    except Exception:
                        pipeline_stats["missing_model_rows_instrument_error"] = True
                pipeline_stats["enforce_calibrated_only"] = True
                pipeline_stats["blend_fallback_disabled"] = True
            if used_blend:
                # Determine if blend is meaningful (non-zero weight AND segmentation rows > 0)
                try:
                    # Preserve original segmentation sample size before any badge-hiding clears
                    if "seg_n_rows" in df.columns and "seg_n_rows_orig" not in df.columns:
                        try:
                            df["seg_n_rows_orig"] = df["seg_n_rows"]
                        except Exception:
                            pass
                    # Initialize blend effectiveness flag (row-level) – will mark True only for retained blended rows
                    if "blend_effective" not in df.columns:
                        df["blend_effective"] = False
                    bw_series = pd.to_numeric(df.get("blend_weight"), errors="coerce") if "blend_weight" in df.columns else None
                    seg_series = pd.to_numeric(df.get("seg_n_rows"), errors="coerce") if "seg_n_rows" in df.columns else None
                    # Mark zero / missing segmentation counts as NaN for UI suppression
                    if seg_series is not None:
                        seg_bad_mask = seg_series.fillna(0) <= 0
                        if seg_bad_mask.any():
                            df.loc[seg_bad_mask, "seg_n_rows"] = np.nan
                    # Identify rows where blend should be suppressed (weight <=0 OR seg_n_rows <=0)
                    if bw_series is not None or seg_series is not None:
                        suppress_mask = None
                        if bw_series is not None:
                            suppress_mask = (bw_series.fillna(0) <= 0)
                        if seg_series is not None:
                            seg_mask = (seg_series.fillna(0) <= 0)
                            suppress_mask = seg_mask if suppress_mask is None else (suppress_mask | seg_mask)
                        # New rule: if original basis was calibrated, prefer calibrated over blend regardless of weights
                        try:
                            if "pred_total_basis_orig" in df.columns:
                                cal_mask = df["pred_total_basis_orig"].astype(str).str.startswith("model_calibrated")
                                suppress_mask = cal_mask if suppress_mask is None else (suppress_mask | cal_mask)
                                if cal_mask.any():
                                    pipeline_stats["blend_rows_calibrated_preferred"] = int(cal_mask.sum())
                        except Exception:
                            pass
                        if suppress_mask is not None and suppress_mask.any():
                            suppressed_count = int(suppress_mask.sum())
                            try:
                                pipeline_stats["blend_rows_suppressed"] = suppressed_count
                            except Exception:
                                pass
                            # Revert predictions & basis for suppressed rows to original (pre-blend)
                            if "pred_total_orig" in df.columns:
                                df.loc[suppress_mask, "pred_total"] = df.loc[suppress_mask, "pred_total_orig"]
                            if "pred_margin_orig" in df.columns:
                                df.loc[suppress_mask, "pred_margin"] = df.loc[suppress_mask, "pred_margin_orig"]
                            if "pred_total_basis_orig" in df.columns:
                                df.loc[suppress_mask, "pred_total_basis"] = df.loc[suppress_mask, "pred_total_basis_orig"]
                            if "pred_margin_basis_orig" in df.columns:
                                df.loc[suppress_mask, "pred_margin_basis"] = df.loc[suppress_mask, "pred_margin_basis_orig"]
                            # If original basis missing, set a fallback model/raw tag instead of 'blend'
                            if "pred_total_basis" in df.columns:
                                df.loc[suppress_mask & ~df["pred_total_basis"].notna(), "pred_total_basis"] = "model_raw"
                            if "pred_margin_basis" in df.columns:
                                df.loc[suppress_mask & ~df["pred_margin_basis"].notna(), "pred_margin_basis"] = "model_raw"
                            # For rows where blend suppressed, clear blend_weight to hide badge downstream
                            if "blend_weight" in df.columns:
                                df.loc[suppress_mask, "blend_weight"] = np.nan
                            # Ensure effectiveness flag remains False for suppressed rows
                            if "blend_effective" in df.columns:
                                df.loc[suppress_mask, "blend_effective"] = False
                    # After potential suppression, decide if any rows still genuinely blended
                    remaining_blend_mask = None
                    if "blend_weight" in df.columns and "seg_n_rows" in df.columns:
                        bw_series2 = pd.to_numeric(df["blend_weight"], errors="coerce")
                        seg_series2 = pd.to_numeric(df["seg_n_rows"], errors="coerce")
                        remaining_blend_mask = (bw_series2.fillna(0) > 0) & (seg_series2.fillna(0) > 0)
                    # Row-level effectiveness check: blended value must differ meaningfully from original to keep.
                    if remaining_blend_mask is not None and remaining_blend_mask.any():
                        try:
                            pt_blend = pd.to_numeric(df.get("pred_total"), errors="coerce")
                            pt_orig = pd.to_numeric(df.get("pred_total_orig"), errors="coerce") if "pred_total_orig" in df.columns else None
                            pm_blend = pd.to_numeric(df.get("pred_margin"), errors="coerce")
                            pm_orig = pd.to_numeric(df.get("pred_margin_orig"), errors="coerce") if "pred_margin_orig" in df.columns else None
                            ineffective_mask = pd.Series([False]*len(df))
                            # Thresholds chosen to avoid retaining cosmetically identical blends (tiny or zero delta)
                            if pt_orig is not None:
                                ineffective_mask = ineffective_mask | (remaining_blend_mask & (pt_blend.notna() & pt_orig.notna() & (pt_blend.sub(pt_orig).abs() <= 0.05)))
                            if pm_orig is not None:
                                ineffective_mask = ineffective_mask | (remaining_blend_mask & (pm_blend.notna() & pm_orig.notna() & (pm_blend.sub(pm_orig).abs() <= 0.05)))
                            if ineffective_mask.any():
                                # Revert ineffective rows to originals; clear badges/weights so UI shows authentic source.
                                if "pred_total_orig" in df.columns:
                                    df.loc[ineffective_mask, "pred_total"] = df.loc[ineffective_mask, "pred_total_orig"]
                                if "pred_margin_orig" in df.columns:
                                    df.loc[ineffective_mask, "pred_margin"] = df.loc[ineffective_mask, "pred_margin_orig"]
                                if "pred_total_basis_orig" in df.columns:
                                    df.loc[ineffective_mask, "pred_total_basis"] = df.loc[ineffective_mask, "pred_total_basis_orig"]
                                if "pred_margin_basis_orig" in df.columns:
                                    df.loc[ineffective_mask, "pred_margin_basis"] = df.loc[ineffective_mask, "pred_margin_basis_orig"]
                                # Clear blend weight / segmentation counts so template hides W/N badges for reverted rows.
                                if "blend_weight" in df.columns:
                                    df.loc[ineffective_mask, "blend_weight"] = np.nan
                                if "seg_n_rows" in df.columns:
                                    df.loc[ineffective_mask, "seg_n_rows"] = np.nan
                                if "blend_effective" in df.columns:
                                    df.loc[ineffective_mask, "blend_effective"] = False
                                try:
                                    pipeline_stats["blend_rows_ineffective"] = int(ineffective_mask.sum())
                                except Exception:
                                    pass
                                # Update remaining_blend_mask to exclude reverted ineffective rows.
                                remaining_blend_mask = remaining_blend_mask & (~ineffective_mask)
                        except Exception:
                            # On any error in effectiveness logic, fall back to leaving remaining_blend_mask as-is.
                            pipeline_stats["blend_effectiveness_error"] = True
                    # Detect uniform/near-uniform collapse among remaining blended rows (e.g., constant 112) and revert.
                    try:
                        if remaining_blend_mask is not None and remaining_blend_mask.any():
                            pt_remain = pd.to_numeric(df.loc[remaining_blend_mask, "pred_total"], errors="coerce")
                            # Consider only finite values
                            pt_valid = pt_remain.replace([np.inf, -np.inf], np.nan).dropna()
                            if not pt_valid.empty:
                                uniq = pt_valid.nunique()
                                rng = float(pt_valid.max() - pt_valid.min()) if pt_valid.notna().any() else 0.0
                                collapse_val = float(pt_valid.iloc[0]) if uniq == 1 else None
                                looks_uniform = False
                                # Heuristics: single unique OR tiny range with <=2 uniques OR classic collapse value ~112
                                if uniq == 1:
                                    looks_uniform = True
                                elif uniq <= 2 and rng < 0.5:
                                    looks_uniform = True
                                elif collapse_val is not None and abs(collapse_val - 112.0) < 0.01:
                                    looks_uniform = True
                                if looks_uniform:
                                    # Revert all remaining blend rows to originals; clear blend indicators.
                                    if "pred_total_orig" in df.columns:
                                        df.loc[remaining_blend_mask, "pred_total"] = df.loc[remaining_blend_mask, "pred_total_orig"]
                                    if "pred_margin_orig" in df.columns:
                                        df.loc[remaining_blend_mask, "pred_margin"] = df.loc[remaining_blend_mask, "pred_margin_orig"]
                                    if "pred_total_basis_orig" in df.columns:
                                        df.loc[remaining_blend_mask, "pred_total_basis"] = df.loc[remaining_blend_mask, "pred_total_basis_orig"]
                                    if "pred_margin_basis_orig" in df.columns:
                                        df.loc[remaining_blend_mask, "pred_margin_basis"] = df.loc[remaining_blend_mask, "pred_margin_basis_orig"]
                                    if "blend_weight" in df.columns:
                                        df.loc[remaining_blend_mask, "blend_weight"] = np.nan
                                    if "seg_n_rows" in df.columns:
                                        df.loc[remaining_blend_mask, "seg_n_rows"] = np.nan
                                    if "blend_effective" in df.columns:
                                        df.loc[remaining_blend_mask, "blend_effective"] = False
                                    pipeline_stats["blend_uniform_reverted"] = int(remaining_blend_mask.sum())
                                    if collapse_val is not None:
                                        pipeline_stats["blend_uniform_value"] = collapse_val
                                    # No remaining effective blended rows after full revert
                                    remaining_blend_mask = remaining_blend_mask & pd.Series([False]*len(df))
                    except Exception:
                        pipeline_stats["blend_uniform_check_error"] = True
                    # After ineffectiveness/uniform pruning, re-evaluate global blended usage.
                    if remaining_blend_mask is not None and remaining_blend_mask.any():
                        pipeline_stats["using_blended_predictions"] = True
                        try:
                            pipeline_stats["blend_rows_effective"] = int(remaining_blend_mask.sum())
                            if "blend_effective" in df.columns:
                                df.loc[remaining_blend_mask, "blend_effective"] = True
                        except Exception:
                            pass
                    else:
                        # No effective blend rows left – treat as unblended (do not set using_blended_predictions)
                        used_blend = False
                except Exception:
                    # On any error, fall back to original at least hiding zero badges
                    try:
                        if "seg_n_rows" in df.columns:
                            df.loc[pd.to_numeric(df["seg_n_rows"], errors="coerce").fillna(0) <= 0, "seg_n_rows"] = np.nan
                        if "blend_weight" in df.columns:
                            df.loc[pd.to_numeric(df["blend_weight"], errors="coerce").fillna(0) <= 0, "blend_weight"] = np.nan
                        if "blend_effective" in df.columns:
                            # Effectiveness unknown -> keep False
                            pass
                    except Exception:
                        pass
                # Blend diagnostics when available (only if still considered blended)
                if used_blend:
                    # Primary diagnostics (weights & sample sizes)
                    try:
                        if "blend_weight" in df.columns:
                            bw = pd.to_numeric(df["blend_weight"], errors="coerce")
                            bw_valid = bw.dropna()
                            if not bw_valid.empty:
                                pipeline_stats["blend_weight_min"] = float(bw_valid.min())
                                pipeline_stats["blend_weight_median"] = float(bw_valid.median())
                                pipeline_stats["blend_weight_max"] = float(bw_valid.max())
                                pipeline_stats["blend_weight_mean"] = float(bw_valid.mean())
                        # Use original segmentation counts for diagnostics (retain visibility even if badges suppressed)
                        if "seg_n_rows_orig" in df.columns:
                            sn_orig = pd.to_numeric(df["seg_n_rows_orig"], errors="coerce").dropna()
                            # Consider only positive counts
                            sn_orig = sn_orig[sn_orig > 0]
                            if not sn_orig.empty:
                                pipeline_stats["seg_n_rows_median"] = float(sn_orig.median())
                                pipeline_stats["seg_n_rows_min"] = float(sn_orig.min())
                                pipeline_stats["seg_n_rows_max"] = float(sn_orig.max())
                    except Exception:
                        pass
                    # Residual performance comparison baseline vs segmented vs blended (totals & margins)
                    try:
                        # Totals residual metrics vs closing_total
                        if {"closing_total","pred_total_base","pred_total_seg","pred_total"}.issubset(df.columns):
                            ct = pd.to_numeric(df["closing_total"], errors="coerce")
                            base = pd.to_numeric(df["pred_total_base"], errors="coerce")
                            seg = pd.to_numeric(df["pred_total_seg"], errors="coerce")
                            blend = pd.to_numeric(df["pred_total"], errors="coerce")
                            def _mae(a,b):
                                try:
                                    m = (a - b).abs().dropna()
                                    return float(m.mean()) if not m.empty else None
                                except Exception:
                                    return None
                            def _rmse(a,b):
                                try:
                                    d = (a - b).dropna()
                                    return float(np.sqrt((d**2).mean())) if not d.empty else None
                                except Exception:
                                    return None
                            mae_base = _mae(base, ct); mae_seg = _mae(seg, ct); mae_blend = _mae(blend, ct)
                            rmse_base = _rmse(base, ct); rmse_seg = _rmse(seg, ct); rmse_blend = _rmse(blend, ct)
                            if mae_base is not None: pipeline_stats["total_mae_base"] = mae_base
                            if mae_seg is not None: pipeline_stats["total_mae_seg"] = mae_seg
                            if mae_blend is not None: pipeline_stats["total_mae_blend"] = mae_blend
                            if rmse_base is not None: pipeline_stats["total_rmse_base"] = rmse_base
                            if rmse_seg is not None: pipeline_stats["total_rmse_seg"] = rmse_seg
                            if rmse_blend is not None: pipeline_stats["total_rmse_blend"] = rmse_blend
                            if mae_base and mae_blend: pipeline_stats["total_mae_blend_improve_vs_base_pct"] = float((mae_base - mae_blend)/mae_base*100.0)
                            if mae_seg and mae_blend: pipeline_stats["total_mae_blend_improve_vs_seg_pct"] = float((mae_seg - mae_blend)/mae_seg*100.0)
                        # Margin residual metrics vs closing_spread_home
                        if {"closing_spread_home","pred_margin_base","pred_margin_seg","pred_margin"}.issubset(df.columns):
                            cs = pd.to_numeric(df["closing_spread_home"], errors="coerce")
                            mb = pd.to_numeric(df["pred_margin_base"], errors="coerce")
                            ms = pd.to_numeric(df["pred_margin_seg"], errors="coerce")
                            mm = pd.to_numeric(df["pred_margin"], errors="coerce")
                            def _mae2(a,b):
                                try:
                                    m = (a - b).abs().dropna()
                                    return float(m.mean()) if not m.empty else None
                                except Exception:
                                    return None
                            def _rmse2(a,b):
                                try:
                                    d = (a - b).dropna()
                                    return float(np.sqrt((d**2).mean())) if not d.empty else None
                                except Exception:
                                    return None
                            mae_mb = _mae2(mb, cs); mae_ms = _mae2(ms, cs); mae_mm = _mae2(mm, cs)
                            rmse_mb = _rmse2(mb, cs); rmse_ms = _rmse2(ms, cs); rmse_mm = _rmse2(mm, cs)
                            if mae_mb is not None: pipeline_stats["margin_mae_base"] = mae_mb
                            if mae_ms is not None: pipeline_stats["margin_mae_seg"] = mae_ms
                            if mae_mm is not None: pipeline_stats["margin_mae_blend"] = mae_mm
                            if rmse_mb is not None: pipeline_stats["margin_rmse_base"] = rmse_mb
                            if rmse_ms is not None: pipeline_stats["margin_rmse_seg"] = rmse_ms
                            if rmse_mm is not None: pipeline_stats["margin_rmse_blend"] = rmse_mm
                            if mae_mb and mae_mm: pipeline_stats["margin_mae_blend_improve_vs_base_pct"] = float((mae_mb - mae_mm)/mae_mb*100.0)
                            if mae_ms and mae_mm: pipeline_stats["margin_mae_blend_improve_vs_seg_pct"] = float((mae_ms - mae_mm)/mae_ms*100.0)
                    except Exception:
                        pipeline_stats["blend_perf_error"] = True
                    # Prediction confidence scoring using team variance + segment effective rows (totals & margins)
                    try:
                        seg_rows_series = None
                        if "seg_eff_n_rows" in df.columns:
                            seg_rows_series = pd.to_numeric(df["seg_eff_n_rows"], errors="coerce")
                        elif "seg_n_rows" in df.columns:
                            seg_rows_series = pd.to_numeric(df["seg_n_rows"], errors="coerce")
                        # Totals confidence
                        if team_variance_total and {"home_team","away_team","pred_total"}.issubset(df.columns):
                            vh = df["home_team"].map(lambda t: team_variance_total.get(str(t), np.nan))
                            va = df["away_team"].map(lambda t: team_variance_total.get(str(t), np.nan))
                            avg_var = (vh + va) / 2.0
                            if seg_rows_series is not None:
                                seg_factor = seg_rows_series / (seg_rows_series + 50.0)
                            else:
                                seg_factor = 0.3
                            conf = (1.0 / (1.0 + avg_var)) * (0.5 + 0.5 * seg_factor)
                            try:
                                conf = conf.clip(lower=0.0, upper=1.0)
                            except Exception:
                                pass
                            df["pred_total_confidence"] = conf
                            c_valid = pd.to_numeric(conf, errors="coerce").replace([np.inf,-np.inf], np.nan).dropna()
                            if not c_valid.empty:
                                pipeline_stats["pred_total_conf_q25"] = float(c_valid.quantile(0.25))
                                pipeline_stats["pred_total_conf_median"] = float(c_valid.median())
                                pipeline_stats["pred_total_conf_q75"] = float(c_valid.quantile(0.75))
                                pipeline_stats["pred_total_conf_mean"] = float(c_valid.mean())
                        # Margin confidence
                        if team_variance_margin and {"home_team","away_team","pred_margin"}.issubset(df.columns):
                            vh_m = df["home_team"].map(lambda t: team_variance_margin.get(str(t), np.nan))
                            va_m = df["away_team"].map(lambda t: team_variance_margin.get(str(t), np.nan))
                            avg_var_m = (vh_m + va_m) / 2.0
                            if seg_rows_series is not None:
                                seg_factor_m = seg_rows_series / (seg_rows_series + 50.0)
                            else:
                                seg_factor_m = 0.3
                            conf_m = (1.0 / (1.0 + avg_var_m)) * (0.5 + 0.5 * seg_factor_m)
                            try:
                                conf_m = conf_m.clip(lower=0.0, upper=1.0)
                            except Exception:
                                pass
                            df["pred_margin_confidence"] = conf_m
                            m_valid = pd.to_numeric(conf_m, errors="coerce").replace([np.inf,-np.inf], np.nan).dropna()
                            if not m_valid.empty:
                                pipeline_stats["pred_margin_conf_q25"] = float(m_valid.quantile(0.25))
                                pipeline_stats["pred_margin_conf_median"] = float(m_valid.median())
                                pipeline_stats["pred_margin_conf_q75"] = float(m_valid.quantile(0.75))
                                pipeline_stats["pred_margin_conf_mean"] = float(m_valid.mean())
                    except Exception:
                        pipeline_stats["prediction_confidence_error"] = True
    except Exception:
        pass

    # Meta probability inference (guarded): load artifacts and compute p_home_cover_meta / p_over_meta
    try:
        if not df.empty:
            from pathlib import Path as _Path
            import joblib as _joblib  # type: ignore
            _outp = _Path(OUT)
            # Prefer pre-enriched meta columns; if present, skip model inference to avoid shape errors
            pre_enriched_cover = ('p_home_cover_meta' in df.columns) and df['p_home_cover_meta'].notna().any()
            pre_enriched_over = ('p_over_meta' in df.columns) and df['p_over_meta'].notna().any()
            if pre_enriched_cover or pre_enriched_over:
                pipeline_stats['meta_pre_enriched_used'] = True
            cover_model_path = _outp / 'meta_cover_lgbm.joblib'
            over_model_path = _outp / 'meta_over_lgbm.joblib'
            cover_model = _joblib.load(cover_model_path) if cover_model_path.exists() else None
            over_model = _joblib.load(over_model_path) if over_model_path.exists() else None

            # Determine expected feature schema/order from model or metrics artifact when available
            expected_cover_feats = None
            expected_over_feats = None
            # Try reading feature names directly from the LightGBM models
            try:
                def _lgbm_feature_names(model):
                    # sklearn API: model.booster_.feature_name() or model.feature_name_
                    try:
                        if hasattr(model, 'booster_') and hasattr(model.booster_, 'feature_name'):
                            return list(model.booster_.feature_name())
                    except Exception:
                        pass
                    try:
                        if hasattr(model, 'feature_name_') and isinstance(model.feature_name_, (list, tuple)):
                            return list(model.feature_name_)
                    except Exception:
                        pass
                    return None
                expected_cover_feats = _lgbm_feature_names(cover_model) if cover_model is not None else None
                expected_over_feats = _lgbm_feature_names(over_model) if over_model is not None else None
            except Exception:
                pipeline_stats['meta_features_schema_model_error'] = True
            # Fallback to metrics artifact if model feature names unavailable
            if (not expected_cover_feats) or (not isinstance(expected_cover_feats, list)):
                try:
                    meta_metrics_path = _outp / 'meta_probs_metrics.json'
                    if meta_metrics_path.exists():
                        meta_payload = _json.loads(meta_metrics_path.read_text(encoding='utf-8'))
                        if isinstance(meta_payload, dict):
                            expected_cover_feats = meta_payload.get('cover_features') or meta_payload.get('features_cover') or expected_cover_feats
                            expected_over_feats = meta_payload.get('over_features') or meta_payload.get('features_over') or expected_over_feats
                except Exception:
                    pipeline_stats['meta_features_schema_load_error'] = True
            # Sidecar schema: prefer explicit ordered lists when present (robust direct JSON load)
            try:
                cover_sidecar_path = _outp / 'meta_features_cover.json'
                # Prefer new over sidecar name when present; fallback to legacy 'total'
                over_sidecar_path_new = _outp / 'meta_features_over.json'
                over_sidecar_path_legacy = _outp / 'meta_features_total.json'
                sidecar_cover = None
                sidecar_over = None
                if cover_sidecar_path.exists():
                    _tmp = _json.loads(cover_sidecar_path.read_text(encoding='utf-8'))
                    sidecar_cover = _tmp if isinstance(_tmp, list) else _tmp.get('features') if isinstance(_tmp, dict) else None
                _ov_path = over_sidecar_path_new if over_sidecar_path_new.exists() else over_sidecar_path_legacy
                if _ov_path.exists():
                    _tmp2 = _json.loads(_ov_path.read_text(encoding='utf-8'))
                    sidecar_over = _tmp2 if isinstance(_tmp2, list) else _tmp2.get('features') if isinstance(_tmp2, dict) else None
                if isinstance(sidecar_cover, list) and sidecar_cover:
                    expected_cover_feats = sidecar_cover
                if isinstance(sidecar_over, list) and sidecar_over:
                    expected_over_feats = sidecar_over
                if (isinstance(sidecar_cover, list) and sidecar_cover) or (isinstance(sidecar_over, list) and sidecar_over):
                    pipeline_stats['meta_sidecar_loaded'] = True
            except Exception:
                pipeline_stats['meta_sidecar_error'] = True
            if expected_cover_feats and isinstance(expected_cover_feats, list):
                pipeline_stats['meta_expected_cover_features'] = int(len(expected_cover_feats))
            if expected_over_feats and isinstance(expected_over_feats, list):
                pipeline_stats['meta_expected_over_features'] = int(len(expected_over_feats))

            # Diagnostics: record which expected features are missing from df
            try:
                df_cols_set = set(map(str, df.columns))
                miss_cover = [c for c in (expected_cover_feats or []) if c not in df_cols_set]
                miss_over = [c for c in (expected_over_feats or []) if c not in df_cols_set]
                pipeline_stats['meta_missing_cover_features_count'] = int(len(miss_cover))
                pipeline_stats['meta_missing_over_features_count'] = int(len(miss_over))
                # Store a short sample to avoid bloating logs
                pipeline_stats['meta_missing_cover_features_sample'] = miss_cover[:8]
                pipeline_stats['meta_missing_over_features_sample'] = miss_over[:8]
                # Persist a lightweight alignment diagnostic for quick inspection
                try:
                    from datetime import datetime as _dt
                    _iso = _dt.utcnow().strftime('%Y-%m-%d')
                    _diag = {
                        'date': _iso,
                        'expected_cover_len': int(len(expected_cover_feats)) if isinstance(expected_cover_feats, list) else None,
                        'expected_over_len': int(len(expected_over_feats)) if isinstance(expected_over_feats, list) else None,
                        'runtime_cols_len': int(len(df.columns)),
                        'missing_cover': miss_cover,
                        'missing_over': miss_over,
                    }
                    (_outp / f'meta_alignment_diag_{_iso}.json').write_text(_json.dumps(_diag, ensure_ascii=False, indent=2), encoding='utf-8')
                    pipeline_stats['meta_alignment_diag_written'] = True
                except Exception:
                    pipeline_stats['meta_alignment_diag_error'] = True
            except Exception:
                pipeline_stats['meta_feature_missing_diag_error'] = True

            # Build feature frames honoring expected order when provided; else fall back to minimal set
            if expected_cover_feats and isinstance(expected_cover_feats, list) and len(expected_cover_feats) > 0:
                feat_cols_cover = [c for c in expected_cover_feats if c in df.columns]
                # Fill any missing expected columns with NaN to maintain shape
                Xc = pd.DataFrame({c: pd.to_numeric(df.get(c), errors='coerce') if c in df.columns else pd.Series(np.nan, index=df.index) for c in expected_cover_feats})
            else:
                feat_cols_cover = [c for c in [
                    'pred_margin','spread_home','ml_home','ml_away','market_total','closing_total',
                    'pred_margin_calibrated','closing_spread_home','edge_total'
                ] if c in df.columns]
                Xc = pd.DataFrame({c: pd.to_numeric(df[c], errors='coerce') for c in feat_cols_cover}) if feat_cols_cover else pd.DataFrame()

            if expected_over_feats and isinstance(expected_over_feats, list) and len(expected_over_feats) > 0:
                feat_cols_over = [c for c in expected_over_feats if c in df.columns]
                Xo = pd.DataFrame({c: pd.to_numeric(df.get(c), errors='coerce') if c in df.columns else pd.Series(np.nan, index=df.index) for c in expected_over_feats})
            else:
                feat_cols_over = [c for c in [
                    'pred_total','market_total','closing_total','edge_total','pred_total_calibrated'
                ] if c in df.columns]
                Xo = pd.DataFrame({c: pd.to_numeric(df[c], errors='coerce') for c in feat_cols_over}) if feat_cols_over else pd.DataFrame()

            # Helper: align a feature frame to an expected ordered list, filling missing
            def _align_features(X: pd.DataFrame, expected_feats: list[str] | None, index_like: pd.Index) -> pd.DataFrame:
                try:
                    if not isinstance(expected_feats, list) or not expected_feats:
                        return X
                    cols = {}
                    for c in expected_feats:
                        if c in X.columns:
                            cols[c] = pd.to_numeric(X[c], errors='coerce')
                        else:
                            cols[c] = pd.Series(np.nan, index=index_like)
                    X2 = pd.DataFrame(cols)
                    # sanitize NaNs by mean-imputing per column to avoid model errors
                    X2 = X2.replace([np.inf, -np.inf], np.nan)
                    for c in X2.columns:
                        col = X2[c]
                        if col.isna().any():
                            X2[c] = col.fillna(col.mean())
                    return X2
                except Exception:
                    return X

            # Predict probabilities when models and features exist
            if cover_model is not None and not Xc.empty and not pre_enriched_cover:
                try:
                    # Ensure feature count matches training expectation strictly from model to avoid LightGBM fatals
                    try:
                        expected_n_cov = getattr(cover_model, 'n_features_in_', None)
                        # Load persisted schema if present
                        try:
                            cov_schema_path = _outp / 'meta_cover_lgbm_features.json'
                            if cov_schema_path.exists():
                                _cov_schema = json.loads(cov_schema_path.read_text(encoding='utf-8')).get('features') or []
                                if _cov_schema:
                                    # Add any missing columns with neutral fill (0.0) then reorder
                                    for _c in _cov_schema:
                                        if _c not in Xc.columns:
                                            Xc[_c] = 0.0
                                    Xc = Xc[_cov_schema]
                        except Exception:
                            pass
                        # If model doesn't expose a reliable feature count, skip prediction to avoid LightGBM shape fatals
                        if (not isinstance(expected_n_cov, int)) or (expected_n_cov <= 0):
                            pipeline_stats['meta_cover_feature_mismatch'] = {'expected': 'unknown', 'got': int(Xc.shape[1])}
                            # Attempt alignment using expected feature names if available, then abort to fallback
                            Xc = _align_features(Xc, expected_cover_feats if isinstance(expected_cover_feats, list) else None, df.index)
                            raise RuntimeError('meta_cover_feature_mismatch')
                        if Xc.shape[1] != expected_n_cov:
                            # Try to rebuild using expected feature names to match exact count
                            Xc = _align_features(Xc, expected_cover_feats if isinstance(expected_cover_feats, list) else None, df.index)
                            # Second attempt: if persisted schema exists but size mismatch, pad/truncate
                            if 'meta_cover_feature_mismatch' in pipeline_stats and isinstance(expected_n_cov, int):
                                try:
                                    cov_schema_path2 = _outp / 'meta_cover_lgbm_features.json'
                                    if cov_schema_path2.exists():
                                        _cov_schema2 = json.loads(cov_schema_path2.read_text(encoding='utf-8')).get('features') or []
                                        if _cov_schema2 and len(_cov_schema2) == expected_n_cov:
                                            for _c in _cov_schema2:
                                                if _c not in Xc.columns:
                                                    Xc[_c] = 0.0
                                            Xc = Xc[_cov_schema2]
                                except Exception:
                                    pass
                            if Xc.shape[1] != expected_n_cov:
                                pipeline_stats['meta_cover_feature_mismatch'] = {'expected': int(expected_n_cov), 'got': int(Xc.shape[1])}
                                # Skip prediction entirely on mismatch; fallback handled below
                                raise RuntimeError('meta_cover_feature_mismatch')
                    except Exception:
                        pass
                    pc = cover_model.predict_proba(Xc) if hasattr(cover_model, 'predict_proba') else cover_model.predict(Xc)
                    pc = pc[:,1] if isinstance(pc, np.ndarray) and pc.ndim==2 and pc.shape[1]>1 else pc
                    df['p_home_cover_meta'] = pd.Series(pc)
                    pipeline_stats['p_home_cover_meta_rows'] = int(df['p_home_cover_meta'].notna().sum())
                except Exception:
                    # Fallback to distribution-based probability when meta model fails or mismatches
                    if 'p_home_cover_dist' in df.columns:
                        df['p_home_cover_meta'] = df['p_home_cover_dist']
                        pipeline_stats['p_home_cover_meta_error'] = False
                        pipeline_stats['p_home_cover_meta_fallback_dist'] = True
                    else:
                        pipeline_stats['p_home_cover_meta_error'] = True
            if over_model is not None and not Xo.empty and not pre_enriched_over:
                try:
                    # Ensure feature count matches training expectation strictly from model to avoid LightGBM fatals
                    try:
                        expected_n_ov = getattr(over_model, 'n_features_in_', None)
                        try:
                            ov_schema_path = _outp / 'meta_over_lgbm_features.json'
                            if ov_schema_path.exists():
                                _ov_schema = json.loads(ov_schema_path.read_text(encoding='utf-8')).get('features') or []
                                if _ov_schema:
                                    for _c in _ov_schema:
                                        if _c not in Xo.columns:
                                            Xo[_c] = 0.0
                                    Xo = Xo[_ov_schema]
                        except Exception:
                            pass
                        if (not isinstance(expected_n_ov, int)) or (expected_n_ov <= 0):
                            pipeline_stats['meta_over_feature_mismatch'] = {'expected': 'unknown', 'got': int(Xo.shape[1])}
                            # Attempt alignment using expected feature names if available, then abort to fallback
                            Xo = _align_features(Xo, expected_over_feats if isinstance(expected_over_feats, list) else None, df.index)
                            raise RuntimeError('meta_over_feature_mismatch')
                        if Xo.shape[1] != expected_n_ov:
                            # Try to rebuild using expected feature names to match exact count
                            Xo = _align_features(Xo, expected_over_feats if isinstance(expected_over_feats, list) else None, df.index)
                            if 'meta_over_feature_mismatch' in pipeline_stats and isinstance(expected_n_ov, int):
                                try:
                                    ov_schema_path2 = _outp / 'meta_over_lgbm_features.json'
                                    if ov_schema_path2.exists():
                                        _ov_schema2 = json.loads(ov_schema_path2.read_text(encoding='utf-8')).get('features') or []
                                        if _ov_schema2 and len(_ov_schema2) == expected_n_ov:
                                            for _c in _ov_schema2:
                                                if _c not in Xo.columns:
                                                    Xo[_c] = 0.0
                                            Xo = Xo[_ov_schema2]
                                except Exception:
                                    pass
                            if Xo.shape[1] != expected_n_ov:
                                pipeline_stats['meta_over_feature_mismatch'] = {'expected': int(expected_n_ov), 'got': int(Xo.shape[1])}
                                # Skip prediction entirely on mismatch; fallback handled below
                                raise RuntimeError('meta_over_feature_mismatch')
                    except Exception:
                        pass
                    po = over_model.predict_proba(Xo) if hasattr(over_model, 'predict_proba') else over_model.predict(Xo)
                    po = po[:,1] if isinstance(po, np.ndarray) and po.ndim==2 and po.shape[1]>1 else po
                    df['p_over_meta'] = pd.Series(po)
                    pipeline_stats['p_over_meta_rows'] = int(df['p_over_meta'].notna().sum())
                except Exception:
                    # Fallback to distribution-based probability when meta model fails or mismatches
                    if 'p_over_dist' in df.columns:
                        df['p_over_meta'] = df['p_over_dist']
                        pipeline_stats['p_over_meta_error'] = False
                        pipeline_stats['p_over_meta_fallback_dist'] = True
                    else:
                        pipeline_stats['p_over_meta_error'] = True

            # Apply isotonic-style meta calibration if available
            try:
                cal_path = _outp / 'meta_calibration.json'
                if cal_path.exists():
                    payload = _json.loads(cal_path.read_text(encoding='utf-8'))
                    def _apply_map(series: pd.Series, key: str) -> pd.Series:
                        try:
                            m = payload.get(key, {}) if isinstance(payload, dict) else {}
                            xs = np.asarray(m.get('x', []), dtype=float)
                            ys = np.asarray(m.get('y', []), dtype=float)
                            if xs.size >= 2 and ys.size == xs.size and series is not None and series.notna().any():
                                s = pd.to_numeric(series, errors='coerce').clip(lower=0.0, upper=1.0)
                                return pd.Series(np.interp(s.fillna(0.5), xs, ys), index=series.index)
                        except Exception:
                            return pd.Series([np.nan]*len(series))
                        return pd.Series([np.nan]*len(series))
                    if 'p_home_cover_meta' in df.columns and df['p_home_cover_meta'].notna().any():
                        df['p_home_cover_meta_cal'] = _apply_map(df['p_home_cover_meta'], 'p_home_cover')
                        pipeline_stats['p_home_cover_meta_cal_rows'] = int(df['p_home_cover_meta_cal'].notna().sum())
                    if 'p_over_meta' in df.columns and df['p_over_meta'].notna().any():
                        df['p_over_meta_cal'] = _apply_map(df['p_over_meta'], 'p_over')
                        pipeline_stats['p_over_meta_cal_rows'] = int(df['p_over_meta_cal'].notna().sum())
                    pipeline_stats['meta_calibration_applied'] = True
                else:
                    pipeline_stats['meta_calibration_missing'] = True
            except Exception:
                pipeline_stats['meta_calibration_error'] = True

            # Portable logistic fallback: compute meta probs from portable JSON artifacts if needed
            try:
                def _portable_predict(prob_kind: str, expected_feats: list[str] | None) -> pd.Series | None:
                    try:
                        pj = (_outp / f'meta_{prob_kind}_portable.json')
                        if not pj.exists():
                            return None
                        import json as _json2
                        payload = _json2.loads(pj.read_text(encoding='utf-8'))
                        coef = payload.get('coef')
                        intercept = payload.get('intercept')
                        feat_names = payload.get('feature_names') or expected_feats or []
                        if not isinstance(coef, list) or not feat_names:
                            return None
                        # Coef may be nested [[...]]
                        coef_arr = np.asarray(coef, dtype=float)
                        if coef_arr.ndim == 2:
                            coef_arr = coef_arr[0]
                        w = coef_arr
                        b = float(np.asarray(intercept, dtype=float)[0]) if intercept is not None else 0.0
                        # Build X with alignment and sanitation
                        cols = [c for c in feat_names if c in df.columns]
                        if not cols:
                            return None
                        Xp = pd.DataFrame({c: pd.to_numeric(df.get(c), errors='coerce') for c in cols})
                        Xp = Xp.replace([np.inf, -np.inf], np.nan)
                        for c in Xp.columns:
                            col = Xp[c]
                            if col.isna().any():
                                Xp[c] = col.fillna(col.mean())
                        # If fewer cols than weights, trim weights; if more, trim cols to len(w)
                        if Xp.shape[1] != len(w):
                            if Xp.shape[1] > len(w):
                                Xp = Xp.iloc[:, :len(w)]
                            else:
                                w = w[:Xp.shape[1]]
                        z = Xp.values @ w + b
                        p = 1.0 / (1.0 + np.exp(-z))
                        return pd.Series(p, index=df.index)
                    except Exception:
                        return None

                # Apply if meta probs missing or empty (cover)
                if ('p_home_cover_meta' not in df.columns) or (df['p_home_cover_meta'].isna().all()):
                    pc_series = _portable_predict('cover', expected_cover_feats if isinstance(expected_cover_feats, list) else None)
                    if pc_series is not None:
                        df['p_home_cover_meta'] = pc_series
                        pipeline_stats['p_home_cover_meta_portable_used'] = True
                        pipeline_stats['p_home_cover_meta_rows'] = int(df['p_home_cover_meta'].notna().sum())
                if ('p_over_meta' not in df.columns) or (df['p_over_meta'].isna().all()):
                    po_series = _portable_predict('over', expected_over_feats if isinstance(expected_over_feats, list) else None)
                    if po_series is not None:
                        df['p_over_meta'] = po_series
                        pipeline_stats['p_over_meta_portable_used'] = True
                        pipeline_stats['p_over_meta_rows'] = int(df['p_over_meta'].notna().sum())
            except Exception:
                pipeline_stats['meta_portable_infer_error'] = True

            # Integrate into display precedence when available
            try:
                # Prefer calibrated meta when present
                if 'p_home_cover_meta_cal' in df.columns and df['p_home_cover_meta_cal'].notna().any():
                    df['p_cover_display'] = df['p_home_cover_meta_cal']
                    pipeline_stats['p_cover_display_basis'] = 'meta_cal'
                elif 'p_home_cover_meta' in df.columns and df['p_home_cover_meta'].notna().any():
                    df['p_cover_display'] = df['p_home_cover_meta']
                    pipeline_stats['p_cover_display_basis'] = 'meta'
                if 'p_over_meta' in df.columns and df['p_over_meta'].notna().any():
                    if 'p_over_meta_cal' in df.columns and df['p_over_meta_cal'].notna().any():
                        df['p_over_display'] = df['p_over_meta_cal']
                        pipeline_stats['p_over_display_basis'] = 'meta_cal'
                    else:
                        df['p_over_display'] = df['p_over_meta']
                        pipeline_stats['p_over_display_basis'] = 'meta'
            except Exception:
                pipeline_stats['meta_selection_precedence_error'] = True
    except Exception:
        pipeline_stats['meta_inference_error'] = True

    # Adaptive sigma by team: derive per-row sigma adjustments for totals and margins
    try:
        if not df.empty:
            base_sigma_total = pd.to_numeric(df.get('sigma_total'), errors='coerce') if 'sigma_total' in df.columns else pd.Series(np.nan, index=df.index)
            base_sigma_margin = pd.to_numeric(df.get('sigma_margin'), errors='coerce') if 'sigma_margin' in df.columns else pd.Series(np.nan, index=df.index)
            if base_sigma_total.isna().all():
                base_sigma_total = pd.Series(12.0, index=df.index)
            if base_sigma_margin.isna().all():
                base_sigma_margin = pd.Series(7.0, index=df.index)
            if team_variance_total and {'home_team','away_team'}.issubset(df.columns):
                vh = df['home_team'].map(lambda t: team_variance_total.get(str(t), np.nan))
                va = df['away_team'].map(lambda t: team_variance_total.get(str(t), np.nan))
                avg_var = (pd.to_numeric(vh, errors='coerce') + pd.to_numeric(va, errors='coerce')) / 2.0
                alpha_t = 0.25
                adj_factor_t = 1.0 + alpha_t * avg_var.fillna(0.0)
                df['sigma_total_adj'] = (base_sigma_total.astype(float) * adj_factor_t.astype(float)).clip(lower=6.0, upper=24.0)
            else:
                df['sigma_total_adj'] = base_sigma_total
            if team_variance_margin and {'home_team','away_team'}.issubset(df.columns):
                vh_m = df['home_team'].map(lambda t: team_variance_margin.get(str(t), np.nan))
                va_m = df['away_team'].map(lambda t: team_variance_margin.get(str(t), np.nan))
                avg_var_m = (pd.to_numeric(vh_m, errors='coerce') + pd.to_numeric(va_m, errors='coerce')) / 2.0
                alpha_m = 0.25
                adj_factor_m = 1.0 + alpha_m * avg_var_m.fillna(0.0)
                df['sigma_margin_adj'] = (base_sigma_margin.astype(float) * adj_factor_m.astype(float)).clip(lower=3.0, upper=15.0)
            else:
                df['sigma_margin_adj'] = base_sigma_margin
            pipeline_stats['sigma_total_adj_populated'] = int(df['sigma_total_adj'].notna().sum())
            pipeline_stats['sigma_margin_adj_populated'] = int(df['sigma_margin_adj'].notna().sum())
    except Exception:
        pipeline_stats['adaptive_sigma_error'] = True

    # Meta feature columns for UI/badges and meta inputs
    try:
        if not df.empty:
            if team_variance_total and {'home_team','away_team'}.issubset(df.columns):
                vh = df['home_team'].map(lambda t: team_variance_total.get(str(t), np.nan))
                va = df['away_team'].map(lambda t: team_variance_total.get(str(t), np.nan))
                avg_var = (pd.to_numeric(vh, errors='coerce') + pd.to_numeric(va, errors='coerce')) / 2.0
                df['volatility_tag'] = np.where(avg_var >= 1.0, 'high', np.where(avg_var >= 0.5, 'med', 'low'))
            et = pd.to_numeric(df.get('edge_total'), errors='coerce') if 'edge_total' in df.columns else pd.Series(np.nan, index=df.index)
            df['edge_conf_tag'] = np.where(et.abs() >= 6.0, 'strong', np.where(et.abs() >= 3.0, 'moderate', 'light'))
            pipeline_stats['meta_feature_tags_populated'] = True
    except Exception:
        pipeline_stats['meta_feature_tags_error'] = True

    # Probability stability artifact generation (basic): write when missing for today
    try:
        if not df.empty:
            from datetime import datetime as _dt
            _today_iso = _dt.utcnow().strftime('%Y-%m-%d')
            stability_path = OUT / f'prob_stability_{_today_iso}.json'
            if not stability_path.exists():
                cols = []
                for c in ['p_home_cover_meta','p_home_cover_dist','p_over_meta','p_over_dist']:
                    if c in df.columns:
                        cols.append(c)
                stats = {}
                for c in cols:
                    s = pd.to_numeric(df[c], errors='coerce')
                    stats[c] = {
                        'count': int(s.notna().sum()),
                        'mean': float(s.mean()) if s.notna().any() else None,
                        'std': float(s.std()) if s.notna().sum() > 1 else None,
                        'min': float(s.min()) if s.notna().any() else None,
                        'max': float(s.max()) if s.notna().any() else None,
                    }
                payload = {
                    'date': _today_iso,
                    'probability_columns': cols,
                    'metrics': stats,
                }
                (OUT / 'prob_stability_last.json').write_text(_json.dumps(payload, ensure_ascii=False, indent=2), encoding='utf-8')
                stability_path.write_text(_json.dumps(payload, ensure_ascii=False, indent=2), encoding='utf-8')
                pipeline_stats['probability_stability_missing'] = False
                pipeline_stats['probability_stability_generated'] = True
    except Exception:
        pipeline_stats['probability_stability_generate_error'] = True

    # Canonical start_dt resolution (strict): prefer odds commence_time in UTC; else fallback to schedule start_time localized then converted to UTC.
    try:
        if not df.empty:
            commence_cols = [c for c in ("commence_time","commence_time_g","_commence","commence_time_odds") if c in df.columns]
            commence_series_utc = None
            for c in commence_cols:
                s = pd.to_datetime(df[c], errors="coerce", utc=True)
                if s.notna().any():
                    commence_series_utc = s
                    break
            start_series = pd.to_datetime(df["start_time"], errors="coerce") if "start_time" in df.columns else None
            start_series_utc = None
            try:
                import pytz  # type: ignore
                sched_tz_name = pipeline_stats.get("schedule_tz_used", "America/New_York")
                sched_tz = pytz.timezone(sched_tz_name)
            except Exception:
                sched_tz = None
            if start_series is not None:
                if getattr(start_series.dt, "tz", None) is None:
                    if sched_tz is not None:
                        try:
                            start_series_local = start_series.dt.tz_localize(sched_tz, nonexistent="shift_forward", ambiguous="NaT")
                            start_series_utc = start_series_local.dt.tz_convert("UTC")
                        except Exception:
                            start_series_utc = pd.to_datetime(df["start_time"], errors="coerce", utc=True)
                    else:
                        start_series_utc = pd.to_datetime(df["start_time"], errors="coerce", utc=True)
                else:
                    try:
                        start_series_utc = start_series.dt.tz_convert("UTC")
                    except Exception:
                        start_series_utc = start_series
            start_dt = None
            if commence_series_utc is not None and commence_series_utc.notna().any():
                start_dt = commence_series_utc
                pipeline_stats["start_dt_aligned_commence_authority_rows"] = int(commence_series_utc.notna().sum())
            elif start_series_utc is not None and start_series_utc.notna().any():
                start_dt = start_series_utc
                pipeline_stats["start_dt_fallback_schedule_rows"] = int(start_series_utc.notna().sum())
            if start_dt is not None:
                df["_start_dt"] = start_dt
                try:
                    df["start_time_iso"] = df["_start_dt"].dt.tz_convert("UTC").dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                except Exception:
                    df["start_time_iso"] = pd.to_datetime(df["_start_dt"], errors="coerce", utc=True).dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                try:
                    tz_name = _get_display_tz_name() if "_get_display_tz_name" in globals() else pipeline_stats.get("display_tz_used", "America/Chicago")
                    import pytz  # type: ignore
                    disp_tz = pytz.timezone(tz_name)
                    disp_series = df["_start_dt"].dt.tz_convert(disp_tz)
                    df["start_time_display"] = disp_series.dt.strftime("%Y-%m-%d %I:%M %p")
                    df["start_tz_abbr"] = disp_series.dt.strftime("%Z")
                    pipeline_stats["display_tz_used"] = tz_name
                except Exception:
                    pipeline_stats["display_tz_error"] = True
            try:
                if start_series_utc is not None and start_dt is not None:
                    diff_minutes = (start_dt - start_series_utc).dt.total_seconds() / 60.0
                    valid_mask = diff_minutes.replace([np.inf,-np.inf], np.nan).notna()
                    pipeline_stats["time_mismatch_count"] = int(valid_mask.sum())
                    if valid_mask.any():
                        ex = pd.DataFrame({
                            "game_id": df.get("game_id"),
                            "home_team": df.get("home_team"),
                            "away_team": df.get("away_team"),
                            "start_utc": start_series_utc.dt.strftime("%Y-%m-%dT%H:%M:%SZ"),
                            "canon_utc": start_dt.dt.strftime("%Y-%m-%dT%H:%M:%SZ"),
                            "diff_minutes": diff_minutes
                        }).dropna(subset=["diff_minutes"]).head(10)
                        pipeline_stats["time_mismatch_examples"] = ex.to_dict(orient="records")
            except Exception:
                pipeline_stats["time_mismatch_diag_error"] = True
    except Exception:
        pipeline_stats["start_dt_resolution_error"] = True

    # Compute simple edges when market total present. Label basis if source_last.
    if "market_total" in df.columns and "pred_total" in df.columns:
        df["edge_total"] = df["pred_total"] - df["market_total"]
        if source_last:
            df["market_basis"] = "last"  # strict last pre-tip odds
        else:
            df["market_basis"] = df.get("market_basis", "market")
        # Z-edge for totals when sigma available
        try:
            if "pred_total_sigma_bootstrap" in df.columns:
                sig = pd.to_numeric(df["pred_total_sigma_bootstrap"], errors="coerce")
                et = pd.to_numeric(df["edge_total"], errors="coerce")
                z = et / sig.replace(0, np.nan)
                df["edge_total_z"] = z
                zv = z.replace([np.inf,-np.inf], np.nan).dropna()
                if not zv.empty:
                    pipeline_stats["edge_total_z_q25"] = float(zv.quantile(0.25))
                    pipeline_stats["edge_total_z_median"] = float(zv.median())
                    pipeline_stats["edge_total_z_q75"] = float(zv.quantile(0.75))
        except Exception:
            pipeline_stats["edge_total_z_error"] = True
    # Compute closing edge separately when explicit closing_total present
    if {"closing_total","pred_total"}.issubset(df.columns):
        try:
            df["edge_closing"] = df["pred_total"] - df["closing_total"]
        except Exception:
            df["edge_closing"] = None
    # Empirical quantile-based probabilities (ATS/OU) using residual quantiles
    try:
        z90 = 1.2815515655446004
        qt10 = pipeline_stats.get('quantile_total_q10')
        qt90 = pipeline_stats.get('quantile_total_q90')
        qm10 = pipeline_stats.get('quantile_margin_q10')
        qm90 = pipeline_stats.get('quantile_margin_q90')
        sigma_t = None
        sigma_m = None
        if isinstance(qt10, (int,float)) and isinstance(qt90, (int,float)):
            try:
                span_t = float(qt90) - float(qt10)
                if abs(span_t) > 1e-6:
                    sigma_t = span_t / (2.0 * z90)
            except Exception:
                pass
        if isinstance(qm10, (int,float)) and isinstance(qm90, (int,float)):
            try:
                span_m = float(qm90) - float(qm10)
                if abs(span_m) > 1e-6:
                    sigma_m = span_m / (2.0 * z90)
            except Exception:
                pass
        if sigma_t is not None:
            pipeline_stats['sigma_total_emp'] = float(sigma_t)
        if sigma_m is not None:
            pipeline_stats['sigma_margin_emp'] = float(sigma_m)
        # Compute OU probability using empirical sigma
        if not df.empty and sigma_t and {"pred_total","market_total"}.issubset(df.columns):
            try:
                pt = pd.to_numeric(df['pred_total'], errors='coerce')
                mt = pd.to_numeric(df['market_total'], errors='coerce')
                z_t = (mt - pt) / float(sigma_t)
                from math import erf, sqrt
                def _phi(v):
                    try:
                        return 0.5 * (1.0 + erf(float(v) / sqrt(2.0)))
                    except Exception:
                        return np.nan
                df['p_over_emp'] = 1.0 - z_t.map(_phi)
                pipeline_stats['p_over_emp_rows'] = int(pd.to_numeric(df['p_over_emp'], errors='coerce').notna().sum())
            except Exception:
                pipeline_stats['p_over_emp_error'] = True
        # Compute ATS probability (home cover) using empirical sigma
        if not df.empty and sigma_m and {"pred_margin","closing_spread_home"}.issubset(df.columns):
            try:
                pm = pd.to_numeric(df['pred_margin'], errors='coerce')
                cs = pd.to_numeric(df['closing_spread_home'], errors='coerce')
                # Home cover when predicted home spread (-pred_margin) > closing_spread_home
                z_m = (cs - (-pm)) / float(sigma_m)
                from math import erf, sqrt
                def _phi2(v):
                    try:
                        return 0.5 * (1.0 + erf(float(v) / sqrt(2.0)))
                    except Exception:
                        return np.nan
                df['p_home_cover_emp'] = 1.0 - z_m.map(_phi2)
                pipeline_stats['p_home_cover_emp_rows'] = int(pd.to_numeric(df['p_home_cover_emp'], errors='coerce').notna().sum())
            except Exception:
                pipeline_stats['p_home_cover_emp_error'] = True
    except Exception:
        pipeline_stats['empirical_quantile_probs_error'] = True
    # Prefer empirical probabilities for display when available
    try:
        if not df.empty:
            if 'p_over_emp' in df.columns:
                df['p_over'] = df.get('p_over_emp')
                pipeline_stats['prob_over_basis'] = 'empirical'
            elif 'p_over_dist' in df.columns:
                df['p_over'] = df.get('p_over_dist')
                pipeline_stats['prob_over_basis'] = 'distribution'
            elif 'p_over_meta' in df.columns:
                df['p_over'] = df.get('p_over_meta')
                pipeline_stats['prob_over_basis'] = 'meta'
            if 'p_home_cover_emp' in df.columns:
                df['p_home_cover'] = df.get('p_home_cover_emp')
                pipeline_stats['prob_cover_basis'] = 'empirical'
            elif 'p_home_cover_dist' in df.columns:
                df['p_home_cover'] = df.get('p_home_cover_dist')
                pipeline_stats['prob_cover_basis'] = 'distribution'
            elif 'p_home_cover_meta' in df.columns:
                df['p_home_cover'] = df.get('p_home_cover_meta')
                pipeline_stats['prob_cover_basis'] = 'meta'
    except Exception:
        pipeline_stats['prob_display_coalesce_error'] = True
    # Distribution-based ATS & OU probabilities (normal approximation)
    try:
        if not df.empty:
            # Means
            margin_mean = pd.to_numeric(df.get('pred_margin_model', df.get('pred_margin')), errors='coerce') if ('pred_margin_model' in df.columns or 'pred_margin' in df.columns) else pd.Series(dtype=float)
            total_mean = pd.to_numeric(df.get('pred_total_model', df.get('pred_total')), errors='coerce') if ('pred_total_model' in df.columns or 'pred_total' in df.columns) else pd.Series(dtype=float)
            spread_series = pd.to_numeric(df.get('spread_home', df.get('closing_spread_home')), errors='coerce') if ('spread_home' in df.columns or 'closing_spread_home' in df.columns) else pd.Series(dtype=float)
            market_total_series = pd.to_numeric(df.get('market_total', df.get('closing_total')), errors='coerce') if ('market_total' in df.columns or 'closing_total' in df.columns) else pd.Series(dtype=float)

            def _resolve_sigma(cands: list[str], fallback: float) -> float:
                for k in cands:
                    v = pipeline_stats.get(k)
                    if isinstance(v, (int,float)) and not pd.isna(v) and v > 0:
                        return float(v)
                return fallback
            # Prefer empirical residual sigma from quantile_model when available
            margin_sigma_base = _resolve_sigma(['sigma_margin_emp','resid_margin_sigma','margin_resid_sigma','margin_sigma','pred_margin_sigma'], 10.5)
            total_sigma_base = _resolve_sigma(['sigma_total_emp','resid_total_sigma','total_resid_sigma','total_sigma','pred_total_sigma'], 11.0)

            def _team_scale(row: pd.Series, base_sigma: float, variance_map: dict[str,float] | None) -> float:
                if not variance_map or 'home_team' not in row or 'away_team' not in row:
                    return base_sigma
                try:
                    hv = float(variance_map.get(str(row['home_team']), base_sigma))
                    av = float(variance_map.get(str(row['away_team']), base_sigma))
                    return float(np.sqrt((hv**2 + av**2)/2.0))
                except Exception:
                    return base_sigma

            if 'home_team' in df.columns and 'away_team' in df.columns:
                margin_sigma_series = df.apply(lambda r: _team_scale(r, margin_sigma_base, team_variance_margin if isinstance(team_variance_margin, dict) else None), axis=1)
                total_sigma_series = df.apply(lambda r: _team_scale(r, total_sigma_base, team_variance_total if isinstance(team_variance_total, dict) else None), axis=1)
            else:
                margin_sigma_series = pd.Series([margin_sigma_base]*len(df))
                total_sigma_series = pd.Series([total_sigma_base]*len(df))

            # Quantile-based refinement: if predictive quantiles exist, derive per-row sigma using IQR mapping.
            try:
                # Total distribution quantiles
                if {'pred_total_q25','pred_total_q75'}.issubset(df.columns):
                    q25 = pd.to_numeric(df['pred_total_q25'], errors='coerce')
                    q75 = pd.to_numeric(df['pred_total_q75'], errors='coerce')
                    iqr = (q75 - q25).replace(0, np.nan)
                    # For normal dist: IQR = 2 * 0.67448975 * sigma
                    sigma_est = (iqr / (2 * 0.67448975)).clip(lower=0.0)
                    if sigma_est.notna().any():
                        total_sigma_series = sigma_est.where(sigma_est.notna(), total_sigma_series)
                        pipeline_stats['total_sigma_quantile_rows'] = int(sigma_est.notna().sum())
                # Margin distribution quantiles
                if {'pred_margin_q25','pred_margin_q75'}.issubset(df.columns):
                    mq25 = pd.to_numeric(df['pred_margin_q25'], errors='coerce')
                    mq75 = pd.to_numeric(df['pred_margin_q75'], errors='coerce')
                    miqr = (mq75 - mq25).replace(0, np.nan)
                    msigma_est = (miqr / (2 * 0.67448975)).clip(lower=0.0)
                    if msigma_est.notna().any():
                        margin_sigma_series = msigma_est.where(msigma_est.notna(), margin_sigma_series)
                        pipeline_stats['margin_sigma_quantile_rows'] = int(msigma_est.notna().sum())
            except Exception:
                pipeline_stats['quantile_sigma_error'] = True

            # Interval bands from residual quantiles (p10/p90) when quantile_model loaded
            try:
                qt10 = pipeline_stats.get('quantile_total_q10')
                qt90 = pipeline_stats.get('quantile_total_q90')
                qm10 = pipeline_stats.get('quantile_margin_q10')
                qm90 = pipeline_stats.get('quantile_margin_q90')
                if isinstance(qt10, (int,float)) and isinstance(qt90, (int,float)) and len(total_mean):
                    df['pred_total_p10'] = total_mean + float(qt10)
                    df['pred_total_p90'] = total_mean + float(qt90)
                    pipeline_stats['pred_total_interval_rows'] = int(df['pred_total_p10'].notna().sum())
                if isinstance(qm10, (int,float)) and isinstance(qm90, (int,float)) and len(margin_mean):
                    df['pred_margin_p10'] = margin_mean + float(qm10)
                    df['pred_margin_p90'] = margin_mean + float(qm90)
                    pipeline_stats['pred_margin_interval_rows'] = int(df['pred_margin_p10'].notna().sum())
            except Exception:
                pipeline_stats['interval_residual_quantiles_error'] = True

            from math import erf, sqrt
            def _norm_sf(z: pd.Series) -> pd.Series:
                return 0.5 * (1 - z.map(lambda x: erf(x / sqrt(2)) if pd.notna(x) else np.nan))

            # ATS probability: P(home covers) = SF((spread - margin_mean)/sigma)
            if len(margin_mean) and len(spread_series):
                z_cover = (spread_series - margin_mean) / margin_sigma_series.replace(0,np.nan)
                df['p_home_cover_dist'] = _norm_sf(z_cover)
                pipeline_stats['p_home_cover_dist_rows'] = int(df['p_home_cover_dist'].notna().sum())

            # OU probability: P(total > line) = SF((market_total - total_mean)/sigma)
            if len(total_mean) and len(market_total_series):
                z_over = (market_total_series - total_mean) / total_sigma_series.replace(0,np.nan)
                df['p_over_dist'] = _norm_sf(z_over)
                pipeline_stats['p_over_dist_rows'] = int(df['p_over_dist'].notna().sum())

            # Quantile piecewise CDF alternative (empirical-style) using available 25/75 quantiles.
            # We avoid assuming full normal shape inside IQR; tail extrapolation still uses normal with sigma from quantiles.
            try:
                have_total_q = {'pred_total_q25','pred_total_q75'}.issubset(df.columns)
                have_margin_q = {'pred_margin_q25','pred_margin_q75'}.issubset(df.columns)
                if have_total_q or have_margin_q:
                    from math import erf as _erf, sqrt as _sqrt
                    # Helper: piecewise CDF between q25, mean, q75; normal tails outside.
                    def _piecewise_prob(line_series: pd.Series,
                                        mean_series: pd.Series,
                                        q25_series: pd.Series,
                                        q75_series: pd.Series,
                                        sigma_series: pd.Series) -> pd.Series:
                        mean = mean_series.astype(float)
                        q25 = q25_series.astype(float)
                        q75 = q75_series.astype(float)
                        line = line_series.astype(float)
                        sigma = sigma_series.astype(float).replace(0, np.nan)
                        # Precompute tail normal CDF function
                        def _norm_cdf(zv: float) -> float:
                            return 0.5 * (1 + _erf(zv / _sqrt(2))) if pd.notna(zv) else np.nan
                        out = []
                        for L, m, a, b, s in zip(line, mean, q25, q75, sigma):
                            if pd.isna(L) or pd.isna(m):
                                out.append(np.nan); continue
                            # F(q25)=0.25, F(mean)=~0.5, F(q75)=0.75 by definition of predictive quantiles.
                            # Guard if quantiles inverted/missing.
                            if pd.isna(a) or pd.isna(b) or a >= b:
                                # Fallback to normal using sigma
                                zL = (L - m)/s if pd.notna(s) else np.nan
                                out.append(_norm_cdf(zL)); continue
                            if L <= a:
                                # Left tail normal extrapolation anchored at quantile a.
                                zL = (L - m)/s if pd.notna(s) else np.nan
                                out.append(_norm_cdf(zL)); continue
                            if L >= b:
                                zL = (L - m)/s if pd.notna(s) else np.nan
                                out.append(_norm_cdf(zL)); continue
                            # Inside IQR: linear interpolation.
                            if L <= m:
                                # Between q25 (0.25) and mean (0.5)
                                span = max(m - a, 1e-9)
                                frac = (L - a)/span
                                out.append(0.25 + frac * (0.5 - 0.25))
                            else:
                                # Between mean (0.5) and q75 (0.75)
                                span = max(b - m, 1e-9)
                                frac = (L - m)/span
                                out.append(0.5 + frac * (0.75 - 0.5))
                        return pd.Series(out)

                    # OU: need p(total > line) = 1 - F(line)
                    if have_total_q and len(market_total_series) and len(total_mean):
                        tq25 = pd.to_numeric(df['pred_total_q25'], errors='coerce')
                        tq75 = pd.to_numeric(df['pred_total_q75'], errors='coerce')
                        F_line = _piecewise_prob(market_total_series, total_mean, tq25, tq75, total_sigma_series)
                        df['p_over_cdf'] = 1.0 - F_line
                        pipeline_stats['p_over_cdf_rows'] = int(df['p_over_cdf'].notna().sum())
                    # ATS: home covers if margin > spread -> 1 - F(spread)
                    if have_margin_q and len(spread_series) and len(margin_mean):
                        mq25 = pd.to_numeric(df['pred_margin_q25'], errors='coerce')
                        mq75 = pd.to_numeric(df['pred_margin_q75'], errors='coerce')
                        F_spread = _piecewise_prob(spread_series, margin_mean, mq25, mq75, margin_sigma_series)
                        df['p_home_cover_cdf'] = 1.0 - F_spread
                        pipeline_stats['p_home_cover_cdf_rows'] = int(df['p_home_cover_cdf'].notna().sum())
                    pipeline_stats['probability_method_added'] = 'quantile_piecewise'
            except Exception:
                pipeline_stats['quantile_cdf_error'] = True

            # Advanced skew-adjusted probabilities via Cornish-Fisher using per-row skew estimate from quantiles.
            try:
                from src.modeling.distributions import estimate_skew, survival_from_cf  # type: ignore
                # Require quantiles + sigma to estimate skewness
                if have_total_q and 'pred_total_q25' in df.columns and 'pred_total_q75' in df.columns:
                    tq25 = pd.to_numeric(df['pred_total_q25'], errors='coerce')
                    tq75 = pd.to_numeric(df['pred_total_q75'], errors='coerce')
                    # Use total_sigma_series computed earlier
                    from src.modeling.distributions import estimate_kurtosis  # type: ignore
                    skew_total = estimate_skew(total_mean, tq25, tq75, total_sigma_series)
                    kurt_total = estimate_kurtosis(tq25, tq75, total_sigma_series)
                    if len(market_total_series):
                        # P(total > line) skew-adjusted
                        df['p_over_skew'] = survival_from_cf(total_mean, total_sigma_series, skew_total, market_total_series, kurt_total)
                        pipeline_stats['p_over_skew_rows'] = int(df['p_over_skew'].notna().sum())
                        # Record skew summary stats
                        st_clean = skew_total.replace([np.inf,-np.inf], np.nan).dropna()
                        if not st_clean.empty:
                            pipeline_stats['skew_total_median'] = float(st_clean.median())
                            pipeline_stats['skew_total_q25'] = float(st_clean.quantile(0.25))
                            pipeline_stats['skew_total_q75'] = float(st_clean.quantile(0.75))
                if have_margin_q and 'pred_margin_q25' in df.columns and 'pred_margin_q75' in df.columns:
                    mq25 = pd.to_numeric(df['pred_margin_q25'], errors='coerce')
                    mq75 = pd.to_numeric(df['pred_margin_q75'], errors='coerce')
                    skew_margin = estimate_skew(margin_mean, mq25, mq75, margin_sigma_series)
                    kurt_margin = estimate_kurtosis(mq25, mq75, margin_sigma_series)
                    if len(spread_series):
                        # P(home covers) = P(margin > spread) = survival_cf at line=spread
                        df['p_home_cover_skew'] = survival_from_cf(margin_mean, margin_sigma_series, skew_margin, spread_series, kurt_margin)
                        pipeline_stats['p_home_cover_skew_rows'] = int(df['p_home_cover_skew'].notna().sum())
                        sm_clean = skew_margin.replace([np.inf,-np.inf], np.nan).dropna()
                        if not sm_clean.empty:
                            pipeline_stats['skew_margin_median'] = float(sm_clean.median())
                            pipeline_stats['skew_margin_q25'] = float(sm_clean.quantile(0.25))
                            pipeline_stats['skew_margin_q75'] = float(sm_clean.quantile(0.75))
                pipeline_stats['probability_skew_added'] = True
            except Exception:
                pipeline_stats['probability_skew_error'] = True

            # Mixture-based skew probabilities (two-component normal) to refine tails beyond CF adjustment
            try:
                from src.modeling.distributions import mixture_survival, estimate_skew  # type: ignore
                if have_total_q and 'pred_total_q25' in df.columns and 'pred_total_q75' in df.columns and len(market_total_series):
                    tq25 = pd.to_numeric(df['pred_total_q25'], errors='coerce')
                    tq75 = pd.to_numeric(df['pred_total_q75'], errors='coerce')
                    skew_total_mix = estimate_skew(total_mean, tq25, tq75, total_sigma_series)
                    df['p_over_mix'] = mixture_survival(total_mean, total_sigma_series, skew_total_mix, market_total_series)
                    pipeline_stats['p_over_mix_rows'] = int(df['p_over_mix'].notna().sum())
                if have_margin_q and 'pred_margin_q25' in df.columns and 'pred_margin_q75' in df.columns and len(spread_series):
                    mq25 = pd.to_numeric(df['pred_margin_q25'], errors='coerce')
                    mq75 = pd.to_numeric(df['pred_margin_q75'], errors='coerce')
                    skew_margin_mix = estimate_skew(margin_mean, mq25, mq75, margin_sigma_series)
                    df['p_home_cover_mix'] = mixture_survival(margin_mean, margin_sigma_series, skew_margin_mix, spread_series)
                    pipeline_stats['p_home_cover_mix_rows'] = int(df['p_home_cover_mix'].notna().sum())
                if 'p_over_mix' in df.columns or 'p_home_cover_mix' in df.columns:
                    pipeline_stats['probability_mixture_added'] = True
            except Exception:
                pipeline_stats['probability_mixture_error'] = True

            # Extended piecewise tail-enhanced probabilities (q10/q90 approximated + skew-aware boundaries)
            try:
                # We reuse quantile + sigma + (optionally) skew to approximate q10/q90 via Cornish-Fisher.
                from src.modeling.distributions import estimate_skew, cornish_fisher_adjust_z  # type: ignore
                # Precompute constants for 10% and 90% quantiles
                z10 = -1.2815515655446004
                z90 = 1.2815515655446004

                def _piecewise_ext(line: pd.Series, mean: pd.Series, q25: pd.Series, q75: pd.Series,
                                    sigma: pd.Series, skew: pd.Series | None) -> pd.Series:
                    out: list[float] = []
                    # Basic guards to avoid division errors
                    for L, m, a, b, s, sk in zip(line, mean, q25, q75, sigma, (skew if skew is not None else [np.nan]*len(line))):
                        if pd.isna(L) or pd.isna(m) or pd.isna(a) or pd.isna(b) or pd.isna(s) or a >= b or s <= 0:
                            out.append(np.nan); continue
                        # Cornish-Fisher adjust boundary z values if skew available and finite
                        if pd.notna(sk) and abs(sk) < 20:
                            z10_adj = cornish_fisher_adjust_z(z10, sk)
                            z90_adj = cornish_fisher_adjust_z(z90, sk)
                        else:
                            z10_adj = z10; z90_adj = z90
                        q10 = m + z10_adj * s
                        q90 = m + z90_adj * s
                        if q10 > a:  # preserve ordering if adjustment pushes inside IQR
                            q10 = min(a, m - 0.01*abs(m))
                        if q90 < b:
                            q90 = max(b, m + 0.01*abs(m))
                        # Piecewise: (-inf,q10] tail normal; (q10,a] linear 0.10->0.25; (a,m] 0.25->0.50; (m,b] 0.50->0.75; (b,q90] 0.75->0.90; (q90,inf) tail normal
                        if L <= q10:
                            zL = (L - m)/s
                            out.append(_norm_cdf(zL)); continue
                        if L >= q90:
                            zL = (L - m)/s
                            out.append(_norm_cdf(zL)); continue
                        # Region (q10,a]
                        if L <= a:
                            span = max(a - q10, 1e-9)
                            frac = (L - q10)/span
                            out.append(0.10 + frac * (0.25 - 0.10)); continue
                        # Region (a,m]
                        if L <= m:
                            span = max(m - a, 1e-9)
                            frac = (L - a)/span
                            out.append(0.25 + frac * (0.50 - 0.25)); continue
                        # Region (m,b]
                        if L <= b:
                            span = max(b - m, 1e-9)
                            frac = (L - m)/span
                            out.append(0.50 + frac * (0.75 - 0.50)); continue
                        # Region (b,q90]
                        span = max(q90 - b, 1e-9)
                        frac = (L - b)/span
                        out.append(0.75 + frac * (0.90 - 0.75))
                    return pd.Series(out)

                # Totals extended piecewise
                if have_total_q and len(market_total_series):
                    tq25 = pd.to_numeric(df['pred_total_q25'], errors='coerce') if 'pred_total_q25' in df.columns else pd.Series(dtype=float)
                    tq75 = pd.to_numeric(df['pred_total_q75'], errors='coerce') if 'pred_total_q75' in df.columns else pd.Series(dtype=float)
                    if not tq25.empty and not tq75.empty:
                        # Skew reused / recomputed for resilience
                        skew_total_ext = None
                        try:
                            skew_total_ext = estimate_skew(total_mean, tq25, tq75, total_sigma_series)
                        except Exception:
                            skew_total_ext = None
                        F_line_ext = _piecewise_ext(market_total_series, total_mean, tq25, tq75, total_sigma_series, skew_total_ext)
                        df['p_over_piecewise_ext'] = 1.0 - F_line_ext
                        pipeline_stats['p_over_piecewise_ext_rows'] = int(df['p_over_piecewise_ext'].notna().sum())
                # Margin extended piecewise
                if have_margin_q and len(spread_series):
                    mq25 = pd.to_numeric(df['pred_margin_q25'], errors='coerce') if 'pred_margin_q25' in df.columns else pd.Series(dtype=float)
                    mq75 = pd.to_numeric(df['pred_margin_q75'], errors='coerce') if 'pred_margin_q75' in df.columns else pd.Series(dtype=float)
                    if not mq25.empty and not mq75.empty:
                        skew_margin_ext = None
                        try:
                            skew_margin_ext = estimate_skew(margin_mean, mq25, mq75, margin_sigma_series)
                        except Exception:
                            skew_margin_ext = None
                        F_spread_ext = _piecewise_ext(spread_series, margin_mean, mq25, mq75, margin_sigma_series, skew_margin_ext)
                        df['p_home_cover_piecewise_ext'] = 1.0 - F_spread_ext
                        pipeline_stats['p_home_cover_piecewise_ext_rows'] = int(df['p_home_cover_piecewise_ext'].notna().sum())
                if 'p_over_piecewise_ext' in df.columns or 'p_home_cover_piecewise_ext' in df.columns:
                    pipeline_stats['probability_piecewise_ext_added'] = True
            except Exception:
                pipeline_stats['piecewise_ext_error'] = True

            # KDE-based probabilities using residual archives (non-parametric tail check)
            try:
                # Load residuals artifact if earlier ingestion did not stash; attempt direct path
                resid_date2 = date_q if date_q else _today_local().strftime('%Y-%m-%d')
                resid_path2 = OUT / f'residuals_{resid_date2}.json'
                resid_payload = None
                if resid_path2.exists():
                    import json as _json_kde
                    resid_payload = _json_kde.loads(resid_path2.read_text(encoding='utf-8'))
                if isinstance(resid_payload, dict):
                    r_tot = resid_payload.get('total_residuals', [])
                    r_mar = resid_payload.get('margin_residuals', [])
                    # Convert to numpy arrays
                    arr_tot = np.array([x for x in r_tot if isinstance(x,(int,float))])
                    arr_mar = np.array([x for x in r_mar if isinstance(x,(int,float))])
                    # Simple Gaussian KDE: bandwidth = 0.9 * min(sd, IQR/1.34) * n^{-1/5}
                    def _kde_survival(base_pred: pd.Series, line_series: pd.Series, residuals: np.ndarray) -> pd.Series:
                        if residuals.size < 30:
                            return pd.Series([np.nan]*len(line_series))
                        bw = 0.0
                        try:
                            sd = float(np.std(residuals))
                            q25_r, q75_r = np.quantile(residuals, [0.25,0.75])
                            iqr_r = q75_r - q25_r
                            n = residuals.size
                            bw = 0.9 * min(sd, iqr_r/1.34) * (n ** (-1/5)) if sd > 0 else 1.0
                            bw = max(bw, 1e-6)
                        except Exception:
                            bw = 1.0
                        out_vals = []
                        for pred, line in zip(base_pred, line_series):
                            if pd.isna(pred) or pd.isna(line):
                                out_vals.append(np.nan); continue
                            # Survival P(X > line) where X = pred + residual
                            # Approximate via averaging kernel tail indicator with Gaussian kernel density weighting
                            # Use direct empirical tail proportion smoothed by kernel weights at residual = line - pred
                            shifted = line - pred
                            # Tail indicator for residual r: pred + r > line => r > shifted
                            mask_tail = residuals > shifted
                            # Weight residuals by Gaussian kernel centered at shifted
                            weights = np.exp(-0.5 * ((residuals - shifted)/bw)**2)
                            w_tail = weights[mask_tail]
                            prob = (w_tail.sum() / weights.sum()) if weights.sum() > 0 else np.nan
                            out_vals.append(float(prob) if not math.isnan(prob) else np.nan)
                        return pd.Series(out_vals)
                    if len(market_total_series) and arr_tot.size:
                        df['p_over_kde'] = _kde_survival(total_mean, market_total_series, arr_tot)
                        pipeline_stats['p_over_kde_rows'] = int(df['p_over_kde'].notna().sum())
                    if len(spread_series) and arr_mar.size:
                        df['p_home_cover_kde'] = _kde_survival(margin_mean, spread_series, arr_mar)
                        pipeline_stats['p_home_cover_kde_rows'] = int(df['p_home_cover_kde'].notna().sum())
                    if 'p_over_kde' in df.columns or 'p_home_cover_kde' in df.columns:
                        pipeline_stats['probability_kde_added'] = True
                else:
                    pipeline_stats['probability_kde_missing_residuals'] = True
            except Exception:
                pipeline_stats['probability_kde_error'] = True

            # Apply isotonic calibrators (optional) producing *_cal columns
            try:
                if calibrate_probs_enabled:
                    import joblib  # type: ignore
                    cal_dir = OUT / 'calibrators'
                    if cal_dir.exists():
                        prob_cols = [c for c in df.columns if c.startswith('p_') and df[c].notna().any()]
                        for c in prob_cols:
                            cal_fp = cal_dir / f'{c}_iso.joblib'
                            if cal_fp.exists():
                                try:
                                    iso = joblib.load(cal_fp)
                                    preds_raw = pd.to_numeric(df[c], errors='coerce')
                                    if preds_raw.notna().any():
                                        df[c + '_cal'] = pd.Series(iso.predict(preds_raw.clip(0,1)))
                                        pipeline_stats[c + '_cal_rows'] = int(df[c + '_cal'].notna().sum())
                                except Exception:
                                    pipeline_stats[c + '_cal_error'] = True
                        pipeline_stats['probability_method_calibration_applied'] = True
                    else:
                        pipeline_stats['probability_method_calibration_missing_dir'] = True
            except Exception:
                pipeline_stats['probability_method_calibration_error'] = True

            # Ensemble with classifier probabilities when available
            if 'p_home_cover' in df.columns and 'p_home_cover_dist' in df.columns:
                df['p_home_cover_ensemble'] = (pd.to_numeric(df['p_home_cover'], errors='coerce') + pd.to_numeric(df['p_home_cover_dist'], errors='coerce')) / 2.0
                pipeline_stats['p_home_cover_ensemble_rows'] = int(df['p_home_cover_ensemble'].notna().sum())
            if 'p_over' in df.columns and 'p_over_dist' in df.columns:
                df['p_over_ensemble'] = (pd.to_numeric(df['p_over'], errors='coerce') + pd.to_numeric(df['p_over_dist'], errors='coerce')) / 2.0
                pipeline_stats['p_over_ensemble_rows'] = int(df['p_over_ensemble'].notna().sum())

            # Final selection columns (priority order: ensemble > classifier > dist)
            def _select_final(order: list[str], out_col: str):
                for c in order:
                    if c in df.columns and df[c].notna().any():
                        df[out_col] = df[c]
                        pipeline_stats[out_col + '_source'] = c
                        return
                df[out_col] = np.nan
                pipeline_stats[out_col + '_source'] = None
            # Updated final selection precedence includes meta_cal > meta > ensemble > classifier > skew > cdf > dist
            # Calibrated variants precedence when enabled
            if calibrate_probs_enabled:
                _select_final(['p_home_cover_meta_cal','p_home_cover_meta','p_home_cover_ensemble','p_home_cover_cal','p_home_cover_mix_cal','p_home_cover_mix','p_home_cover_skew_cal','p_home_cover_skew','p_home_cover_piecewise_ext_cal','p_home_cover_piecewise_ext','p_home_cover_kde_cal','p_home_cover_kde','p_home_cover_cdf_cal','p_home_cover_cdf','p_home_cover_dist'], 'p_home_cover_final')
                _select_final(['p_over_meta_cal','p_over_meta','p_over_ensemble','p_over_cal','p_over_mix_cal','p_over_mix','p_over_skew_cal','p_over_skew','p_over_piecewise_ext_cal','p_over_piecewise_ext','p_over_kde_cal','p_over_kde','p_over_cdf_cal','p_over_cdf','p_over_dist'], 'p_over_final')
            else:
                _select_final(['p_home_cover_meta_cal','p_home_cover_meta','p_home_cover_ensemble','p_home_cover','p_home_cover_mix','p_home_cover_skew','p_home_cover_piecewise_ext','p_home_cover_kde','p_home_cover_cdf','p_home_cover_dist'], 'p_home_cover_final')
                _select_final(['p_over_meta_cal','p_over_meta','p_over_ensemble','p_over','p_over_mix','p_over_skew','p_over_piecewise_ext','p_over_kde','p_over_cdf','p_over_dist'], 'p_over_final')
            pipeline_stats['probability_distribution_enrichment'] = True

            # Edge confidence tagging
            try:
                if 'edge_total' in df.columns and 'pred_total' in df.columns and 'market_total' in df.columns:
                    sigma_total_clean = pd.to_numeric(df.get('pred_total_sigma'), errors='coerce') if 'pred_total_sigma' in df.columns else pd.Series([np.nan]*len(df))
                    # Confidence heuristic: lower sigma + higher absolute edge -> higher confidence
                    et = pd.to_numeric(df['edge_total'], errors='coerce')
                    conf_scores = (et.abs() / (sigma_total_clean.replace(0,np.nan)))
                    thresholds = conf_scores.quantile([0.33,0.66]) if conf_scores.notna().any() else pd.Series([np.nan,np.nan])
                    def _bucket(v):
                        if pd.isna(v): return None
                        if v <= thresholds.iloc[0]: return 'LOW'
                        if v <= thresholds.iloc[1]: return 'MED'
                        return 'HIGH'
                    df['confidence_total'] = conf_scores.map(_bucket)
                    pipeline_stats['confidence_total_populated'] = int(df['confidence_total'].notna().sum())
                if 'edge_total' in df.columns and 'edge_total'.startswith('edge_total'):
                    pass
                if 'edge_total' in df.columns and 'edge_total'.startswith('edge_total'):
                    pass
                if 'pred_margin' in df.columns and 'spread_home' in df.columns and 'edge_total' in df.columns:
                    sigma_margin_clean = pd.to_numeric(df.get('pred_margin_sigma'), errors='coerce') if 'pred_margin_sigma' in df.columns else pd.Series([np.nan]*len(df))
                    # Rebuild margin edge if not present
                    if 'edge_margin' not in df.columns and 'pred_margin' in df.columns and 'spread_home' in df.columns:
                        try:
                            df['edge_margin'] = pd.to_numeric(df['pred_margin'], errors='coerce') - pd.to_numeric(df['spread_home'], errors='coerce')
                        except Exception:
                            df['edge_margin'] = np.nan
                    em = pd.to_numeric(df.get('edge_margin'), errors='coerce')
                    conf_m_scores = em.abs() / (sigma_margin_clean.replace(0,np.nan))
                    thresholds_m = conf_m_scores.quantile([0.33,0.66]) if conf_m_scores.notna().any() else pd.Series([np.nan,np.nan])
                    def _bucket_m(v):
                        if pd.isna(v): return None
                        if v <= thresholds_m.iloc[0]: return 'LOW'
                        if v <= thresholds_m.iloc[1]: return 'MED'
                        return 'HIGH'
                    df['confidence_margin'] = conf_m_scores.map(_bucket_m)
                    pipeline_stats['confidence_margin_populated'] = int(df['confidence_margin'].notna().sum())
            except Exception:
                pipeline_stats['confidence_tag_error'] = True

            # Meta ensemble (logistic stacking) if trained artifacts exist
            try:
                import joblib  # type: ignore
                meta_cover_path = OUT / 'meta_cover.joblib'
                meta_over_path = OUT / 'meta_over.joblib'
                meta_used = []
                if meta_cover_path.exists() and {'p_home_cover','p_home_cover_dist','p_home_cover_ensemble'}.issubset(df.columns):
                    try:
                        m_cov = joblib.load(meta_cover_path)
                        Xc = df[['p_home_cover','p_home_cover_dist','p_home_cover_ensemble']].astype(float).fillna(df[['p_home_cover','p_home_cover_dist','p_home_cover_ensemble']].astype(float).mean())
                        df['p_home_cover_meta'] = m_cov.predict_proba(Xc)[:,1]
                        meta_used.append('cover')
                    except Exception as _e_cov:
                        pipeline_stats['meta_cover_error'] = str(_e_cov)
                if meta_over_path.exists() and {'p_over','p_over_dist','p_over_ensemble'}.issubset(df.columns):
                    try:
                        m_ov = joblib.load(meta_over_path)
                        Xo = df[['p_over','p_over_dist','p_over_ensemble']].astype(float).fillna(df[['p_over','p_over_dist','p_over_ensemble']].astype(float).mean())
                        df['p_over_meta'] = m_ov.predict_proba(Xo)[:,1]
                        meta_used.append('over')
                    except Exception as _e_over:
                        pipeline_stats['meta_over_error'] = str(_e_over)
                if meta_used:
                    pipeline_stats['meta_probs_used'] = meta_used
                    # Update final selection precedence to prefer meta over prior choices
                    if 'p_home_cover_meta_cal' in df.columns and df['p_home_cover_meta_cal'].notna().any():
                        df['p_home_cover_final'] = df['p_home_cover_meta_cal']
                        pipeline_stats['p_home_cover_final_source'] = 'p_home_cover_meta_cal'
                    elif 'p_home_cover_meta' in df.columns and df['p_home_cover_meta'].notna().any():
                        df['p_home_cover_final'] = df['p_home_cover_meta']
                        pipeline_stats['p_home_cover_final_source'] = 'p_home_cover_meta'
                    if 'p_over_meta_cal' in df.columns and df['p_over_meta_cal'].notna().any():
                        df['p_over_final'] = df['p_over_meta_cal']
                        pipeline_stats['p_over_final_source'] = 'p_over_meta_cal'
                    elif 'p_over_meta' in df.columns and df['p_over_meta'].notna().any():
                        df['p_over_final'] = df['p_over_meta']
                        pipeline_stats['p_over_final_source'] = 'p_over_meta'
            except Exception:
                pipeline_stats['meta_probs_error'] = True
    except Exception:
        pipeline_stats['probability_distribution_error'] = True

    # Merge model predictions (if not already merged) then compute model-based edges
    try:
        if 'model_preds' in locals() and not model_preds.empty and 'game_id' in df.columns and 'game_id' in model_preds.columns:
            # Avoid duplicate merges; include calibrated columns when present
            all_model_cols = ['pred_total_model','pred_margin_model','pred_total_calibrated','pred_margin_calibrated','pred_total_model_basis','pred_margin_model_basis']
            need_cols = [c for c in all_model_cols if c not in df.columns and c in model_preds.columns]
            if need_cols:
                mp = model_preds.copy()
                mp['game_id'] = mp['game_id'].astype(str)
                df['game_id'] = df['game_id'].astype(str)
                keep = ['game_id'] + need_cols
                df = df.merge(mp[keep], on='game_id', how='left', suffixes=('','_m'))
        # ------------------------------------------------------------------
        # Team-level universal prediction fallback: ensure ALL games have
        # point & margin predictions even if model inference skipped rows.
        # For any missing pred_total_model/pred_margin_model we derive
        # lightweight estimates from features (tempo/off/def) and basic
        # historical averages. This guarantees downstream staking and
        # edge metrics exist for every team.
        # ------------------------------------------------------------------
        if 'game_id' in df.columns:
            # Identify rows missing model totals
            missing_model_total = ('pred_total_model' not in df.columns) or df['pred_total_model'].isna().all() or df['pred_total_model'].isna()
            missing_model_margin = ('pred_margin_model' not in df.columns) or df['pred_margin_model'].isna().all() or df['pred_margin_model'].isna()
            # Load features once for derived tempo/off/def based estimates
            feat_sources = ["features_curr.csv","features_all.csv","features_week.csv","features_last2.csv"]
            feat_df = pd.DataFrame()
            for name in feat_sources:
                p = OUT / name
                if p.exists():
                    try:
                        ftmp = pd.read_csv(p)
                        if not ftmp.empty and 'game_id' in ftmp.columns:
                            feat_df = ftmp
                            break
                    except Exception:
                        continue
            # Feature completeness fallback enrichment
            try:
                feat_df = _feature_fallback_enrich(feat_df)
            except Exception:
                pass
            if not feat_df.empty and 'game_id' in feat_df.columns:
                feat_df['game_id'] = feat_df['game_id'].astype(str)
                # Precompute derived base total from offensive/def + tempo when available
                if {'home_off_rating','away_off_rating','home_def_rating','away_def_rating','home_tempo_rating','away_tempo_rating'}.issubset(feat_df.columns):
                    try:
                        ho = pd.to_numeric(feat_df['home_off_rating'], errors='coerce')
                        ao = pd.to_numeric(feat_df['away_off_rating'], errors='coerce')
                        hd = pd.to_numeric(feat_df['home_def_rating'], errors='coerce')
                        ad = pd.to_numeric(feat_df['away_def_rating'], errors='coerce')
                        ht = pd.to_numeric(feat_df['home_tempo_rating'], errors='coerce')
                        at = pd.to_numeric(feat_df['away_tempo_rating'], errors='coerce')
                        # Simple possession scale (tempo ratings assumed ~70 baseline)
                        poss_scale = (ht + at) / 140.0
                        raw_total_est = ((ho + ao) / 2.0 + (200 - (hd + ad) / 2.0)) * poss_scale * 0.5
                        feat_df['derived_total_est'] = raw_total_est
                    except Exception:
                        feat_df['derived_total_est'] = np.nan
                else:
                    feat_df['derived_total_est'] = np.nan
                # Margin estimate from off/def differential if available
                if {'home_off_rating','home_def_rating','away_off_rating','away_def_rating'}.issubset(feat_df.columns):
                    try:
                        ho = pd.to_numeric(feat_df['home_off_rating'], errors='coerce')
                        hd = pd.to_numeric(feat_df['home_def_rating'], errors='coerce')
                        ao = pd.to_numeric(feat_df['away_off_rating'], errors='coerce')
                        ad = pd.to_numeric(feat_df['away_def_rating'], errors='coerce')
                        feat_df['derived_margin_est'] = (ho - hd) - (ao - ad)
                    except Exception:
                        feat_df['derived_margin_est'] = np.nan
                else:
                    feat_df['derived_margin_est'] = np.nan
                # Build maps
                total_map = feat_df.set_index('game_id')['derived_total_est'].to_dict()
                margin_map = feat_df.set_index('game_id')['derived_margin_est'].to_dict()
            else:
                total_map = {}
                margin_map = {}
            if isinstance(missing_model_total, pd.Series) and missing_model_total.any():
                # League average fallback if no derived feature
                league_avg_total = 141.5
                rng = random.Random(42)
                new_totals = []
                for idx, row in df.iterrows():
                    if missing_model_total.iloc[idx]:
                        gid = str(row.get('game_id'))
                        base = total_map.get(gid, league_avg_total)
                        # Add tiny deterministic jitter from team names to avoid identical predictions
                        ht = str(row.get('home_team',''))
                        at = str(row.get('away_team',''))
                        seed_val = hash(ht + '|' + at) & 0xffff
                        rng.seed(seed_val)
                        jitter = rng.uniform(-2.2, 2.2)
                        new_totals.append(base + jitter)
                    else:
                        new_totals.append(row.get('pred_total_model'))
                df['pred_total_model'] = new_totals
                df['pred_total_model_basis'] = df.get('pred_total_model_basis')
                df['pred_total_model_basis'] = df['pred_total_model_basis'].where(df['pred_total_model_basis'].notna(), 'fallback_derived' if total_map else 'fallback_league_avg')
            if isinstance(missing_model_margin, pd.Series) and missing_model_margin.any():
                league_avg_margin = 4.8
                rng = random.Random(99)
                new_margins = []
                for idx, row in df.iterrows():
                    if missing_model_margin.iloc[idx]:
                        gid = str(row.get('game_id'))
                        base = margin_map.get(gid, league_avg_margin)
                        ht = str(row.get('home_team',''))
                        at = str(row.get('away_team',''))
                        seed_val = hash('m:' + ht + '|' + at) & 0xffff
                        rng.seed(seed_val)
                        jitter = rng.uniform(-1.5, 1.5)
                        new_margins.append(base + jitter)
                    else:
                        new_margins.append(row.get('pred_margin_model'))
                df['pred_margin_model'] = new_margins
                df['pred_margin_model_basis'] = df.get('pred_margin_model_basis')
                df['pred_margin_model_basis'] = df['pred_margin_model_basis'].where(df['pred_margin_model_basis'].notna(), 'fallback_derived' if margin_map else 'fallback_league_avg')
        # Margin sigma-based Kelly adjustment if available
        try:
            if {'kelly_fraction_total','pred_total_sigma'}.issubset(df.columns):
                rel_scale_t = pd.to_numeric(df['pred_total_sigma'], errors='coerce') / max(float(pipeline_stats.get('pred_total_sigma_mean', 12.0)) or 12.0, 1e-6)
                df['kelly_fraction_total_adj'] = pd.to_numeric(df['kelly_fraction_total'], errors='coerce') / rel_scale_t.clip(lower=0.5, upper=2.5)
            if {'kelly_fraction_ml_home','pred_margin_model'}.issubset(df.columns) and 'pred_margin_sigma' in df.columns:
                rel_scale_m = pd.to_numeric(df['pred_margin_sigma'], errors='coerce') / max(float(pipeline_stats.get('pred_margin_sigma_mean', 8.0)) or 8.0, 1e-6)
                df['kelly_fraction_margin_adj'] = pd.to_numeric(df.get('kelly_fraction_ml_home'), errors='coerce') / rel_scale_m.clip(lower=0.5, upper=2.5)
        except Exception:
            pass
        # Compute model edges vs market & closing
        if {'pred_total_model','market_total'}.issubset(df.columns):
            df['edge_total_model'] = pd.to_numeric(df['pred_total_model'], errors='coerce') - pd.to_numeric(df['market_total'], errors='coerce')
        if {'pred_total_model','closing_total'}.issubset(df.columns):
            try:
                df['edge_closing_model'] = pd.to_numeric(df['pred_total_model'], errors='coerce') - pd.to_numeric(df['closing_total'], errors='coerce')
            except Exception:
                df['edge_closing_model'] = None
        if {'pred_margin_model','spread_home'}.issubset(df.columns):
            try:
                # Spread convention: negative spread means home favored; edge is model margin - (-spread_home)
                sh = pd.to_numeric(df['spread_home'], errors='coerce')
                pm = pd.to_numeric(df['pred_margin_model'], errors='coerce')
                df['edge_margin_model'] = pm + sh  # since sh is negative when home favored
            except Exception:
                df['edge_margin_model'] = None
        # Display ATS edge based on displayed pred_margin when spread present
        try:
            if {'pred_margin','spread_home'}.issubset(df.columns):
                pm_d = pd.to_numeric(df['pred_margin'], errors='coerce')
                sh_d = pd.to_numeric(df['spread_home'], errors='coerce')
                # ATS edge is predicted home spread minus market spread: (-pred_margin) - spread_home
                df['edge_ats'] = (-pm_d) - sh_d
                # Z for ATS if sigma available
                if 'pred_margin_sigma_bootstrap' in df.columns:
                    sigm = pd.to_numeric(df['pred_margin_sigma_bootstrap'], errors='coerce').replace(0, np.nan)
                    z_m = ((-pm_d) - sh_d) / sigm
                    df['edge_ats_z'] = z_m
                    zv_m = z_m.replace([np.inf,-np.inf], np.nan).dropna()
                    if not zv_m.empty:
                        pipeline_stats['edge_ats_z_q25'] = float(zv_m.quantile(0.25))
                        pipeline_stats['edge_ats_z_median'] = float(zv_m.median())
                        pipeline_stats['edge_ats_z_q75'] = float(zv_m.quantile(0.75))
        except Exception:
            pipeline_stats['edge_ats_z_error'] = True
        # Unify: prefer model predictions as primary displayed values; override legacy pred_total/pred_margin if present
        try:
            if 'pred_total_model' in df.columns:
                # Optional bias correction using model_tuning.json totals_bias (if loaded earlier in pipeline_stats)
                bias = None
                try:
                    bias = float(pipeline_stats.get('totals_bias')) if 'pipeline_stats' in locals() and isinstance(pipeline_stats.get('totals_bias'), (int,float)) else None
                except Exception:
                    bias = None
                raw_model_total = pd.to_numeric(df['pred_total_model'], errors='coerce')
                if bias is not None and not raw_model_total.isna().all():
                    raw_model_total = raw_model_total + bias  # shift raw model totals by bias if tuning bias provided
                # Prefer calibrated totals if present; fall back to raw model
                calibrated_available = 'pred_total_calibrated' in df.columns
                calibrated_total = pd.to_numeric(df['pred_total_calibrated'], errors='coerce') if calibrated_available else pd.Series([np.nan]*len(df))
                # Use calibrated when it has any non-NaN values; else fallback to raw
                use_calibrated = calibrated_available and calibrated_total.notna().any()
                chosen_total = calibrated_total if use_calibrated else raw_model_total
                # Bootstrap totals uncertainty (global) if historical residuals available
                try:
                    if 'pred_total_sigma_bootstrap' not in df.columns:
                        boot_sigma_t = np.nan
                        hist_dir_t = OUT / 'daily_results'
                        if hist_dir_t.exists():
                            res_files_t = sorted(hist_dir_t.glob('results_*.csv'))[-60:]
                            actual_list_t = []
                            pred_list_t = []
                            for rp_t in reversed(res_files_t):
                                try:
                                    rdf_t = pd.read_csv(rp_t)
                                except Exception:
                                    continue
                                if rdf_t.empty or not {'home_score','away_score'}.issubset(rdf_t.columns):
                                    continue
                                pt_col = 'pred_total_model' if 'pred_total_model' in rdf_t.columns else ('pred_total' if 'pred_total' in rdf_t.columns else None)
                                if pt_col is None:
                                    continue
                                hs_t = pd.to_numeric(rdf_t['home_score'], errors='coerce')
                                as_t = pd.to_numeric(rdf_t['away_score'], errors='coerce')
                                done_t = hs_t.notna() & as_t.notna() & ((hs_t + as_t) > 0)
                                if done_t.sum() == 0:
                                    continue
                                actual_tot = (hs_t + as_t)[done_t]
                                pred_tot_hist = pd.to_numeric(rdf_t[pt_col], errors='coerce')[done_t]
                                good_t = actual_tot.notna() & pred_tot_hist.notna()
                                if good_t.sum() == 0:
                                    continue
                                actual_list_t.append(actual_tot[good_t])
                                pred_list_t.append(pred_tot_hist[good_t])
                                if sum(len(x) for x in actual_list_t) >= 400:
                                    break
                            hist_rows_t = sum(len(x) for x in actual_list_t)
                            if hist_rows_t >= 25:
                                actual_all_t = pd.concat(actual_list_t, ignore_index=True)
                                pred_all_t = pd.concat(pred_list_t, ignore_index=True)
                                # Use simple residuals of raw predictions (ignore calibration for bootstrap variety)
                                try:
                                    residuals_t = actual_all_t - pred_all_t
                                    boot_sigma_t = float(np.std(residuals_t)) if residuals_t.notna().any() else np.nan
                                except Exception:
                                    boot_sigma_t = np.nan
                                pipeline_stats['totals_bootstrap_rows'] = int(hist_rows_t)
                        df['pred_total_sigma_bootstrap'] = boot_sigma_t
                        pipeline_stats['pred_total_sigma_bootstrap_global'] = boot_sigma_t if not pd.isna(boot_sigma_t) else None
                except Exception:
                    pipeline_stats['pred_total_sigma_bootstrap_error'] = True
                # Record stats for both raw and calibrated distributions
                try:
                    pipeline_stats['pred_total_model_raw_stats'] = {
                        'count': int(raw_model_total.notna().sum()),
                        'min': float(raw_model_total.min()) if raw_model_total.notna().any() else None,
                        'max': float(raw_model_total.max()) if raw_model_total.notna().any() else None,
                        'mean': float(raw_model_total.mean()) if raw_model_total.notna().any() else None,
                        'std': float(raw_model_total.std()) if raw_model_total.notna().any() else None,
                        'unique': int(raw_model_total.nunique()) if raw_model_total.notna().any() else 0
                    }
                except Exception:
                    pass
                if use_calibrated:
                    try:
                        pipeline_stats['pred_total_model_calibrated_stats'] = {
                            'count': int(calibrated_total.notna().sum()),
                            'min': float(calibrated_total.min()) if calibrated_total.notna().any() else None,
                            'max': float(calibrated_total.max()) if calibrated_total.notna().any() else None,
                            'mean': float(calibrated_total.mean()) if calibrated_total.notna().any() else None,
                            'std': float(calibrated_total.std()) if calibrated_total.notna().any() else None,
                            'unique': int(calibrated_total.nunique()) if calibrated_total.notna().any() else 0
                        }
                        pipeline_stats['pred_total_model_calibrated_head'] = calibrated_total.head(10).tolist()
                    except Exception:
                        pass
                try:
                    pipeline_stats['pred_total_model_unified_stats'] = {
                        'count': int(chosen_total.notna().sum()),
                        'min': float(chosen_total.min()) if chosen_total.notna().any() else None,
                        'max': float(chosen_total.max()) if chosen_total.notna().any() else None,
                        'mean': float(chosen_total.mean()) if chosen_total.notna().any() else None,
                        'std': float(chosen_total.std()) if chosen_total.notna().any() else None,
                        'unique': int(chosen_total.nunique()) if chosen_total.notna().any() else 0,
                        'source': 'calibrated' if use_calibrated else 'raw_model'
                    }
                    pipeline_stats['pred_total_model_head_unified'] = chosen_total.head(10).tolist()
                except Exception:
                    pass
                # Preserve raw model under separate column; don't degrade existing calibrated pred_total
                df['pred_total_model_raw'] = raw_model_total
                # Decide whether to override displayed pred_total: prefer calibrated over blended/base when available.
                existing_pred = pd.to_numeric(df.get('pred_total'), errors='coerce') if 'pred_total' in df.columns else pd.Series(np.nan, index=df.index)
                basis_ser = df.get('pred_total_basis') if 'pred_total_basis' in df.columns else pd.Series([None] * len(df))
                basis_str = basis_ser.astype(str).str.lower()
                # Lower-precedence bases we can override when calibrated model is available
                lower_bases = {'blen','blend','blended','blended_model_baseline','market_copy'}
                lower_mask = basis_ser.isna() | basis_str.isin(lower_bases) | basis_str.str.startswith(('synthetic','fallback','derived'))
                # chosen_total may be all NaN if model preds file is stale; guard against overwriting real calibrated values
                if use_calibrated:
                    override_mask = (existing_pred.isna() | lower_mask) & chosen_total.notna()
                else:
                    # When only raw model is present, override strictly missing values to avoid masking true calibrated/blended decisions
                    override_mask = existing_pred.isna() & chosen_total.notna()
                if use_calibrated:
                    try:
                        pipeline_stats['prefer_cal_effective'] = True
                    except Exception:
                        pass
                if override_mask.any():
                    df.loc[override_mask, 'pred_total'] = chosen_total[override_mask]
                    # Basis only for overridden rows
                    if 'pred_total_basis' not in df.columns:
                        df['pred_total_basis'] = None
                    new_basis = 'model_calibrated' if use_calibrated else 'model_raw'
                    df.loc[override_mask, 'pred_total_basis'] = new_basis
                    try:
                        pipeline_stats['model_unify_overrode_pred_total_rows'] = int(override_mask.sum())
                        pipeline_stats['model_unify_overrode_due_to_lower_basis'] = int(((existing_pred.notna()) & lower_mask & override_mask).sum())
                    except Exception:
                        pass
                # Always expose unified value for diagnostics
                df['pred_total_model_unified'] = chosen_total
                if not override_mask.any():
                    pipeline_stats['model_unify_no_override'] = True
                # Edge recompute only for rows we changed; keep previous edge_total/edge_closing otherwise
                if override_mask.any():
                    if 'market_total' in df.columns:
                        df.loc[override_mask, 'edge_total'] = pd.to_numeric(df.loc[override_mask, 'pred_total'], errors='coerce') - pd.to_numeric(df.loc[override_mask, 'market_total'], errors='coerce')
                        # Recompute z for affected rows if sigma present
                        if 'pred_total_sigma_bootstrap' in df.columns:
                            try:
                                sig_ser = pd.to_numeric(df['pred_total_sigma_bootstrap'], errors='coerce').replace(0, np.nan)
                                df.loc[override_mask, 'edge_total_z'] = pd.to_numeric(df.loc[override_mask, 'edge_total'], errors='coerce') / sig_ser[override_mask]
                            except Exception:
                                pass
                    if 'closing_total' in df.columns:
                        try:
                            df.loc[override_mask, 'edge_closing'] = pd.to_numeric(df.loc[override_mask, 'pred_total'], errors='coerce') - pd.to_numeric(df.loc[override_mask, 'closing_total'], errors='coerce')
                        except Exception:
                            pass
                # Final guard: if EVERY displayed total is NaN but unified chosen_total has values, force promotion
                try:
                    if 'pred_total' in df.columns and df['pred_total'].isna().all() and chosen_total.notna().any():
                        df['pred_total'] = chosen_total
                        if 'pred_total_basis' not in df.columns:
                            df['pred_total_basis'] = 'model_force_fill'
                        else:
                            df['pred_total_basis'] = df['pred_total_basis'].where(~df['pred_total'].notna(), df['pred_total_basis'])
                            fill_mask_ff = df['pred_total'].notna() & (df['pred_total_basis'].isna() | (df['pred_total_basis'].astype(str) == ''))
                            if fill_mask_ff.any():
                                df.loc[fill_mask_ff, 'pred_total_basis'] = 'model_force_fill'
                        pipeline_stats['pred_total_force_fill_from_unified'] = int(chosen_total.notna().sum())
                except Exception:
                    pipeline_stats['pred_total_force_fill_error'] = True
                # Meta ensemble totals (optional): combine raw/calibrated/market/derived components with learned weights
                try:
                    if 'pred_total_meta' not in df.columns:
                        meta_path = OUT / 'meta_ensemble_totals.txt'
                        if meta_path.exists():
                            try:
                                import lightgbm as lgb  # type: ignore
                            except Exception:
                                meta_path = None
                        if meta_path and meta_path.exists():
                            # Build feature frame
                            feat_cols = []
                            base_map = {
                                'pred_total_model_raw': 'raw_model_total',
                                'pred_total_calibrated': 'calibrated_total',
                                'market_total': 'market_total',
                                'pred_total': 'unified_pred_total'
                            }
                            tmp = pd.DataFrame({'game_id': df.get('game_id')})
                            for out_col, src_col in base_map.items():
                                if src_col in locals() or src_col in df.columns or out_col in df.columns:
                                    # Use df columns (some already set) for consistency
                                    source_series = df[out_col] if out_col in df.columns else (df[src_col] if src_col in df.columns else None)
                                    if source_series is not None:
                                        tmp[out_col] = pd.to_numeric(source_series, errors='coerce')
                                        feat_cols.append(out_col)
                            # Add market_total and spread derived if present
                            for c in ['market_total','closing_total']:
                                if c in df.columns and c not in tmp.columns:
                                    tmp[c] = pd.to_numeric(df[c], errors='coerce')
                                    feat_cols.append(c)
                            # LightGBM expects no NaNs; simple impute with column means
                            if feat_cols:
                                for c in feat_cols:
                                    if c in tmp.columns:
                                        ser = tmp[c]
                                        if ser.isna().any():
                                            mval = ser.mean()
                                            tmp[c] = ser.fillna(mval)
                                try:
                                    booster = lgb.Booster(model_file=str(meta_path))
                                    meta_pred = booster.predict(tmp[feat_cols])
                                    df['pred_total_meta'] = meta_pred
                                    pipeline_stats['meta_ensemble_totals_rows'] = int(len(df))
                                    pipeline_stats['meta_ensemble_totals_features'] = feat_cols
                                except Exception:
                                    pipeline_stats['meta_ensemble_totals_error'] = True
                except Exception:
                    pipeline_stats['meta_ensemble_totals_error'] = True
            if 'pred_margin_model' in df.columns:
                adj_model_margin = pd.to_numeric(df['pred_margin_model'], errors='coerce')
                # ------------------------------
                # Margin calibration expansion
                # Fit linear calibration: actual_margin ~ pred_margin_model
                # Uses recent daily_results history; falls back to raw if insufficient.
                # ------------------------------
                calib_intercept = 0.0
                calib_slope = 1.0
                calib_rows = 0
                actual_all = None
                pred_all = None
                try:
                    hist_dir = OUT / 'daily_results'
                    if hist_dir.exists():
                        result_files = sorted(hist_dir.glob('results_*.csv'))[-60:]  # last ~60 days max
                        actual_list = []
                        pred_list = []
                        for rp in reversed(result_files):  # iterate newest first for recency weighting
                            try:
                                rdf = pd.read_csv(rp)
                            except Exception:
                                continue
                            if rdf.empty or not {'home_score','away_score'}.issubset(rdf.columns):
                                continue
                            pm_col = 'pred_margin_model' if 'pred_margin_model' in rdf.columns else ('pred_margin' if 'pred_margin' in rdf.columns else None)
                            if pm_col is None:
                                continue
                            hs = pd.to_numeric(rdf['home_score'], errors='coerce')
                            as_ = pd.to_numeric(rdf['away_score'], errors='coerce')
                            done_mask = hs.notna() & as_.notna() & ((hs + as_) > 0)
                            if done_mask.sum() == 0:
                                continue
                            actual_margin = (hs - as_)[done_mask]
                            pred_margin_hist = pd.to_numeric(rdf[pm_col], errors='coerce')[done_mask]
                            good_mask = actual_margin.notna() & pred_margin_hist.notna()
                            if good_mask.sum() == 0:
                                continue
                            actual_list.append(actual_margin[good_mask])
                            pred_list.append(pred_margin_hist[good_mask])
                            calib_rows = sum(len(x) for x in actual_list)
                            if calib_rows >= 400:
                                break
                        if calib_rows >= 25:
                            actual_all = pd.concat(actual_list, ignore_index=True)
                            pred_all = pd.concat(pred_list, ignore_index=True)
                            try:
                                var_pred = float(np.var(pred_all))
                                if var_pred > 1e-8:
                                    cov = float(np.cov(pred_all, actual_all)[0,1])
                                    calib_slope = cov / var_pred
                                    calib_intercept = float(np.mean(actual_all)) - calib_slope * float(np.mean(pred_all))
                                # Guardrails
                                calib_slope = float(np.clip(calib_slope, 0.4, 1.6))
                                calib_intercept = float(np.clip(calib_intercept, -8.0, 8.0))
                            except Exception:
                                calib_intercept, calib_slope = 0.0, 1.0
                        pipeline_stats['margin_calibration_rows'] = int(calib_rows)
                        pipeline_stats['margin_calibration_intercept'] = float(calib_intercept)
                        pipeline_stats['margin_calibration_slope'] = float(calib_slope)
                except Exception:
                    pipeline_stats['margin_calibration_error'] = True
                # Apply calibration decision
                if calib_rows >= 25:
                    calibrated_margin = calib_intercept + calib_slope * adj_model_margin
                    df['pred_margin_calibrated'] = calibrated_margin
                    chosen_margin = calibrated_margin
                    margin_basis_code = 'model_calibrated'
                else:
                    df['pred_margin_calibrated'] = np.nan
                    chosen_margin = adj_model_margin
                    margin_basis_code = 'model_raw'
                # Bootstrap global margin uncertainty if not already present
                try:
                    if 'pred_margin_sigma_bootstrap' not in df.columns:
                        if calib_rows >= 25 and actual_all is not None and pred_all is not None:
                            residuals = actual_all - (calib_intercept + calib_slope * pred_all)
                            boot_sigma = float(np.std(residuals)) if residuals.notna().any() else np.nan
                        else:
                            boot_sigma = np.nan
                        df['pred_margin_sigma_bootstrap'] = boot_sigma
                        pipeline_stats['pred_margin_sigma_bootstrap_global'] = boot_sigma if not pd.isna(boot_sigma) else None
                except Exception:
                    pass
                df['pred_margin'] = chosen_margin
                df['pred_margin_basis'] = df.get('pred_margin_basis')
                df['pred_margin_basis'] = df['pred_margin_basis'].where(df['pred_margin_basis'].notna(), margin_basis_code)
                # Final guard: if pred_margin ended up all NaN but chosen_margin has values, force fill
                try:
                    if 'pred_margin' in df.columns and df['pred_margin'].isna().all() and pd.to_numeric(chosen_margin, errors='coerce').notna().any():
                        df['pred_margin'] = pd.to_numeric(chosen_margin, errors='coerce')
                        if 'pred_margin_basis' not in df.columns:
                            df['pred_margin_basis'] = 'model_force_fill'
                        else:
                            fill_mask_m = df['pred_margin'].notna() & (df['pred_margin_basis'].isna() | (df['pred_margin_basis'].astype(str) == ''))
                            if fill_mask_m.any():
                                df.loc[fill_mask_m, 'pred_margin_basis'] = 'model_force_fill'
                        pipeline_stats['pred_margin_force_fill_from_chosen'] = int(pd.to_numeric(chosen_margin, errors='coerce').notna().sum())
                except Exception:
                    pipeline_stats['pred_margin_force_fill_error'] = True
                if 'spread_home' in df.columns:
                    sh2 = pd.to_numeric(df['spread_home'], errors='coerce')
                    # ATS edge: predicted home spread minus market spread
                    df['edge_ats'] = (-pd.to_numeric(df['pred_margin'], errors='coerce')) - sh2
                # Recompute favored side / by values
                pm_num = pd.to_numeric(df['pred_margin'], errors='coerce')
                df['favored_side'] = np.where(pm_num > 0, 'Home', np.where(pm_num < 0, 'Away', 'Even'))
                df['favored_by'] = pm_num.abs()
                # Meta ensemble margin (optional)
                try:
                    if 'pred_margin_meta' not in df.columns:
                        meta_path_m = OUT / 'meta_ensemble_margin.txt'
                        if meta_path_m.exists():
                            try:
                                import lightgbm as lgb  # type: ignore
                            except Exception:
                                meta_path_m = None
                        if meta_path_m and meta_path_m.exists():
                            feat_cols_m = []
                            tmpm = pd.DataFrame({'game_id': df.get('game_id')})
                            # Candidate sources
                            cand_map_m = {
                                'pred_margin_model': 'pred_margin_model',
                                'pred_margin_calibrated': 'pred_margin_calibrated',
                                'spread_home': 'spread_home'
                            }
                            for out_col, src_col in cand_map_m.items():
                                if src_col in df.columns:
                                    tmpm[out_col] = pd.to_numeric(df[src_col], errors='coerce')
                                    feat_cols_m.append(out_col)
                            if feat_cols_m:
                                for c in feat_cols_m:
                                    ser = tmpm[c]
                                    if ser.isna().any():
                                        tmpm[c] = ser.fillna(ser.mean())
                                try:
                                    import lightgbm as lgb  # type: ignore
                                    booster_m = lgb.Booster(model_file=str(meta_path_m))
                                    meta_margin_pred = booster_m.predict(tmpm[feat_cols_m])
                                    df['pred_margin_meta'] = meta_margin_pred
                                    pipeline_stats['meta_ensemble_margin_rows'] = int(len(df))
                                    pipeline_stats['meta_ensemble_margin_features'] = feat_cols_m
                                except Exception:
                                    pipeline_stats['meta_ensemble_margin_error'] = True
                except Exception:
                    pipeline_stats['meta_ensemble_margin_error'] = True
        except Exception:
            pass
        # Correlation & aggregate diagnostics (legacy divergence vs pred_total removed post-unification)
        if 'pipeline_stats' in locals():
            try:
                if {'pred_total_model','market_total'}.issubset(df.columns):
                    corr_mt = df[['pred_total_model','market_total']].dropna()
                    if not corr_mt.empty:
                        x = pd.to_numeric(corr_mt['pred_total_model'], errors='coerce')
                        y = pd.to_numeric(corr_mt['market_total'], errors='coerce')
                        # Guard against zero-variance vectors which cause divide-by-zero in correlation
                        if x.notna().sum() > 1 and y.notna().sum() > 1 and x.std(ddof=0) > 0 and y.std(ddof=0) > 0:
                            pipeline_stats['corr_pred_total_model_market'] = float(np.corrcoef(x, y)[0, 1])
                        else:
                            pipeline_stats['corr_pred_total_model_market'] = None
                if 'market_total_basis' in df.columns:
                    pipeline_stats['synthetic_market_total_count'] = int((df['market_total_basis'].astype(str).str.startswith('synthetic')).sum())
                if 'spread_home_basis' in df.columns:
                    pipeline_stats['synthetic_spread_home_count'] = int((df['spread_home_basis'].astype(str).str.startswith('synthetic')).sum())
                if 'edge_total_model' in df.columns:
                    pipeline_stats['model_edge_rows'] = int(df['edge_total_model'].notna().sum())
                # Record bias applied if any
                if 'totals_bias' in pipeline_stats and 'pred_total_model' in df.columns:
                    pipeline_stats['model_totals_bias_applied'] = pipeline_stats.get('totals_bias')
                # Adaptive team variance application: enrich per-row sigma columns using team-level rolling std
                try:
                    if team_variance_total and 'home_team' in df.columns and 'away_team' in df.columns:
                        def _tv_total(row):
                            ht = str(row.get('home_team'))
                            at = str(row.get('away_team'))
                            v1 = team_variance_total.get(ht)
                            v2 = team_variance_total.get(at)
                            if v1 is None and v2 is None:
                                return None
                            if v1 is None: v1 = v2
                            if v2 is None: v2 = v1
                            return float(np.mean([v for v in [v1, v2] if v is not None])) if v1 or v2 else None
                        df['pred_total_sigma_team'] = df.apply(_tv_total, axis=1)
                    if team_variance_margin and 'home_team' in df.columns and 'away_team' in df.columns:
                        def _tv_margin(row):
                            ht = str(row.get('home_team'))
                            at = str(row.get('away_team'))
                            v1 = team_variance_margin.get(ht)
                            v2 = team_variance_margin.get(at)
                            if v1 is None and v2 is None:
                                return None
                            if v1 is None: v1 = v2
                            if v2 is None: v2 = v1
                            return float(np.mean([v for v in [v1, v2] if v is not None])) if v1 or v2 else None
                        df['pred_margin_sigma_team'] = df.apply(_tv_margin, axis=1)
                    # Combine baseline sigma with team component when available (geometric mean for stability)
                    if 'pred_total_sigma' in df.columns and 'pred_total_sigma_team' in df.columns:
                        base = pd.to_numeric(df['pred_total_sigma'], errors='coerce')
                        teamc = pd.to_numeric(df['pred_total_sigma_team'], errors='coerce')
                        combo = np.sqrt(base * teamc)
                        df['pred_total_sigma_adaptive'] = np.where(teamc.notna(), combo, base)
                    if 'pred_margin_sigma' in df.columns and 'pred_margin_sigma_team' in df.columns:
                        basem = pd.to_numeric(df['pred_margin_sigma'], errors='coerce')
                        teamm = pd.to_numeric(df['pred_margin_sigma_team'], errors='coerce')
                        combo_m = np.sqrt(basem * teamm)
                        df['pred_margin_sigma_adaptive'] = np.where(teamm.notna(), combo_m, basem)
                    # Pipeline stats aggregates for adaptive sigma
                    if 'pred_total_sigma_adaptive' in df.columns:
                        pipeline_stats['pred_total_sigma_adaptive_mean'] = float(pd.to_numeric(df['pred_total_sigma_adaptive'], errors='coerce').mean())
                    if 'pred_margin_sigma_adaptive' in df.columns:
                        pipeline_stats['pred_margin_sigma_adaptive_mean'] = float(pd.to_numeric(df['pred_margin_sigma_adaptive'], errors='coerce').mean())
                    # Context note when recalibration flagged
                    if pipeline_stats.get('recalibration_needed'):
                        pipeline_stats['recalibration_variance_context'] = 'Adaptive sigmas computed; recalibration flag active.'
                except Exception:
                    pipeline_stats['adaptive_team_variance_error'] = True
            except Exception:
                pass
    except Exception:
        pass
    # Merge richer aggregated edge metrics if edges file present (kelly fractions, edge_margin)
    if not edges.empty and "game_id" in df.columns and "game_id" in edges.columns:
        try:
            edges_f = edges.copy()
            edges_f["game_id"] = edges_f["game_id"].astype(str)
            df["game_id"] = df["game_id"].astype(str)
            keep_cols = [c for c in ["game_id","edge_total","edge_margin","kelly_fraction_total","kelly_fraction_ml_home","kelly_fraction_ml_away"] if c in edges_f.columns]
            if keep_cols:
                agg = edges_f[keep_cols].groupby("game_id").median(numeric_only=True).reset_index()
                df = df.merge(agg, on="game_id", how="left", suffixes=("", "_agg"))
        except Exception:
            pass

    # Merge model probabilities (win/ATS/OU) if present for the selected date
    try:
        if 'game_id' in df.columns:
            prob_path = None
            try:
                if 't_date3' in locals() and isinstance(t_date3, str):
                    cand = OUT / f"model_probs_{t_date3}.csv"
                    if cand.exists():
                        prob_path = cand
            except Exception:
                pass
            if prob_path is None and date_q:
                cand2 = OUT / f"model_probs_{str(date_q)}.csv"
                if cand2.exists():
                    prob_path = cand2
            if prob_path is not None and prob_path.exists():
                probs = _safe_read_csv(prob_path)
                need_cols = {"game_id","p_home_win","p_home_cover","p_over"}
                if isinstance(probs, pd.DataFrame) and need_cols.issubset(probs.columns):
                    probs["game_id"] = probs["game_id"].astype(str)
                    df = df.merge(probs[["game_id","p_home_win","p_home_cover","p_over"]], on="game_id", how="left")
                    pipeline_stats['model_probs_merged'] = int(probs['game_id'].nunique())
                else:
                    pipeline_stats['model_probs_unmerged_reason'] = 'missing_columns'
            else:
                pipeline_stats['model_probs_missing'] = True
    except Exception:
        pipeline_stats['model_probs_merge_error'] = True

    # Dynamic weighting: blend model predictions with baseline totals when health degraded.
    try:
        if 'perf_model_health' in pipeline_stats and 'pred_total_model' in df.columns and 'pred_total' in df.columns:
            health = pipeline_stats.get('perf_model_health')
            ps = pipeline_stats.get('perf_predictability_score')
            if isinstance(ps, (int, float)):
                base_w = max(0.3, min(1.0, ps / 0.8))  # scale relative to desired high score ~0.8
            else:
                base_w = 0.6
            if health == 'critical':
                w = base_w * 0.55
            elif health == 'degraded':
                w = base_w * 0.75
            else:
                w = base_w
            w = max(0.25, min(0.95, w))
            pm = pd.to_numeric(df['pred_total_model'], errors='coerce')
            pb = pd.to_numeric(df['pred_total'], errors='coerce')
            blended = (w * pm) + ((1 - w) * pb)
            mask = pm.notna() & pb.notna()
            if mask.any():
                df.loc[mask, 'pred_total_model_blended'] = blended[mask]
                df.loc[mask, 'pred_total_model_weight'] = w
                pipeline_stats['dynamic_weight_applied'] = int(mask.sum())
                pipeline_stats['dynamic_weight_w'] = w
    except Exception:
        pipeline_stats['dynamic_weight_error'] = True

    # Projected team scores from pred_total and pred_margin
    # Fallback / adjustment for implausibly low or missing totals (e.g., early-season sparse features causing collapsed predictions).
    # We derive a tempo/off/def based estimate from features CSV when available and use it to fill missing
    # predictions or blend if the raw prediction looks implausibly low.
    try:
        if "pred_total" in df.columns:
            # Preserve raw value
            df["pred_total_raw"] = df["pred_total"]
            # Load features to compute derived totals map
            feat_df = pd.DataFrame()
            for name in ("features_curr.csv", "features_all.csv", "features_week.csv", "features_last2.csv"):
                p = OUT / name
                if p.exists():
                    try:
                        feat_df = pd.read_csv(p)
                        break
                    except Exception:
                        feat_df = pd.DataFrame()
            derived_map: dict[str, float] = {}
            derived_margin_map: dict[str, float] = {}
            # Pair-based fallback maps (using canonical slugs) to handle game_id mismatches due to normalization
            derived_pair_map: dict[str, float] = {}
            derived_margin_pair_map: dict[str, float] = {}
            if not feat_df.empty and "game_id" in feat_df.columns:
                feat_df["game_id"] = feat_df["game_id"].astype(str)
                # Ensure needed columns exist
                h_off = feat_df.get("home_off_rating")
                a_off = feat_df.get("away_off_rating")
                h_def = feat_df.get("home_def_rating")
                a_def = feat_df.get("away_def_rating")
                h_tmp = feat_df.get("home_tempo_rating")
                a_tmp = feat_df.get("away_tempo_rating")
                tmp_sum = feat_df.get("tempo_rating_sum")
                # Precompute canonical pair key when possible
                try:
                    if {"home_team","away_team"}.issubset(feat_df.columns):
                        feat_df["_home_slug"] = feat_df["home_team"].astype(str).map(_canon_slug)
                        feat_df["_away_slug"] = feat_df["away_team"].astype(str).map(_canon_slug)
                        feat_df["_pair_key"] = feat_df.apply(lambda rr: "::".join(sorted([str(rr.get("_home_slug")), str(rr.get("_away_slug"))])), axis=1)
                except Exception:
                    pass
                for _, r in feat_df.iterrows():
                    gid = str(r.get("game_id"))
                    try:
                        # Tempo average
                        if pd.notna(r.get("home_tempo_rating")) and pd.notna(r.get("away_tempo_rating")):
                            tempo_avg = (float(r.get("home_tempo_rating")) + float(r.get("away_tempo_rating"))) / 2.0
                        elif pd.notna(r.get("tempo_rating_sum")):
                            tempo_avg = float(r.get("tempo_rating_sum")) / 2.0
                        else:
                            tempo_avg = 70.0  # fallback typical pace
                        off_home = float(r.get("home_off_rating")) if pd.notna(r.get("home_off_rating")) else 100.0
                        off_away = float(r.get("away_off_rating")) if pd.notna(r.get("away_off_rating")) else 100.0
                        def_home = float(r.get("home_def_rating")) if pd.notna(r.get("home_def_rating")) else 100.0
                        def_away = float(r.get("away_def_rating")) if pd.notna(r.get("away_def_rating")) else 100.0
                        # Derived offensive efficiency adjusted total (simple possession model)
                        # Expected points per 100 possessions for each side: off - opp_def, clipped
                        exp_home_pp100 = np.clip(off_home - def_away, 80, 130)
                        exp_away_pp100 = np.clip(off_away - def_home, 80, 130)
                        # Convert to per-game using tempo average (possessions ~ tempo_avg)
                        derived_total = (exp_home_pp100 + exp_away_pp100) / 100.0 * tempo_avg
                        # Simple derived margin: difference in per-100 scaled by tempo
                        derived_margin = (exp_home_pp100 - exp_away_pp100) / 100.0 * tempo_avg
                        # Clamp plausible NCAA range
                        derived_total = float(np.clip(derived_total, 110, 185))
                        derived_margin = float(np.clip(derived_margin, -35, 35))
                        derived_map[gid] = derived_total
                        derived_margin_map[gid] = derived_margin
                        # Also record by pair when available
                        pk = r.get("_pair_key")
                        if isinstance(pk, str) and pk:
                            derived_pair_map[pk] = derived_total
                            derived_margin_pair_map[pk] = derived_margin
                    except Exception:
                        continue
            # If pred_total missing, fill from derived when available; also fill pred_margin if missing
            if derived_map:
                if "pred_total" in df.columns:
                    mask_missing_pt = df["pred_total"].isna()
                    if mask_missing_pt.any():
                        # Try by game_id; if missing, fallback via pair-key lookup
                        def _fill_pt(idx, gid):
                            val = derived_map.get(str(gid))
                            if val is not None:
                                return val
                            # Build row pair key
                            try:
                                hn = _canon_slug(str(df.at[idx, "home_team"]))
                                an = _canon_slug(str(df.at[idx, "away_team"]))
                                pk = "::".join(sorted([hn, an]))
                                return derived_pair_map.get(pk)
                            except Exception:
                                return None
                        for idx in df.index[mask_missing_pt]:
                            df.loc[idx, "pred_total"] = _fill_pt(idx, df.at[idx, "game_id"])
                        # Mark basis for visibility
                        df.loc[mask_missing_pt, "pred_total_basis"] = df.loc[mask_missing_pt, "pred_total_basis"].where(df.loc[mask_missing_pt, "pred_total_basis"].notna(), "features_derived") if "pred_total_basis" in df.columns else "features_derived"
                if "pred_margin" in df.columns and derived_margin_map:
                    mask_missing_pm = df["pred_margin"].isna()
                    if mask_missing_pm.any():
                        def _fill_pm(idx, gid):
                            val = derived_margin_map.get(str(gid))
                            if val is not None:
                                return val
                            try:
                                hn = _canon_slug(str(df.at[idx, "home_team"]))
                                an = _canon_slug(str(df.at[idx, "away_team"]))
                                pk = "::".join(sorted([hn, an]))
                                return derived_margin_pair_map.get(pk)
                            except Exception:
                                return None
                        for idx in df.index[mask_missing_pm]:
                            df.loc[idx, "pred_margin"] = _fill_pm(idx, df.at[idx, "game_id"])
                        df.loc[mask_missing_pm, "pred_margin_basis"] = df.loc[mask_missing_pm, "pred_margin_basis"].where(df.loc[mask_missing_pm, "pred_margin_basis"].notna(), "features_derived") if "pred_margin_basis" in df.columns else "features_derived"
            # Blend predictions when implausibly low vs derived; capture instrumentation for diagnostics
            pred_total_was_adj = []
            blend_weights_model: list[float] = []
            blend_weights_derived: list[float] = []
            pred_total_derived_used: list[float | None] = []
            pred_total_raw_list: list[float | None] = []
            blend_severe_flags: list[bool] = []
            if derived_map:
                adj_vals = []
                for _, r in df.iterrows():
                    gid = str(r.get("game_id"))
                    pred = r.get("pred_total")
                    pred_total_raw_list.append(r.get("pred_total_raw"))
                    derived = derived_map.get(gid)
                    if derived is None:
                        try:
                            hn = _canon_slug(str(r.get("home_team")))
                            an = _canon_slug(str(r.get("away_team")))
                            pk = "::".join(sorted([hn, an]))
                            derived = derived_pair_map.get(pk)
                        except Exception:
                            derived = None
                    pred_total_derived_used.append(derived)
                    if pred is None or pd.isna(pred) or derived is None:
                        # No adjustment possible
                        adj_vals.append(pred)
                        pred_total_was_adj.append(False)
                        blend_weights_model.append(np.nan)
                        blend_weights_derived.append(np.nan)
                        blend_severe_flags.append(False)
                        continue
                    # Revised adjustment criteria:
                    # Only blend when derived appears substantially higher AND credible (>=130) and raw pred is far lower.
                    # Skip adjustment if derived is near lower clamp (<=120) to avoid uniform floor inflation.
                    derived_val = float(derived) if derived is not None else None
                    pred_val = float(pred) if pred is not None else None
                    severe_gap = False
                    do_blend = False
                    if derived_val is not None and pred_val is not None:
                        if derived_val >= 130 and pred_val < 0.78 * derived_val and (derived_val - pred_val) >= 18:
                            do_blend = True
                            severe_gap = pred_val < 0.70 * derived_val
                    if do_blend:
                        w_model = 0.45 if severe_gap else 0.55  # lean slightly more to model than before
                        w_derived = 1.0 - w_model
                        blended = w_model * pred_val + w_derived * derived_val
                        # Allow lower values (remove hard 112 floor); retain upper plausibility cap
                        # Ensure blended totals respect guardrails
                        blended = float(np.clip(blended, 110, 188))
                        adj_vals.append(blended)
                        pred_total_was_adj.append(True)
                        blend_weights_model.append(w_model)
                        blend_weights_derived.append(w_derived)
                        blend_severe_flags.append(severe_gap)
                    else:
                        adj_vals.append(pred_val)
                        pred_total_was_adj.append(False)
                        blend_weights_model.append(1.0)
                        blend_weights_derived.append(0.0)
                        blend_severe_flags.append(False)
                df["pred_total"] = adj_vals
                df["pred_total_adjusted"] = pred_total_was_adj
                df["pred_total_blend_w_model"] = blend_weights_model
                df["pred_total_blend_w_derived"] = blend_weights_derived
                df["pred_total_blend_severe"] = blend_severe_flags
                try:
                    pt_post = pd.to_numeric(df['pred_total'], errors='coerce')
                    pipeline_stats['pred_total_post_blend_stats'] = {
                        'min': float(pt_post.min()) if pt_post.notna().any() else None,
                        'max': float(pt_post.max()) if pt_post.notna().any() else None,
                        'mean': float(pt_post.mean()) if pt_post.notna().any() else None,
                        'std': float(pt_post.std()) if pt_post.notna().any() else None,
                        'unique': int(pt_post.nunique()) if pt_post.notna().any() else 0,
                        'adjusted_rows': int(pd.Series(pred_total_was_adj).sum()),
                        'skipped_rows': int(len(pred_total_was_adj) - pd.Series(pred_total_was_adj).sum())
                    }
                except Exception:
                    pass
                df["derived_total"] = pred_total_derived_used if "derived_total" not in df.columns else df["derived_total"].where(df["derived_total"].notna(), pred_total_derived_used)
                # Keep raw value accessible if overwritten
                if "pred_total_raw" not in df.columns:
                    df["pred_total_raw"] = pred_total_raw_list
                # Basis column for template badge
                if "pred_total_basis" not in df.columns:
                    df["pred_total_basis"] = None
                df["pred_total_basis"] = np.where(df["pred_total_adjusted"], df["pred_total_basis"].where(df["pred_total_basis"].notna(), "blended_low"), df["pred_total_basis"])
                try:
                    final_pt = pd.to_numeric(df['pred_total'], errors='coerce')
                    pipeline_stats['pred_total_final_stats'] = {
                        'count': int(final_pt.notna().sum()),
                        'min': float(final_pt.min()) if final_pt.notna().any() else None,
                        'max': float(final_pt.max()) if final_pt.notna().any() else None,
                        'mean': float(final_pt.mean()) if final_pt.notna().any() else None,
                        'std': float(final_pt.std()) if final_pt.notna().any() else None,
                        'unique': int(final_pt.nunique()) if final_pt.notna().any() else 0,
                        'adjusted_rows': int(df.get('pred_total_adjusted', pd.Series(dtype=int)).sum() if 'pred_total_adjusted' in df.columns else 0)
                    }
                    pipeline_stats['pred_total_head_final'] = final_pt.head(10).tolist()
                    try:
                        pipeline_stats['pred_total_missing_final'] = int(final_pt.isna().sum())
                    except Exception:
                        pass
                    # Safety net: if final pred_total collapsed to a single constant (classic 112 trap) but we retained richer originals, revert.
                    try:
                        uniq_ct = int(final_pt.nunique()) if final_pt.notna().any() else 0
                        if uniq_ct == 1:
                            const_val = float(final_pt.dropna().iloc[0]) if final_pt.notna().any() else None
                            has_orig = 'pred_total_orig' in df.columns
                            if const_val is not None and has_orig:
                                orig_series = pd.to_numeric(df['pred_total_orig'], errors='coerce')
                                orig_unique = int(orig_series.nunique()) if orig_series.notna().any() else 0
                                # Revert only if originals had reasonable diversity (>5 uniques) and constant is near the known collapse value or std ~0.
                                if orig_unique > 5 and (abs(const_val - 112.0) < 0.75 or float(final_pt.std()) < 0.05):
                                    # Revert rows whose basis currently indicates blend / adjusted low-total operations.
                                    basis_ser = df.get('pred_total_basis').astype(str).str.lower() if 'pred_total_basis' in df.columns else pd.Series(['']*len(df))
                                    revert_mask = basis_ser.isin(['blend','blended_low','blend_low','model_v1'])
                                    if revert_mask.any():
                                        df.loc[revert_mask, 'pred_total'] = df.loc[revert_mask, 'pred_total_orig']
                                        # Restore original basis where available
                                        if 'pred_total_basis_orig' in df.columns:
                                            df.loc[revert_mask, 'pred_total_basis'] = df.loc[revert_mask, 'pred_total_basis_orig']
                                        pipeline_stats['final_uniform_reverted'] = int(revert_mask.sum())
                                        pipeline_stats['final_uniform_value'] = const_val
                                        # Recompute stats after revert
                                        final_pt2 = pd.to_numeric(df['pred_total'], errors='coerce')
                                        pipeline_stats['pred_total_final_stats']['min'] = float(final_pt2.min()) if final_pt2.notna().any() else None
                                        pipeline_stats['pred_total_final_stats']['max'] = float(final_pt2.max()) if final_pt2.notna().any() else None
                                        pipeline_stats['pred_total_final_stats']['mean'] = float(final_pt2.mean()) if final_pt2.notna().any() else None
                                        pipeline_stats['pred_total_final_stats']['std'] = float(final_pt2.std()) if final_pt2.notna().any() else None
                                        pipeline_stats['pred_total_final_stats']['unique'] = int(final_pt2.nunique()) if final_pt2.notna().any() else 0
                                        pipeline_stats['pred_total_head_final'] = final_pt2.head(10).tolist()
                    except Exception:
                        pipeline_stats['final_uniform_revert_error'] = True
                except Exception:
                    pass
                # Global market-aware low-total guardrail: even if a total exists (model/calibrated),
                # floor extreme outliers using the current market. This prevents cases like total ~80
                # when market is ~150.5. We do not change basis labels here; instrumentation records hits.
                try:
                    if {'pred_total','market_total'}.issubset(df.columns):
                        pt_series = pd.to_numeric(df['pred_total'], errors='coerce')
                        mt_series = pd.to_numeric(df['market_total'], errors='coerce')
                        # Floor = max(110, 0.80 * market_total)
                        floor_series = np.maximum(110.0, mt_series * 0.80)
                        mask_floor = pt_series.notna() & mt_series.notna() & (pt_series < floor_series)
                        if mask_floor.any():
                            df.loc[mask_floor, 'pred_total'] = floor_series[mask_floor]
                            try:
                                pipeline_stats.setdefault('low_total_guard_global_applied', 0)
                                pipeline_stats['low_total_guard_global_applied'] += int(mask_floor.sum())
                            except Exception:
                                pass
                except Exception:
                    try:
                        pipeline_stats['low_total_guard_global_error'] = True
                    except Exception:
                        pass
            # Compute projected team scores using adjusted pred_total
        if {"pred_total", "pred_margin"}.issubset(df.columns):
            df["proj_home"] = (pd.to_numeric(df["pred_total"], errors="coerce") + pd.to_numeric(df["pred_margin"], errors="coerce")) / 2.0
            df["proj_away"] = pd.to_numeric(df["pred_total"], errors="coerce") - df["proj_home"]
            # Derive half projections for display
            half_ratio = 0.485
            df["pred_total_1h"] = pd.to_numeric(df["pred_total"], errors="coerce") * half_ratio
            df["pred_total_2h"] = pd.to_numeric(df["pred_total"], errors="coerce") * (1.0 - half_ratio)
            df["pred_margin_1h"] = pd.to_numeric(df["pred_margin"], errors="coerce") * 0.5
            df["pred_margin_2h"] = pd.to_numeric(df["pred_margin"], errors="coerce") * 0.5
            # Derived 1H projected team scores and winner/ATS labels
            try:
                pt1 = pd.to_numeric(df["pred_total_1h"], errors="coerce")
                pm1 = pd.to_numeric(df["pred_margin_1h"], errors="coerce")
                df["proj_home_1h"] = (pt1 + pm1) / 2.0
                df["proj_away_1h"] = pt1 - df["proj_home_1h"]
                df["pred_winner_1h"] = np.where(pm1 > 0, "Home", np.where(pm1 < 0, "Away", "Even"))
                # ATS descriptor for predictions (not stored as a single string; computed in template when needed)
            except Exception:
                df["proj_home_1h"] = None
                df["proj_away_1h"] = None
                df["pred_winner_1h"] = None
            # Derived 2H projected team scores and winner label
            try:
                pt2 = pd.to_numeric(df["pred_total_2h"], errors="coerce")
                pm2 = pd.to_numeric(df["pred_margin_2h"], errors="coerce")
                df["proj_home_2h"] = (pt2 + pm2) / 2.0
                df["proj_away_2h"] = pt2 - df["proj_home_2h"]
                df["pred_winner_2h"] = np.where(pm2 > 0, "Home", np.where(pm2 < 0, "Away", "Even"))
            except Exception:
                df["proj_home_2h"] = None
                df["proj_away_2h"] = None
                df["pred_winner_2h"] = None
    except Exception:
        df["proj_home"] = None
        df["proj_away"] = None

    # Prediction coverage hard fallback: ensure pred_total/pred_margin exist for all rows
    try:
        if isinstance(df, pd.DataFrame) and len(df):
            if 'pred_total' not in df.columns:
                df['pred_total'] = np.nan
            if 'pred_margin' not in df.columns:
                df['pred_margin'] = np.nan
            pt = pd.to_numeric(df['pred_total'], errors='coerce')
            pm = pd.to_numeric(df['pred_margin'], errors='coerce')
            # Fill totals: use model, then derived, then league average
            if pt.isna().any():
                model_pt = pd.to_numeric(df.get('pred_total_model'), errors='coerce') if 'pred_total_model' in df.columns else pd.Series(np.nan, index=df.index)
                derived_pt = pd.to_numeric(df.get('derived_total'), errors='coerce') if 'derived_total' in df.columns else pd.Series(np.nan, index=df.index)
                fill_pt = model_pt.where(model_pt.notna(), derived_pt)
                # League average fallback
                fill_pt = fill_pt.where(fill_pt.notna(), 141.5)
                miss = pt.isna()
                if miss.any():
                    df.loc[miss, 'pred_total'] = fill_pt[miss]
                    try:
                        df['pred_total_basis'] = df.get('pred_total_basis')
                        df.loc[miss, 'pred_total_basis'] = df.loc[miss, 'pred_total_basis'].where(df.loc[miss, 'pred_total_basis'].notna(), 'fallback_final')
                    except Exception:
                        pass
            # Fill margins: use model, then derived, then neutral
            if pm.isna().any():
                model_pm = pd.to_numeric(df.get('pred_margin_model'), errors='coerce') if 'pred_margin_model' in df.columns else pd.Series(np.nan, index=df.index)
                # If features produced a derived margin map earlier
                derived_pm = pd.to_numeric(df.get('derived_margin'), errors='coerce') if 'derived_margin' in df.columns else pd.Series(np.nan, index=df.index)
                fill_pm = model_pm.where(model_pm.notna(), derived_pm)
                fill_pm = fill_pm.where(fill_pm.notna(), 0.0)
                missm = pm.isna()
                if missm.any():
                    df.loc[missm, 'pred_margin'] = fill_pm[missm]
                    try:
                        df['pred_margin_basis'] = df.get('pred_margin_basis')
                        df.loc[missm, 'pred_margin_basis'] = df.loc[missm, 'pred_margin_basis'].where(df.loc[missm, 'pred_margin_basis'].notna(), 'fallback_final')
                    except Exception:
                        pass
    except Exception:
        pass

    # Coverage + teams fix: backfill missing team names from schedule and append missing games
    try:
        if isinstance(df, pd.DataFrame) and 'game_id' in df.columns:
            df['game_id'] = df['game_id'].astype(str)
            # Load schedule for today/selected date
            games_curr_path = OUT / 'games_curr.csv'
            sched_df = _safe_read_csv(games_curr_path) if games_curr_path.exists() else pd.DataFrame()
            if not sched_df.empty and 'game_id' in sched_df.columns:
                # Promote generic columns to canonical names on base df
                try:
                    if 'home_team' not in df.columns and 'home' in df.columns:
                        df['home_team'] = df['home'].astype(str)
                    if 'away_team' not in df.columns and 'away' in df.columns:
                        df['away_team'] = df['away'].astype(str)
                except Exception:
                    pass
                sched_df['game_id'] = sched_df['game_id'].astype(str)
                # Filter schedule by date if available
                if date_q and 'date' in sched_df.columns:
                    sched_df = sched_df[sched_df['date'].astype(str) == str(date_q)].copy()
                # Canonicalize team names using team_map.csv when available
                try:
                    tm = _safe_read_csv(ROOT / 'data' / 'team_map.csv')
                    # Support either (input,canon) or (raw,canonical) header styles
                    cols_ic = {'input','canon'}
                    cols_rc = {'raw','canonical'}
                    mp = {}
                    if not tm.empty and cols_ic.issubset(tm.columns):
                        mp = dict(zip(tm['input'].astype(str), tm['canon'].astype(str)))
                    elif not tm.empty and cols_rc.issubset(tm.columns):
                        mp = dict(zip(tm['raw'].astype(str), tm['canonical'].astype(str)))
                        if 'home_team' in df.columns:
                            df['home_team'] = df['home_team'].astype(str).map(lambda x: mp.get(x, x))
                        if 'away_team' in df.columns:
                            df['away_team'] = df['away_team'].astype(str).map(lambda x: mp.get(x, x))
                        if 'home_team' in sched_df.columns:
                            sched_df['home_team'] = sched_df['home_team'].astype(str).map(lambda x: mp.get(x, x))
                        if 'away_team' in sched_df.columns:
                            sched_df['away_team'] = sched_df['away_team'].astype(str).map(lambda x: mp.get(x, x))
                except Exception:
                    pass
                # Backfill team names where missing on existing rows
                lut_cols = ['game_id']
                if 'home_team' in sched_df.columns: lut_cols.append('home_team')
                if 'away_team' in sched_df.columns: lut_cols.append('away_team')
                lut = sched_df[lut_cols].drop_duplicates()
                df = df.merge(lut, on='game_id', how='left', suffixes=('', '_sched'))
                if 'home_team_sched' in df.columns:
                    df['home_team'] = df['home_team'].where(df['home_team'].notna() & df['home_team'].astype(str).str.len().gt(0), df['home_team_sched'])
                if 'away_team_sched' in df.columns:
                    df['away_team'] = df['away_team'].where(df['away_team'].notna() & df['away_team'].astype(str).str.len().gt(0), df['away_team_sched'])
                drop_cols = [c for c in ['home_team_sched','away_team_sched'] if c in df.columns]
                if drop_cols:
                    df = df.drop(columns=drop_cols)
                # Append missing games from schedule
                existing_ids = set(df['game_id'].astype(str))
                missing_ids = [gid for gid in sched_df['game_id'].unique().tolist() if gid not in existing_ids]
                if missing_ids:
                    add_df = sched_df[sched_df['game_id'].isin(missing_ids)].copy()
                    # Ensure canonical team columns exist
                    if 'home_team' not in add_df.columns and 'home' in add_df.columns:
                        add_df['home_team'] = add_df['home'].astype(str)
                    if 'away_team' not in add_df.columns and 'away' in add_df.columns:
                        add_df['away_team'] = add_df['away'].astype(str)
                    # Start time propagation
                    if 'start_time' not in add_df.columns:
                        for st_col in ('commence_time','commence_time_g','start_time_g'):
                            if st_col in add_df.columns:
                                add_df['start_time'] = add_df[st_col]
                                break
                    keep_min = [c for c in ['game_id','date','home_team','away_team','start_time','venue'] if c in add_df.columns]
                    add_df = add_df[keep_min] if keep_min else add_df[['game_id']]
                    for c in ['pred_total','pred_margin','market_total','spread_home']:
                        if c not in add_df.columns:
                            add_df[c] = np.nan
                    df = pd.concat([df, add_df], ignore_index=True)
                    try:
                        pipeline_stats['coverage_supplement_added'] = int(len(missing_ids))
                    except Exception:
                        pass
        # Drop any residual rows lacking team names to avoid blank cards
        if not df.empty and {'home_team','away_team'}.issubset(df.columns):
            # Schedule backfill: coalesce teams from games_curr.csv by game_id
            try:
                games_curr_path = OUT / 'games_curr.csv'
                if games_curr_path.exists():
                    sched_df = _safe_read_csv(games_curr_path)
                    if not sched_df.empty and 'game_id' in sched_df.columns:
                        sched_df['game_id'] = sched_df['game_id'].astype(str)
                        hcol = 'home_team' if 'home_team' in sched_df.columns else ('home' if 'home' in sched_df.columns else None)
                        acol = 'away_team' if 'away_team' in sched_df.columns else ('away' if 'away' in sched_df.columns else None)
                        lut_cols = {'game_id': sched_df['game_id']}
                        if hcol:
                            lut_cols['home_team_sched'] = sched_df[hcol].astype(str)
                        if acol:
                            lut_cols['away_team_sched'] = sched_df[acol].astype(str)
                        lut = pd.DataFrame(lut_cols)
                        if not lut.empty:
                            df = df.merge(lut, on='game_id', how='left', suffixes=('', '_sched'))
                            if 'home_team_sched' in df.columns:
                                df['home_team'] = df['home_team'].where(df['home_team'].notna() & (df['home_team'].astype(str).str.strip() != ''), df['home_team_sched'])
                            if 'away_team_sched' in df.columns:
                                df['away_team'] = df['away_team'].where(df['away_team'].notna() & (df['away_team'].astype(str).str.strip() != ''), df['away_team_sched'])
                            drop_cols = [c for c in ('home_team_sched','away_team_sched') if c in df.columns]
                            if drop_cols:
                                df = df.drop(columns=drop_cols)
            except Exception:
                pass
            # Fill teams from synthetic game_id tokens when available
            try:
                if 'game_id' in df.columns:
                    from pathlib import Path
                    def _parse_synth_gid(gid: str):
                        try:
                            if isinstance(gid, str) and gid.startswith('synthetic:'):
                                core = gid.split('@', 1)[0].replace('synthetic:', '')
                                parts = core.split('::')
                                if len(parts) == 2:
                                    return parts[0], parts[1]
                        except Exception:
                            return None, None
                        return None, None
                    # Build alias map from team_map.csv when present
                    alias_map = {}
                    try:
                        tm_path = Path(__file__).parent / 'data' / 'team_map.csv'
                        if tm_path.exists():
                            tm_df = pd.read_csv(tm_path)
                            if {'input','canon'}.issubset(tm_df.columns):
                                alias_map = dict(zip(tm_df['input'].astype(str), tm_df['canon'].astype(str)))
                            elif {'raw','canonical'}.issubset(tm_df.columns):
                                alias_map = dict(zip(tm_df['raw'].astype(str), tm_df['canonical'].astype(str)))
                    except Exception:
                        alias_map = {}
                    def _canon(tok: str | None):
                        if tok is None:
                            return None
                        t = str(tok).strip()
                        return alias_map.get(t, t.replace('-', ' ').title())
                    mask_fill = df['home_team'].isna() | (df['home_team'].astype(str).str.strip() == '') | df['away_team'].isna() | (df['away_team'].astype(str).str.strip() == '')
                    if mask_fill.any():
                        for idx in df.index[mask_fill]:
                            htok, atok = _parse_synth_gid(str(df.at[idx, 'game_id']))
                            if htok and (pd.isna(df.at[idx, 'home_team']) or str(df.at[idx, 'home_team']).strip() == ''):
                                df.at[idx, 'home_team'] = _canon(htok)
                            if atok and (pd.isna(df.at[idx, 'away_team']) or str(df.at[idx, 'away_team']).strip() == ''):
                                df.at[idx, 'away_team'] = _canon(atok)
            except Exception:
                pass
            mask_bad = df['home_team'].isna() | (df['home_team'].astype(str).str.strip() == '') | df['away_team'].isna() | (df['away_team'].astype(str).str.strip() == '')
            if mask_bad.any():
                # Try backfilling from pre-filter frames (games_all, preds_all)
                try:
                    if 'game_id' in df.columns:
                        df['game_id'] = df['game_id'].astype(str)
                        # Backfill from games_all
                        if 'game_id' in games_all.columns:
                            ga = games_all.copy()
                            ga['game_id'] = ga['game_id'].astype(str)
                            # Normalize legacy columns
                            if 'home' in ga.columns and 'home_team' not in ga.columns:
                                ga['home_team'] = ga['home']
                            if 'away' in ga.columns and 'away_team' not in ga.columns:
                                ga['away_team'] = ga['away']
                            keep = [c for c in ['game_id','home_team','away_team'] if c in ga.columns]
                            if set(['game_id','home_team','away_team']).issubset(keep):
                                df = df.merge(ga[keep], on='game_id', how='left', suffixes=('', '_ga'))
                                df['home_team'] = df['home_team'].where(df['home_team'].notna() & (df['home_team'].astype(str).str.strip() != ''), df.get('home_team_ga'))
                                df['away_team'] = df['away_team'].where(df['away_team'].notna() & (df['away_team'].astype(str).str.strip() != ''), df.get('away_team_ga'))
                                for c in ('home_team_ga','away_team_ga'):
                                    if c in df.columns:
                                        try:
                                            df.drop(columns=[c], inplace=True)
                                        except Exception:
                                            pass
                        # Backfill from preds_all
                        if 'game_id' in preds_all.columns:
                            pa = preds_all.copy()
                            pa['game_id'] = pa['game_id'].astype(str)
                            # Find any pair of team columns
                            candidates = [
                                ('home_team','away_team'),
                                ('home','away'),
                                ('home_name','away_name'),
                                ('homeTeam','awayTeam')
                            ]
                            pair = next((pr for pr in candidates if set(pr).issubset(set(pa.columns))), None)
                            if pair:
                                hcol, acol = pair
                                if 'home_team' not in pa.columns:
                                    pa['home_team'] = pa[hcol]
                                if 'away_team' not in pa.columns:
                                    pa['away_team'] = pa[acol]
                                keep = ['game_id','home_team','away_team']
                                df = df.merge(pa[keep], on='game_id', how='left', suffixes=('', '_pa'))
                                df['home_team'] = df['home_team'].where(df['home_team'].notna() & (df['home_team'].astype(str).str.strip() != ''), df.get('home_team_pa'))
                                df['away_team'] = df['away_team'].where(df['away_team'].notna() & (df['away_team'].astype(str).str.strip() != ''), df.get('away_team_pa'))
                                for c in ('home_team_pa','away_team_pa'):
                                    if c in df.columns:
                                        try:
                                            df.drop(columns=[c], inplace=True)
                                        except Exception:
                                            pass
                except Exception:
                    pass
                # Re-evaluate mask and drop only true residual blanks
                mask_bad = df['home_team'].isna() | (df['home_team'].astype(str).str.strip() == '') | df['away_team'].isna() | (df['away_team'].astype(str).str.strip() == '')
                # Targeted ESPN numeric game_id resolver via odds/predictions before dropping
                try:
                    if mask_bad.any() and 'game_id' in df.columns:
                        df['game_id'] = df['game_id'].astype(str)
                        numeric_mask = df['game_id'].str.fullmatch(r"\d+")
                        if numeric_mask.any():
                            # Odds-based backfill
                            if not odds.empty and 'game_id' in odds.columns:
                                o2 = odds.copy()
                                o2['game_id'] = o2['game_id'].astype(str)
                                if 'home' in o2.columns and 'home_team' not in o2.columns:
                                    o2['home_team'] = o2['home']
                                if 'away' in o2.columns and 'away_team' not in o2.columns:
                                    o2['away_team'] = o2['away']
                                keep = [c for c in ['game_id','home_team','away_team'] if c in o2.columns]
                                if set(['game_id','home_team','away_team']).issubset(keep):
                                    df = df.merge(o2[keep], on='game_id', how='left', suffixes=('', '_od2'))
                                    nm_h = numeric_mask & (df['home_team'].isna() | (df['home_team'].astype(str).str.strip()==''))
                                    nm_a = numeric_mask & (df['away_team'].isna() | (df['away_team'].astype(str).str.strip()==''))
                                    if nm_h.any():
                                        df.loc[nm_h, 'home_team'] = df.loc[nm_h, 'home_team_od2']
                                    if nm_a.any():
                                        df.loc[nm_a, 'away_team'] = df.loc[nm_a, 'away_team_od2']
                                    for c in ('home_team_od2','away_team_od2'):
                                        if c in df.columns:
                                            try:
                                                df.drop(columns=[c], inplace=True)
                                            except Exception:
                                                pass
                            # Predictions-based backfill
                            if not preds_all.empty and 'game_id' in preds_all.columns:
                                p2 = preds_all.copy()
                                p2['game_id'] = p2['game_id'].astype(str)
                                cand = [
                                    ('home_team','away_team'),
                                    ('home','away'),
                                    ('home_name','away_name'),
                                    ('homeTeam','awayTeam')
                                ]
                                pair = next((pr for pr in cand if set(pr).issubset(set(p2.columns))), None)
                                if pair:
                                    hcol, acol = pair
                                    if 'home_team' not in p2.columns:
                                        p2['home_team'] = p2[hcol]
                                    if 'away_team' not in p2.columns:
                                        p2['away_team'] = p2[acol]
                                    keep = ['game_id','home_team','away_team']
                                    df = df.merge(p2[keep], on='game_id', how='left', suffixes=('', '_pd2'))
                                    nm_h = numeric_mask & (df['home_team'].isna() | (df['home_team'].astype(str).str.strip()==''))
                                    nm_a = numeric_mask & (df['away_team'].isna() | (df['away_team'].astype(str).str.strip()==''))
                                    if nm_h.any():
                                        df.loc[nm_h, 'home_team'] = df.loc[nm_h, 'home_team_pd2']
                                    if nm_a.any():
                                        df.loc[nm_a, 'away_team'] = df.loc[nm_a, 'away_team_pd2']
                                    for c in ('home_team_pd2','away_team_pd2'):
                                        if c in df.columns:
                                            try:
                                                df.drop(columns=[c], inplace=True)
                                            except Exception:
                                                pass
                except Exception:
                    pass
                if mask_bad.any():
                    # Defer dropping until after historical backfill block executes
                    try:
                        pipeline_stats['pre_hist_blank_team_rows'] = int(mask_bad.sum())
                    except Exception:
                        pass
    except Exception:
        pass

    # Historical team-name backfill: use recent results and outputs artifacts to fill blanks for numeric ESPN game_ids
    try:
        if isinstance(df, pd.DataFrame) and 'game_id' in df.columns and {'home_team','away_team'}.issubset(df.columns):
            df['game_id'] = df['game_id'].astype(str)
            ht_blank = df['home_team'].isna() | (df['home_team'].astype(str).str.strip() == '')
            at_blank = df['away_team'].isna() | (df['away_team'].astype(str).str.strip() == '')
            need_fill = (ht_blank | at_blank)
            if need_fill.any():
                gid_to_home = {}
                gid_to_away = {}
                # Pull from recent daily_results
                try:
                    hist_dir = OUT / 'daily_results'
                    if hist_dir.exists():
                        res_files = sorted(hist_dir.glob('results_*.csv'))[-60:]
                        for rp in reversed(res_files):
                            try:
                                rdf = pd.read_csv(rp)
                            except Exception:
                                continue
                            if rdf.empty or 'game_id' not in rdf.columns:
                                continue
                            if 'home_team' not in rdf.columns:
                                rdf['home_team'] = None
                            if 'away_team' not in rdf.columns:
                                rdf['away_team'] = None
                            for _, r in rdf.iterrows():
                                gid = str(r.get('game_id'))
                                if gid and gid not in ('', 'nan', 'None'):
                                    ht = r.get('home_team')
                                    at = r.get('away_team')
                                    if ht and str(ht).strip():
                                        gid_to_home.setdefault(gid, str(ht).strip())
                                    if at and str(at).strip():
                                        gid_to_away.setdefault(gid, str(at).strip())
                except Exception:
                    pass
                # Pull from outputs align_period files
                try:
                    out_dir = OUT
                    if out_dir.exists():
                        game_files = sorted(out_dir.glob('align_period_*_edges.csv'))[-40:] + sorted(out_dir.glob('align_period_*.csv'))[-40:]
                        for gp in reversed(game_files):
                            try:
                                gdf = pd.read_csv(gp)
                            except Exception:
                                continue
                            if gdf.empty or 'game_id' not in gdf.columns:
                                continue
                            if 'home_team' not in gdf.columns:
                                gdf['home_team'] = None
                            if 'away_team' not in gdf.columns:
                                gdf['away_team'] = None
                            for _, r in gdf.iterrows():
                                gid = str(r.get('game_id'))
                                if gid and gid not in ('', 'nan', 'None'):
                                    ht = r.get('home_team')
                                    at = r.get('away_team')
                                    if ht and str(ht).strip():
                                        gid_to_home.setdefault(gid, str(ht).strip())
                                    if at and str(at).strip():
                                        gid_to_away.setdefault(gid, str(at).strip())
                except Exception:
                    pass
                # Pull from odds and predictions artifacts (today/unified) keyed by game_id
                try:
                    # Odds
                    for name in ('odds_today.csv','odds_last.csv','odds_unified.csv'):
                        p = OUT / name
                        if p.exists():
                            try:
                                od = pd.read_csv(p)
                            except Exception:
                                continue
                            if not od.empty and 'game_id' in od.columns:
                                if 'home_team' not in od.columns:
                                    od['home_team'] = None
                                if 'away_team' not in od.columns:
                                    od['away_team'] = None
                                for _, r in od.iterrows():
                                    gid = str(r.get('game_id'))
                                    if gid and gid not in ('', 'nan', 'None'):
                                        ht = r.get('home_team')
                                        at = r.get('away_team')
                                        if ht and str(ht).strip():
                                            gid_to_home.setdefault(gid, str(ht).strip())
                                        if at and str(at).strip():
                                            gid_to_away.setdefault(gid, str(at).strip())
                    # Predictions unified
                    for name in ('predictions_unified_today.csv','predictions_unified.csv'):
                        p = OUT / name
                        if p.exists():
                            try:
                                pdx = pd.read_csv(p)
                            except Exception:
                                continue
                            if not pdx.empty and 'game_id' in pdx.columns:
                                if 'home_team' not in pdx.columns:
                                    pdx['home_team'] = None
                                if 'away_team' not in pdx.columns:
                                    pdx['away_team'] = None
                                for _, r in pdx.iterrows():
                                    gid = str(r.get('game_id'))
                                    if gid and gid not in ('', 'nan', 'None'):
                                        ht = r.get('home_team')
                                        at = r.get('away_team')
                                        if ht and str(ht).strip():
                                            gid_to_home.setdefault(gid, str(ht).strip())
                                        if at and str(at).strip():
                                            gid_to_away.setdefault(gid, str(at).strip())
                    # Games with last/closing (per-date preferred)
                    try:
                        date_str = str(date_q) if 'date_q' in locals() else None
                    except Exception:
                        date_str = None
                    candidates = []
                    if date_str:
                        candidates += [OUT / f'games_with_last_{date_str}.csv', OUT / f'games_with_closing_{date_str}.csv']
                    candidates += [OUT / 'games_with_last.csv', OUT / 'games_with_closing.csv']
                    for gp in candidates:
                        if gp and gp.exists():
                            try:
                                gdf = pd.read_csv(gp)
                            except Exception:
                                continue
                            if not gdf.empty and 'game_id' in gdf.columns:
                                if 'home_team' not in gdf.columns:
                                    gdf['home_team'] = gdf.get('home') if 'home' in gdf.columns else None
                                if 'away_team' not in gdf.columns:
                                    gdf['away_team'] = gdf.get('away') if 'away' in gdf.columns else None
                                for _, r in gdf.iterrows():
                                    gid = str(r.get('game_id'))
                                    if gid and gid not in ('', 'nan', 'None'):
                                        ht = r.get('home_team')
                                        at = r.get('away_team')
                                        if ht and str(ht).strip():
                                            gid_to_home.setdefault(gid, str(ht).strip())
                                        if at and str(at).strip():
                                            gid_to_away.setdefault(gid, str(at).strip())
                except Exception:
                    pass
                # Apply fills (try exact match, then numeric-only gid fallback)
                if gid_to_home or gid_to_away:
                    fill_mask = need_fill
                    if fill_mask.any():
                        try:
                            # First pass: exact game_id match
                            df.loc[fill_mask & ht_blank, 'home_team'] = df.loc[fill_mask & ht_blank, 'game_id'].map(gid_to_home).where(
                                df.loc[fill_mask & ht_blank, 'home_team'].isna() | (df.loc[fill_mask & ht_blank, 'home_team'].astype(str).str.strip() == '')
                            )
                            df.loc[fill_mask & at_blank, 'away_team'] = df.loc[fill_mask & at_blank, 'game_id'].map(gid_to_away).where(
                                df.loc[fill_mask & at_blank, 'away_team'].isna() | (df.loc[fill_mask & at_blank, 'away_team'].astype(str).str.strip() == '')
                            )
                            # Second pass: numeric-only id mapping to handle prefix/suffix differences
                            def _digits(s: str) -> str:
                                return ''.join(ch for ch in str(s) if ch.isdigit()) if s is not None else ''
                            gid_to_home_num = { _digits(k): v for k, v in gid_to_home.items() }
                            gid_to_away_num = { _digits(k): v for k, v in gid_to_away.items() }
                            gnum = df['game_id'].astype(str).map(_digits)
                            need_ht = fill_mask & (df['home_team'].isna() | (df['home_team'].astype(str).str.strip() == ''))
                            need_at = fill_mask & (df['away_team'].isna() | (df['away_team'].astype(str).str.strip() == ''))
                            if need_ht.any() and gid_to_home_num:
                                df.loc[need_ht, 'home_team'] = gnum.map(gid_to_home_num)
                            if need_at.any() and gid_to_away_num:
                                df.loc[need_at, 'away_team'] = gnum.map(gid_to_away_num)
                        except Exception:
                            pass
                        try:
                            pipeline_stats['historical_team_fill_applied'] = int((df['home_team'].notna() & df['home_team'].astype(str).str.strip().ne('') & fill_mask).sum() + (df['away_team'].notna() & df['away_team'].astype(str).str.strip().ne('') & fill_mask).sum())
                        except Exception:
                            pass
        # Final drop of any rows still missing team names after historical attempts
        try:
            if isinstance(df, pd.DataFrame) and {'home_team','away_team'}.issubset(df.columns):
                post_mask = df['home_team'].isna() | (df['home_team'].astype(str).str.strip()=='') | df['away_team'].isna() | (df['away_team'].astype(str).str.strip()=='')
                if post_mask.any():
                    try:
                        pipeline_stats['post_hist_blank_team_rows'] = int(post_mask.sum())
                    except Exception:
                        pass
                    df = df[~post_mask].copy()
                # Persist enriched unified artifact (team names guaranteed) for the target date
                try:
                    # Determine target date string: prefer explicit query param, else infer from start_time or today
                    t_date = None
                    if 'date_q' in locals() and date_q:
                        t_date = str(date_q)
                    if not t_date:
                        # Try start_time column
                        if 'start_time' in df.columns:
                            try:
                                st = pd.to_datetime(df['start_time'], errors='coerce')
                                if st.notna().any():
                                    t_date = st.dt.strftime('%Y-%m-%d').mode().iloc[0]
                            except Exception:
                                t_date = None
                    if not t_date:
                        from datetime import datetime as _dt
                        t_date = _dt.now().strftime('%Y-%m-%d')
                    # Filter to scheduled game_ids for that date to avoid cross-date leakage
                    sched_path = OUT / 'games_curr.csv'
                    if sched_path.exists():
                        try:
                            sched_df = pd.read_csv(sched_path)
                            if not sched_df.empty and 'game_id' in sched_df.columns:
                                if 'date' in sched_df.columns:
                                    try:
                                        sched_df['date'] = pd.to_datetime(sched_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                                        sched_df = sched_df[sched_df['date'] == t_date]
                                    except Exception:
                                        pass
                                valid_ids = set(sched_df['game_id'].astype(str))
                                if valid_ids:
                                    df = df[df['game_id'].astype(str).isin(valid_ids)].copy()
                        except Exception:
                            pass
                    # Write artifact
                    enriched_path = OUT / f'predictions_unified_enriched_{t_date}.csv'
                    try:
                        # Drop placeholder TBD/TBA rows before initial write
                        if not df.empty and {'home_team','away_team'}.issubset(df.columns):
                            _bads = {"tbd","t.b.d","tba","t.b.a","to be determined","to-be-determined","to be announced","to-be-announced","unknown","na","n/a","","none","null"}
                            _htl = df['home_team'].astype(str).str.strip().str.lower()
                            _atl = df['away_team'].astype(str).str.strip().str.lower()
                            _mb = _htl.isin(_bads) | _atl.isin(_bads)
                            if _mb.any():
                                pipeline_stats['enriched_unified_initial_placeholders_dropped'] = int(_mb.sum())
                                df = df[~_mb].copy()
                        df.to_csv(enriched_path, index=False)
                        pipeline_stats['enriched_unified_written'] = str(enriched_path.name)
                        pipeline_stats['enriched_unified_rows'] = int(len(df))
                        # Post-process to ensure coverage (preds/time/display)
                        try:
                            ok_pp = _postprocess_enriched_file(t_date)
                            pipeline_stats['enriched_unified_postprocessed'] = bool(ok_pp)
                        except Exception:
                            pipeline_stats['enriched_unified_postprocessed'] = False
                    except Exception:
                        pipeline_stats['enriched_unified_write_error'] = True
                except Exception:
                    pipeline_stats['enriched_unified_error'] = True
        except Exception:
            pass
    except Exception:
        pass

    # Compute ATS and ML helpers if odds present
    try:
        if "spread_home" in df.columns and "pred_margin" in df.columns:
            # ATS edge (full game): predicted home spread minus market spread
            df["edge_ats"] = -pd.to_numeric(df["pred_margin"], errors="coerce") - pd.to_numeric(df["spread_home"], errors="coerce")
        if "spread_home_1h" in df.columns and "pred_margin_1h" in df.columns:
            # ATS edge (1H): predicted home spread minus market spread
            df["edge_ats_1h"] = -pd.to_numeric(df["pred_margin_1h"], errors="coerce") - pd.to_numeric(df["spread_home_1h"], errors="coerce")
        if "market_total_1h" in df.columns and "pred_total_1h" in df.columns:
            df["edge_total_1h"] = pd.to_numeric(df["pred_total_1h"], errors="coerce") - pd.to_numeric(df["market_total_1h"], errors="coerce")
        if "market_total_2h" in df.columns and "pred_total_2h" in df.columns:
            df["edge_total_2h"] = pd.to_numeric(df["pred_total_2h"], errors="coerce") - pd.to_numeric(df["market_total_2h"], errors="coerce")
        if "spread_home_2h" in df.columns and "pred_margin_2h" in df.columns:
            # ATS edge (2H): predicted home spread minus market spread
            df["edge_ats_2h"] = -pd.to_numeric(df["pred_margin_2h"], errors="coerce") - pd.to_numeric(df["spread_home_2h"], errors="coerce")
        # Derivative leans for 1H/2H totals and ATS
        if "edge_total_1h" in df.columns:
            et1 = pd.to_numeric(df["edge_total_1h"], errors="coerce")
            df["lean_ou_side_1h"] = np.where(et1 > 0, "Over", np.where(et1 < 0, "Under", None))
            df["lean_ou_edge_abs_1h"] = et1.abs()
        if "edge_total_2h" in df.columns:
            et2 = pd.to_numeric(df["edge_total_2h"], errors="coerce")
            df["lean_ou_side_2h"] = np.where(et2 > 0, "Over", np.where(et2 < 0, "Under", None))
            df["lean_ou_edge_abs_2h"] = et2.abs()
        if "edge_ats_1h" in df.columns:
            ea1 = pd.to_numeric(df["edge_ats_1h"], errors="coerce")
            df["lean_ats_side_1h"] = np.where(ea1 > 0, "Home ATS", np.where(ea1 < 0, "Away ATS", None))
            df["lean_ats_edge_abs_1h"] = ea1.abs()
        if "edge_ats_2h" in df.columns:
            ea2 = pd.to_numeric(df["edge_ats_2h"], errors="coerce")
            df["lean_ats_side_2h"] = np.where(ea2 > 0, "Home ATS", np.where(ea2 < 0, "Away ATS", None))
            df["lean_ats_edge_abs_2h"] = ea2.abs()
        if "ml_home" in df.columns and "pred_margin" in df.columns:
            scale = 7.0
            pm = pd.to_numeric(df["pred_margin"], errors="coerce")
            df["ml_prob_model"] = 1.0 / (1.0 + np.exp(-pm / scale))
            # American odds to implied probability for home side
            def _imp_p(price):
                try:
                    if pd.isna(price):
                        return np.nan
                    price = float(price)
                    return (abs(price) / (abs(price) + 100.0)) if price > 0 else (100.0 / (abs(price) + 100.0))
                except Exception:
                    return np.nan
            df["ml_prob_implied"] = df["ml_home"].map(_imp_p)
            if "ml_prob_implied" in df.columns:
                df["ml_prob_edge"] = df["ml_prob_model"] - df["ml_prob_implied"]
        # Margin direction labels and lean signals
        if "pred_margin" in df.columns:
            pm = pd.to_numeric(df["pred_margin"], errors="coerce")
            df["favored_side"] = np.where(pm > 0, "Home", np.where(pm < 0, "Away", "Even"))
            df["favored_by"] = pm.abs()
        # Over/Under lean based on edge_total (requires market_total)
        if "edge_total" in df.columns:
            et = pd.to_numeric(df["edge_total"], errors="coerce")
            df["lean_ou_side"] = np.where(et > 0, "Over", np.where(et < 0, "Under", None))
            df["lean_ou_edge_abs"] = et.abs()
        # ATS lean based on edge_ats and spread_home
        if "edge_ats" in df.columns and "spread_home" in df.columns:
            ea = pd.to_numeric(df["edge_ats"], errors="coerce")
            df["lean_ats_side"] = np.where(ea > 0, "Home ATS", np.where(ea < 0, "Away ATS", None))
            df["lean_ats_edge_abs"] = ea.abs()
        # Edge vs closing spread if available
        if {"closing_spread_home","pred_margin"}.issubset(df.columns):
            try:
                df["edge_closing_ats"] = pd.to_numeric(df["pred_margin"], errors="coerce") - pd.to_numeric(df["closing_spread_home"], errors="coerce")
            except Exception:
                df["edge_closing_ats"] = None
    except Exception:
        pass

    # Attach picks (per-game) for picks strip display
    top_picks: list[dict[str, Any]] = []
    try:
        # Prefer expanded picks_raw for richer context; fallback to clean picks
        raw_path = OUT / "picks_raw.csv"
        picks_df = pd.read_csv(raw_path) if raw_path.exists() else _load_picks()
        picks_map: dict[str, list[dict[str, Any]]] = {}
        if not picks_df.empty and "game_id" in picks_df.columns:
            picks_df["game_id"] = picks_df["game_id"].astype(str)
            if date_q and "date" in picks_df.columns:
                try:
                    picks_df["date"] = pd.to_datetime(picks_df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                    picks_df = picks_df[picks_df["date"] == date_q]
                except Exception:
                    pass
            # ----------------------------------------------------------
            # Augment picks with model uncertainty & adjusted Kelly if available
            # Adds columns: total_sigma, margin_sigma, kelly_total_adj, kelly_margin_adj
            # Persists back to picks_raw.csv if new columns introduced.
            # ----------------------------------------------------------
            try:
                if not df.empty and 'game_id' in df.columns:
                    gmap_sigma_t = df.set_index('game_id')['pred_total_sigma'] if 'pred_total_sigma' in df.columns else None
                    gmap_sigma_m = df.set_index('game_id')['pred_margin_sigma'] if 'pred_margin_sigma' in df.columns else None
                    gmap_sigma_t_boot = df.set_index('game_id')['pred_total_sigma_bootstrap'] if 'pred_total_sigma_bootstrap' in df.columns else None
                    gmap_sigma_m_boot = df.set_index('game_id')['pred_margin_sigma_bootstrap'] if 'pred_margin_sigma_bootstrap' in df.columns else None
                    gmap_kelly_t_adj = df.set_index('game_id')['kelly_fraction_total_adj'] if 'kelly_fraction_total_adj' in df.columns else None
                    gmap_kelly_m_adj = df.set_index('game_id')['kelly_fraction_margin_adj'] if 'kelly_fraction_margin_adj' in df.columns else None
                    if gmap_sigma_t is not None and 'total_sigma' not in picks_df.columns:
                        picks_df['total_sigma'] = picks_df['game_id'].map(gmap_sigma_t)
                    if gmap_sigma_m is not None and 'margin_sigma' not in picks_df.columns:
                        picks_df['margin_sigma'] = picks_df['game_id'].map(gmap_sigma_m)
                    if gmap_sigma_t_boot is not None and 'total_sigma_bootstrap' not in picks_df.columns:
                        picks_df['total_sigma_bootstrap'] = picks_df['game_id'].map(gmap_sigma_t_boot)
                    if gmap_sigma_m_boot is not None and 'margin_sigma_bootstrap' not in picks_df.columns:
                        picks_df['margin_sigma_bootstrap'] = picks_df['game_id'].map(gmap_sigma_m_boot)
                    if gmap_kelly_t_adj is not None and 'kelly_total_adj' not in picks_df.columns:
                        picks_df['kelly_total_adj'] = picks_df['game_id'].map(gmap_kelly_t_adj)
                    if gmap_kelly_m_adj is not None and 'kelly_margin_adj' not in picks_df.columns:
                        picks_df['kelly_margin_adj'] = picks_df['game_id'].map(gmap_kelly_m_adj)
                    # Persist enriched picks back to disk (best-effort)
                    try:
                        raw_path_out = OUT / 'picks_raw.csv'
                        picks_df.to_csv(raw_path_out, index=False)
                        pipeline_stats['picks_raw_enriched'] = True
                    except Exception:
                        pipeline_stats['picks_raw_enriched'] = False
            except Exception:
                pipeline_stats['picks_raw_enriched_error'] = True
            # Normalize columns for market/selection/line/edge
            def pick_col(df_, names):
                return next((c for c in names if c in df_.columns), None)
            col_market = pick_col(picks_df, ["market","bet_type","type"]) or "market"
            col_sel = pick_col(picks_df, ["selection","pick","bet","side","team"]) or "selection"
            col_edge = pick_col(picks_df, ["edge","abs_edge","edge_total","edge_ats"]) or "edge"
            potential_line_cols = [c for c in ["line","line_value","total","home_spread","spread_home","pick_line"] if c in picks_df.columns]
            col_price = pick_col(picks_df, ["price","odds","american_odds"]) or None
            # Coerce edge to numeric and build abs edge
            if col_edge in picks_df.columns:
                picks_df["_edge_val"] = pd.to_numeric(picks_df[col_edge], errors="coerce")
                picks_df["_abs_edge"] = picks_df["_edge_val"].abs()
            else:
                picks_df["_edge_val"] = np.nan
                picks_df["_abs_edge"] = np.nan
            # Build per-game pick lists (dedupe by selection/market/line and cap to top 3)
            for gid, grp in picks_df.groupby("game_id"):
                g = grp.copy()
                if "_abs_edge" in g.columns:
                    g = g.sort_values(["_abs_edge"], ascending=[False])
                # Deduplicate by market+selection+line to avoid many books duplicates
                dedup_cols = [c for c in [col_market, col_sel] if c in g.columns] + (potential_line_cols[:1] or [])
                if dedup_cols:
                    g = g.drop_duplicates(subset=dedup_cols, keep="first")
                items: list[dict[str, Any]] = []
                for _, pr in g.head(3).iterrows():
                    item: dict[str, Any] = {"game_id": str(gid)}
                    item["market"] = pr.get(col_market)
                    item["selection"] = pr.get(col_sel)
                    # Prefer absolute edge for display clarity
                    try:
                        item["edge"] = float(abs(float(pr.get(col_edge)))) if pd.notna(pr.get(col_edge)) else None
                    except Exception:
                        item["edge"] = pr.get(col_edge)
                    # Line detection
                    line_val = None
                    for lc in potential_line_cols:
                        if lc in g.columns and pd.notna(pr.get(lc)):
                            line_val = pr.get(lc)
                            break
                    if line_val is not None:
                        item["line"] = line_val
                    if col_price and col_price in g.columns:
                        item["price"] = pr.get(col_price)
                    items.append(item)
                if items:
                    picks_map[str(gid)] = items
            # Build global top picks: top 12 unique games by abs edge
            try:
                pf = picks_df.copy()
                pf = pf.sort_values(["_abs_edge"], ascending=[False]) if "_abs_edge" in pf.columns else pf
                teams_map: dict[str, tuple[Any, Any]] = {}
                if {"game_id","home_team","away_team"}.issubset(df.columns):
                    for _, rr in df.iterrows():
                        teams_map[str(rr.get("game_id"))] = (rr.get("home_team"), rr.get("away_team"))
                # Keep one best pick per game
                if "_abs_edge" in pf.columns:
                    pf = pf.sort_values(["game_id","_abs_edge"], ascending=[True, False]).drop_duplicates(subset=["game_id"], keep="first")
                top_rows: list[dict[str, Any]] = []
                for _, pr in pf.head(12).iterrows():
                    gid = str(pr.get("game_id"))
                    h, a = teams_map.get(gid, (None, None))
                    item: dict[str, Any] = {"game_id": gid, "home_team": h, "away_team": a}
                    item["market"] = pr.get(col_market)
                    item["selection"] = pr.get(col_sel)
                    try:
                        val = pr.get(col_edge)
                        item["edge"] = float(abs(float(val))) if pd.notna(val) else None
                    except Exception:
                        item["edge"] = pr.get(col_edge)
                    line_val = None
                    for lc in potential_line_cols:
                        v = pr.get(lc)
                        if pd.notna(v):
                            line_val = v
                            break
                    if line_val is not None:
                        item["line"] = line_val
                    if col_price:
                        item["price"] = pr.get(col_price)
                    top_rows.append(item)
                top_picks = top_rows
            except Exception:
                top_picks = []
        df["_picks_list"] = df["game_id"].map(lambda x: picks_map.get(str(x), [])) if picks_map else [[] for _ in range(len(df))]
    except Exception as _ep:
        df["_picks_list"] = [[] for _ in range(len(df))]

    # Order by start_time if available; fallback to date then home_team/game_id; else by abs edge
    # ------------------------------------------------------------------
    # Coverage supplementation: append any games or model prediction rows
    # that are missing after daily_df or merge logic. This addresses cases
    # where a game (e.g., Marshall vs Arkansas-Pine Bluff) exists in
    # games_curr.csv and predictions_model_<date>.csv but is absent from
    # the unified display DataFrame due to earlier branch selection.
    # ------------------------------------------------------------------
    try:
        # Ensure df exists and has a game_id column
        if isinstance(df, pd.DataFrame) and 'game_id' in df.columns:
            existing_ids = set(df['game_id'].astype(str))
            # Build a robust schedule DataFrame for the target date using multiple fallbacks
            games_curr_path = OUT / 'games_curr.csv'
            sched_df = _safe_read_csv(games_curr_path) if games_curr_path.exists() else pd.DataFrame()
            try:
                # Append additional candidates if available (date-specific and season-wide)
                cand_list = []
                if date_q:
                    dstr = str(date_q)
                    try:
                        year = str(pd.to_datetime(dstr, errors='coerce').year)
                    except Exception:
                        year = None
                    cand_list.extend([
                        OUT / f"games_{dstr}.csv",
                        OUT / f"games_{dstr}_fused.csv",
                    ])
                    if year:
                        cand_list.extend([
                            OUT / f"games_{year}.csv",
                            OUT / f"games_{year}_fused.csv",
                        ])
                # Load and concat unique schedule rows
                extra = []
                for pth in cand_list:
                    try:
                        if pth.exists():
                            tmp = _safe_read_csv(pth)
                            if not tmp.empty:
                                extra.append(tmp)
                    except Exception:
                        continue
                if extra:
                    try:
                        sched_df = pd.concat([sched_df] + extra, ignore_index=True)
                    except Exception:
                        pass
            except Exception:
                pass
            # Normalize and filter schedule by date when possible
            if not sched_df.empty and 'game_id' in sched_df.columns:
                try:
                    sched_df['game_id'] = sched_df['game_id'].astype(str)
                except Exception:
                    pass
                if date_q and 'date' in sched_df.columns:
                    try:
                        sched_df['date'] = pd.to_datetime(sched_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                        # Primary filter by explicit date column
                        sched_by_date = sched_df[sched_df['date'] == str(date_q)]
                        # Supplemental filter by commence_time in schedule tz when available
                        try:
                            import os
                            from zoneinfo import ZoneInfo  # py>=3.9
                            tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                            try:
                                sched_tz = ZoneInfo(tz_name)
                            except Exception:
                                sched_tz = None
                        except Exception:
                            sched_tz = None
                        sched_by_commence = pd.DataFrame()
                        if 'commence_time' in sched_df.columns and sched_tz is not None:
                            try:
                                ct = pd.to_datetime(sched_df['commence_time'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                                ct_local = ct.dt.tz_convert(sched_tz)
                                sched_df['_commence_slate'] = ct_local.dt.strftime('%Y-%m-%d')
                                sched_by_commence = sched_df[sched_df['_commence_slate'] == str(date_q)]
                            except Exception:
                                sched_by_commence = pd.DataFrame()
                        try:
                            if not sched_by_commence.empty:
                                sched_df = pd.concat([sched_by_date, sched_by_commence], ignore_index=True).drop_duplicates(subset=['game_id'], keep='last')
                            else:
                                sched_df = sched_by_date
                        except Exception:
                            sched_df = sched_by_date
                    except Exception:
                        pass
                # Drop duplicates to avoid inflating later concatenations
                try:
                    sched_df = sched_df.drop_duplicates(subset=['game_id'], keep='last')
                except Exception:
                    pass
                # Identify missing schedule rows vs existing df
                missing_game_ids = [gid for gid in sched_df['game_id'] if gid not in existing_ids]
                if missing_game_ids:
                    add_games = sched_df[sched_df['game_id'].isin(missing_game_ids)].copy()
                    # Guarantee prediction placeholder columns
                    for col in ('pred_total','pred_margin'):
                        if col not in add_games.columns:
                            add_games[col] = np.nan
                    df = pd.concat([df, add_games], ignore_index=True)
                    pipeline_stats['appended_missing_games_rows'] = int(len(add_games))
                    existing_ids.update(missing_game_ids)
            # Add missing model predictions as rows
            if 'model_preds' in locals() and isinstance(model_preds, pd.DataFrame) and not model_preds.empty and 'game_id' in model_preds.columns:
                mp = model_preds.copy()
                mp['game_id'] = mp['game_id'].astype(str)
                missing_model_ids = [gid for gid in mp['game_id'] if gid not in existing_ids]
                if missing_model_ids:
                    add_mp = mp[mp['game_id'].isin(missing_model_ids)].copy()
                    # Enrich with schedule metadata if available
                    if not sched_df.empty and 'game_id' in sched_df.columns:
                        enrich_cols = [c for c in ['game_id','date','home_team','away_team','start_time','venue'] if c in sched_df.columns]
                        if enrich_cols:
                            add_mp = add_mp.merge(sched_df[enrich_cols], on='game_id', how='left')
                    # Promote model predictions
                    if 'pred_total_model' in add_mp.columns and 'pred_total' not in add_mp.columns:
                        add_mp['pred_total'] = add_mp['pred_total_model']
                        add_mp['pred_total_basis'] = add_mp.get('pred_total_model_basis','model_raw')
                    if 'pred_margin_model' in add_mp.columns and 'pred_margin' not in add_mp.columns:
                        add_mp['pred_margin'] = add_mp['pred_margin_model']
                        add_mp['pred_margin_basis'] = add_mp.get('pred_margin_model_basis','model')
                    df = pd.concat([df, add_mp], ignore_index=True)
                    pipeline_stats['appended_missing_model_rows'] = int(len(add_mp)) + int(pipeline_stats.get('appended_missing_model_rows', 0))
            # Explicit safeguard: if specific known game_id (Marshall vs Arkansas-Pine Bluff) still missing, force add from sources
            target_ids = []
            try:
                # Try to detect from todays games_curr if no date filter (or matches date_q)
                if not date_q or date_q == today_str:
                    # Hard-coded ID observed earlier
                    target_ids.append('401827130')
            except Exception:
                pass
            for tgt in target_ids:
                if tgt not in set(df['game_id'].astype(str)):
                    # Attempt to build a single-row DataFrame from games_curr + model_preds
                    row_parts = []
                    if not sched_df.empty and tgt in set(sched_df['game_id']):
                        row_parts.append(sched_df[sched_df['game_id'] == tgt])
                    if 'model_preds' in locals() and not model_preds.empty:
                        mp_row = model_preds[model_preds['game_id'].astype(str) == tgt]
                        if not mp_row.empty:
                            row_parts.append(mp_row)
                    if row_parts:
                        force_row = row_parts[0].copy()
                        # Add model columns if second part
                        if len(row_parts) > 1:
                            for _, r in row_parts[1].iterrows():
                                for c, v in r.items():
                                    if c not in force_row.columns:
                                        force_row[c] = v
                        # Promote prediction fields
                        if 'pred_total_model' in force_row.columns and 'pred_total' not in force_row.columns:
                            force_row['pred_total'] = force_row['pred_total_model']
                            force_row['pred_total_basis'] = force_row.get('pred_total_model_basis','model_raw')
                        if 'pred_margin_model' in force_row.columns and 'pred_margin' not in force_row.columns:
                            force_row['pred_margin'] = force_row['pred_margin_model']
                            force_row['pred_margin_basis'] = force_row.get('pred_margin_model_basis','model')
                        df = pd.concat([df, force_row], ignore_index=True)
                        pipeline_stats['forced_game_insertion'] = pipeline_stats.get('forced_game_insertion', []) + [tgt]
            # Targeted on-demand supplementation: if a known D1 vs D1 matchup is reported missing locally,
            # and not present in schedule/odds/model merges, synthesize a minimal row so it renders.
            try:
                # Saint Francis (PA) vs Troy requested for today's slate
                target_pairs = [
                    ("Troy Trojans", "Saint Francis (PA) Red Flash"),
                    ("Towson Tigers", "UC San Diego Tritons"),
                    ("Syracuse Orange", "Iowa State Cyclones"),
                ]
                # Determine slate date string
                try:
                    slate_date_str = str(date_q) if date_q else _today_local().strftime('%Y-%m-%d')
                    slate_year = pd.to_datetime(slate_date_str, errors='coerce').year
                except Exception:
                    slate_date_str = None
                    slate_year = None
                if slate_date_str:
                    # Build existing unordered pair keys for this df (if home/away present)
                    existing_pairs = set()
                    try:
                        if {'home_team','away_team'}.issubset(df.columns):
                            existing_pairs = set(
                                "::".join(sorted([_canon_slug(str(r.get('home_team'))), _canon_slug(str(r.get('away_team')))]))
                                for _, r in df.iterrows()
                            )
                    except Exception:
                        existing_pairs = set()
                    for home_name, away_name in target_pairs:
                        try:
                            pkey = "::".join(sorted([_canon_slug(home_name), _canon_slug(away_name)]))
                            if pkey not in existing_pairs:
                                # Inject a minimal, deterministic row
                                gid = f"synthetic:{pkey}@{slate_date_str}"
                                row = {
                                    'game_id': gid,
                                    'season': int(slate_year) if pd.notna(slate_year) else None,
                                    'date': slate_date_str,
                                    'start_time': None,
                                    'home_team': home_name,
                                    'away_team': away_name,
                                    'home_score': 0,
                                    'away_score': 0,
                                    'neutral_site': False,
                                    'venue': None,
                                    'pred_total': np.nan,
                                    'pred_margin': np.nan,
                                }
                                df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)
                                # Track injected pair so we don't add twice if code re-enters
                                existing_pairs.add(pkey)
                                pipeline_stats.setdefault('manual_pair_inserts', []).append(gid)
                        except Exception:
                            continue
            except Exception:
                pipeline_stats['manual_pair_insert_error'] = True
            pipeline_stats['post_coverage_rows'] = int(len(df))
            pipeline_stats['post_coverage_unique_games'] = int(df['game_id'].nunique())
            # Half prediction fallback derivation (1H/2H) from full-game projections – fill per-row where missing
            try:
                # De-fragment before a burst of sequential inserts to improve performance
                try:
                    df = df.copy()
                except Exception:
                    pass
                # Columns we will (create and) fill: pred_total_1h, pred_total_2h, pred_margin_1h, pred_margin_2h, proj_home_1h, proj_away_1h, proj_home_2h, proj_away_2h
                have_full_total = 'pred_total' in df.columns
                have_full_margin = 'pred_margin' in df.columns
                # Ensure target columns exist
                if 'pred_total_1h' not in df.columns:
                    df['pred_total_1h'] = np.nan
                if 'pred_total_1h_basis' not in df.columns:
                    df['pred_total_1h_basis'] = None
                if 'pred_total_2h' not in df.columns:
                    df['pred_total_2h'] = np.nan
                if 'pred_total_2h_basis' not in df.columns:
                    df['pred_total_2h_basis'] = None
                if 'pred_margin_1h' not in df.columns:
                    df['pred_margin_1h'] = np.nan
                if 'pred_margin_1h_basis' not in df.columns:
                    df['pred_margin_1h_basis'] = None
                if 'pred_margin_2h' not in df.columns:
                    df['pred_margin_2h'] = np.nan
                if 'pred_margin_2h_basis' not in df.columns:
                    df['pred_margin_2h_basis'] = None

                # Market half totals/spreads (may exist)
                mt1 = pd.to_numeric(df.get('market_total_1h'), errors='coerce') if 'market_total_1h' in df.columns else pd.Series(np.nan, index=df.index)
                mt2 = pd.to_numeric(df.get('market_total_2h'), errors='coerce') if 'market_total_2h' in df.columns else pd.Series(np.nan, index=df.index)
                sh1 = pd.to_numeric(df.get('spread_home_1h'), errors='coerce') if 'spread_home_1h' in df.columns else pd.Series(np.nan, index=df.index)
                sh2 = pd.to_numeric(df.get('spread_home_2h'), errors='coerce') if 'spread_home_2h' in df.columns else pd.Series(np.nan, index=df.index)
                pt_full = pd.to_numeric(df['pred_total'], errors='coerce') if have_full_total else pd.Series(np.nan, index=df.index)
                pm_full = pd.to_numeric(df['pred_margin'], errors='coerce') if have_full_margin else pd.Series(np.nan, index=df.index)

                # Stable half derivatives: use consistent factors across environments.
                # First half scoring tends to be slightly lower; use 0.485 of total as baseline.
                HALF_TOTAL_FACTOR = 0.485
                # Margin scales roughly linearly; use 0.50 for first half margin expectation.
                HALF_MARGIN_FACTOR = 0.50

                # 1H totals: fill only where missing
                mask_pt1h = pd.to_numeric(df['pred_total_1h'], errors='coerce').isna()
                if mask_pt1h.any():
                    vals_1h = []
                    bases_1h = []
                    for i in df.index[mask_pt1h]:
                        base = pt_full.loc[i]
                        if pd.notna(base):
                            v = base * HALF_TOTAL_FACTOR
                            if pd.notna(mt1.loc[i]):
                                v = 0.70 * v + 0.30 * float(mt1.loc[i])
                                b = 'blend_model_market'
                            else:
                                b = 'derived_full'
                            vals_1h.append(float(np.clip(v, 20, max(base - 10, 20))))
                            bases_1h.append(b)
                        else:
                            if pd.notna(mt1.loc[i]):
                                vals_1h.append(float(mt1.loc[i]))
                                bases_1h.append('market_copy')
                            else:
                                vals_1h.append(np.nan)
                                bases_1h.append(None)
                    df.loc[mask_pt1h, 'pred_total_1h'] = vals_1h
                    # Ensure basis column uses object dtype to accommodate string/None entries
                    if 'pred_total_1h_basis' in df.columns:
                        df['pred_total_1h_basis'] = df['pred_total_1h_basis'].astype('object')
                    df.loc[mask_pt1h, 'pred_total_1h_basis'] = pd.Series(bases_1h, index=df.index[mask_pt1h]).astype('object')

                # 2H totals: fill only where missing (remainder of full minus 1H)
                mask_pt2h = pd.to_numeric(df['pred_total_2h'], errors='coerce').isna()
                if mask_pt2h.any():
                    pt1_series = pd.to_numeric(df.get('pred_total_1h'), errors='coerce')
                    vals_2h = []
                    bases_2h = []
                    for i in df.index[mask_pt2h]:
                        full = pt_full.loc[i]
                        first = pt1_series.loc[i] if i in pt1_series.index else np.nan
                        if pd.notna(full) and pd.notna(first):
                            remain = full - first
                            if pd.notna(mt2.loc[i]):
                                v2 = 0.70 * remain + 0.30 * float(mt2.loc[i])
                                b2 = 'blend_model_market'
                            else:
                                v2 = remain
                                b2 = 'derived_remainder'
                            vals_2h.append(float(np.clip(v2, 20, max(full - 15, 20))))
                            bases_2h.append(b2)
                        else:
                            if pd.notna(mt2.loc[i]):
                                vals_2h.append(float(mt2.loc[i]))
                                bases_2h.append('market_copy')
                            else:
                                vals_2h.append(np.nan)
                                bases_2h.append(None)
                    df.loc[mask_pt2h, 'pred_total_2h'] = vals_2h
                    df.loc[mask_pt2h, 'pred_total_2h_basis'] = bases_2h

                # 1H margins: fill only where missing
                mask_pm1h = pd.to_numeric(df['pred_margin_1h'], errors='coerce').isna()
                if mask_pm1h.any():
                    vals_m1 = []
                    bases_m1 = []
                    for i in df.index[mask_pm1h]:
                        mfull = pm_full.loc[i]
                        if pd.notna(mfull):
                            vm = mfull * HALF_MARGIN_FACTOR
                            if pd.notna(sh1.loc[i]):
                                vm = 0.55 * vm + 0.45 * float(-sh1.loc[i])
                                bm = 'blend_model_spread'
                            else:
                                bm = 'derived_full'
                            vals_m1.append(float(np.clip(vm, -40, 40)))
                            bases_m1.append(bm)
                        else:
                            if pd.notna(sh1.loc[i]):
                                vals_m1.append(float(-sh1.loc[i]))
                                bases_m1.append('spread_copy')
                            else:
                                vals_m1.append(0.0)
                                bases_m1.append('even')
                    df.loc[mask_pm1h, 'pred_margin_1h'] = vals_m1
                    df.loc[mask_pm1h, 'pred_margin_1h_basis'] = bases_m1

                # 2H margins: fill only where missing
                mask_pm2h = pd.to_numeric(df['pred_margin_2h'], errors='coerce').isna()
                if mask_pm2h.any():
                    pm1_series = pd.to_numeric(df.get('pred_margin_1h'), errors='coerce')
                    vals_m2 = []
                    bases_m2 = []
                    for i in df.index[mask_pm2h]:
                        mfull = pm_full.loc[i]
                        m1 = pm1_series.loc[i] if i in pm1_series.index else np.nan
                        if pd.notna(mfull) and pd.notna(m1):
                            remain_m = mfull - m1
                            if pd.notna(sh2.loc[i]):
                                remain_m = 0.55 * remain_m + 0.45 * float(-sh2.loc[i])
                                bm2 = 'blend_model_spread'
                            else:
                                bm2 = 'derived_remainder'
                            vals_m2.append(float(np.clip(remain_m, -50, 50)))
                            bases_m2.append(bm2)
                        else:
                            if pd.notna(sh2.loc[i]):
                                vals_m2.append(float(-sh2.loc[i]))
                                bases_m2.append('spread_copy')
                            else:
                                vals_m2.append(0.0)
                                bases_m2.append('even')
                    df.loc[mask_pm2h, 'pred_margin_2h'] = vals_m2
                    df.loc[mask_pm2h, 'pred_margin_2h_basis'] = bases_m2
                # Team half projections from total + margin splits
                if {'proj_home','proj_away','pred_total','pred_margin'}.issubset(df.columns):
                    ph = pd.to_numeric(df['proj_home'], errors='coerce')
                    pa = pd.to_numeric(df['proj_away'], errors='coerce')
                    # Use ratio rather than simple constant factor for team halves preserving distribution
                    if 'pred_total_1h' in df.columns and 'pred_total_2h' in df.columns:
                        t1 = pd.to_numeric(df['pred_total_1h'], errors='coerce')
                        t2 = pd.to_numeric(df['pred_total_2h'], errors='coerce')
                        # Split team projections proportionally
                        # Ensure columns exist to allow selective filling
                        if 'proj_home_1h' not in df.columns: df['proj_home_1h'] = np.nan
                        if 'proj_away_1h' not in df.columns: df['proj_away_1h'] = np.nan
                        if 'proj_home_2h' not in df.columns: df['proj_home_2h'] = np.nan
                        if 'proj_away_2h' not in df.columns: df['proj_away_2h'] = np.nan
                        mask_ph1 = pd.to_numeric(df['proj_home_1h'], errors='coerce').isna()
                        mask_pa1 = pd.to_numeric(df['proj_away_1h'], errors='coerce').isna()
                        mask_ph2 = pd.to_numeric(df['proj_home_2h'], errors='coerce').isna()
                        mask_pa2 = pd.to_numeric(df['proj_away_2h'], errors='coerce').isna()
                        # Compute per-row only where missing
                        for i in df.index:
                            full_total = (ph.loc[i] + pa.loc[i]) if (i in ph.index and i in pa.index and pd.notna(ph.loc[i]) and pd.notna(pa.loc[i])) else np.nan
                            first_total = t1.loc[i] if i in t1.index else np.nan
                            second_total = t2.loc[i] if i in t2.index else np.nan
                            if pd.notna(full_total) and pd.notna(first_total) and full_total > 0:
                                off_share_home = float(ph.loc[i] / full_total) if pd.notna(ph.loc[i]) else 0.5
                                h1 = off_share_home * float(first_total)
                                a1 = (1.0 - off_share_home) * float(first_total)
                                if mask_ph1.loc[i]: df.at[i, 'proj_home_1h'] = h1
                                if mask_pa1.loc[i]: df.at[i, 'proj_away_1h'] = a1
                            if pd.notna(full_total) and pd.notna(second_total) and full_total > 0:
                                off_share_home = float(ph.loc[i] / full_total) if pd.notna(ph.loc[i]) else 0.5
                                h2 = off_share_home * float(second_total)
                                a2 = (1.0 - off_share_home) * float(second_total)
                                if mask_ph2.loc[i]: df.at[i, 'proj_home_2h'] = h2
                                if mask_pa2.loc[i]: df.at[i, 'proj_away_2h'] = a2
                # Instrumentation
                if 'pred_total_1h' in df.columns:
                    pipeline_stats['pred_total_1h_rows'] = int(df['pred_total_1h'].notna().sum())
                if 'pred_total_2h' in df.columns:
                    pipeline_stats['pred_total_2h_rows'] = int(df['pred_total_2h'].notna().sum())
                if 'pred_margin_1h' in df.columns:
                    pipeline_stats['pred_margin_1h_rows'] = int(df['pred_margin_1h'].notna().sum())
                if 'pred_margin_2h' in df.columns:
                    pipeline_stats['pred_margin_2h_rows'] = int(df['pred_margin_2h'].notna().sum())
                if 'proj_home_1h' in df.columns:
                    pipeline_stats['proj_home_1h_rows'] = int(df['proj_home_1h'].notna().sum())
                if 'proj_away_1h' in df.columns:
                    pipeline_stats['proj_away_1h_rows'] = int(df['proj_away_1h'].notna().sum())
                # Instrument stable half factors for diagnostics/alignment
                try:
                    pipeline_stats['half_factors'] = {'total': float(HALF_TOTAL_FACTOR), 'margin': float(HALF_MARGIN_FACTOR)}
                except Exception:
                    pass
            except Exception:
                pipeline_stats['half_pred_error'] = True
            # Second-pass synthetic + margin fill after coverage filtering: some earlier synthetic fills
            # may have been dropped if those games were excluded. Re-fill remaining NaNs so exports
            # reflect predictions for all covered games.
            try:
                if 'pred_total' in df.columns:
                    # Only fill rows that are truly missing and not already backed by calibrated/model/blended bases.
                    pt_series2 = pd.to_numeric(df['pred_total'], errors='coerce')
                    basis_series2 = df.get('pred_total_basis') if 'pred_total_basis' in df.columns else pd.Series([None]*len(df))
                    protected_bases = {'model_calibrated','model_raw','cal','blend_model_market','blended_model_baseline'}
                    # Eligible if value missing AND basis missing or basis is synthetic/derived/fallback/even
                    basis_str = basis_series2.astype(str).str.lower()
                    eligible_mask = pt_series2.isna() & (~basis_str.isin(protected_bases)) & (
                        basis_series2.isna() | basis_str.str.startswith(('synthetic','fallback','derived','even'))
                    )
                    if eligible_mask.any():
                        pipeline_stats['second_pass_pred_total_fill_candidates'] = int(eligible_mask.sum())
                        mt_series2 = pd.to_numeric(df.get('market_total'), errors='coerce') if 'market_total' in df.columns else pd.Series([np.nan]*len(df))
                        baseline_league_avg2 = 141.5  # simplified baseline (avoid heavy tempo logic duplication)
                        ht2 = pd.to_numeric(df.get('home_tempo_rating'), errors='coerce') if 'home_tempo_rating' in df.columns else pd.Series([np.nan]*len(df))
                        at2 = pd.to_numeric(df.get('away_tempo_rating'), errors='coerce') if 'away_tempo_rating' in df.columns else pd.Series([np.nan]*len(df))
                        tempo_avg2 = np.where(ht2.notna() & at2.notna(), (ht2+at2)/2.0, np.nan)
                        import zlib
                        def _noise2(h,a):
                            try:
                                return (((zlib.adler32(f"{h}::{a}".encode()) % 1000)/1000.0) - 0.5) * 3.4
                            except Exception:
                                return 0.0
                        applied = 0
                        for idx in df.index[eligible_mask]:
                            h = df.at[idx,'home_team'] if 'home_team' in df.columns else ''
                            a = df.at[idx,'away_team'] if 'away_team' in df.columns else ''
                            mt_val2 = mt_series2.loc[idx] if idx in mt_series2.index else np.nan
                            tempo_avg_val2 = tempo_avg2[idx] if not (isinstance(tempo_avg2, float) or pd.isna(tempo_avg2[idx])) else np.nan
                            tempo_component2 = ((tempo_avg_val2 - 70.0) * (0.55 if not pd.isna(tempo_avg_val2) else 0)) if not pd.isna(tempo_avg_val2) else 0.0
                            base_val = baseline_league_avg2 + tempo_component2 + _noise2(h,a)
                            if not pd.isna(mt_val2):
                                val2 = 0.55 * baseline_league_avg2 + 0.30 * float(mt_val2) + 0.15 * base_val
                            else:
                                val2 = base_val
                            # Second-pass synthetic baseline guardrail
                            pre_clip3 = val2
                            val2 = float(np.clip(val2, 110, 192))
                            if val2 > pre_clip3 and pre_clip3 < 110:
                                try:
                                    pipeline_stats.setdefault('low_total_guard_applied', 0)
                                    pipeline_stats['low_total_guard_applied'] += 1
                                except Exception:
                                    pass
                            df.at[idx,'pred_total'] = val2
                            if 'pred_total_basis' in df.columns and pd.isna(df.at[idx,'pred_total_basis']):
                                df.at[idx,'pred_total_basis'] = 'synthetic_baseline_final'
                            elif 'pred_total_basis' not in df.columns:
                                df.loc[idx,'pred_total_basis'] = 'synthetic_baseline_final'
                            applied += 1
                        pipeline_stats['second_pass_pred_total_fills'] = applied
                        pipeline_stats['second_pass_pred_total_fills_protected_skipped'] = int(pt_series2.isna().sum() - applied)
                if 'pred_margin' in df.columns:
                    pm_series2 = pd.to_numeric(df['pred_margin'], errors='coerce')
                    basis_margin2 = df.get('pred_margin_basis') if 'pred_margin_basis' in df.columns else pd.Series([None]*len(df))
                    protected_margin_bases = {'model_calibrated','model_raw','cal','model_margin_calibrated'}
                    basis_margin_str = basis_margin2.astype(str).str.lower()
                    pm_missing2 = pm_series2.isna() & (~basis_margin_str.isin(protected_margin_bases))
                    if pm_missing2.any():
                        pipeline_stats['second_pass_pred_margin_fill_candidates'] = int(pm_missing2.sum())
                        # Try spread-based first if available
                        if 'spread_home' in df.columns:
                            spread_series2 = pd.to_numeric(df['spread_home'], errors='coerce')
                            can_spread = pm_missing2 & spread_series2.notna()
                            applied_m_spread = 0
                            for idx in df.index[can_spread]:
                                df.at[idx,'pred_margin'] = float(-spread_series2.loc[idx])
                                if 'pred_margin_basis' in df.columns and pd.isna(df.at[idx,'pred_margin_basis']):
                                    df.at[idx,'pred_margin_basis'] = 'synthetic_from_spread_final'
                                elif 'pred_margin_basis' not in df.columns:
                                    df.loc[idx,'pred_margin_basis'] = 'synthetic_from_spread_final'
                                applied_m_spread += 1
                            pipeline_stats['second_pass_pred_margin_spread_fills'] = applied_m_spread
                        # Remaining -> even margin
                        pm_missing3 = pd.to_numeric(df['pred_margin'], errors='coerce').isna() & pm_missing2  # still candidates after spread attempt
                        if pm_missing3.any():
                            applied_m_even = 0
                            for idx in df.index[pm_missing3]:
                                df.at[idx,'pred_margin'] = 0.0
                                if 'pred_margin_basis' in df.columns and pd.isna(df.at[idx,'pred_margin_basis']):
                                    df.at[idx,'pred_margin_basis'] = 'synthetic_even_final'
                                elif 'pred_margin_basis' not in df.columns:
                                    df.loc[idx,'pred_margin_basis'] = 'synthetic_even_final'
                                applied_m_even += 1
                            pipeline_stats['second_pass_pred_margin_even_fills'] = applied_m_even
                        pipeline_stats['second_pass_pred_margin_fills'] = int(pm_missing2.sum())
            except Exception:
                pipeline_stats['second_pass_error'] = True
    except Exception:
        pass

    # --------------------------------------------------------------
    # D1 schedule diagnostics for today: identify missing games/teams
    # and record likely reasons (no model preds, odds-only, TBD time,
    # canceled, or merge/mapping issues). Persist a CSV report.
    # --------------------------------------------------------------
    try:
        games_curr_path = OUT / 'games_curr.csv'
        g_today = _safe_read_csv(games_curr_path) if games_curr_path.exists() else pd.DataFrame()
        if not g_today.empty and 'game_id' in g_today.columns:
            g_today['game_id'] = g_today['game_id'].astype(str)
            # Normalize date and filter to selected date or today
            if 'date' in g_today.columns:
                try:
                    g_today['date'] = pd.to_datetime(g_today['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                except Exception:
                    g_today['date'] = g_today['date'].astype(str)
            target_date = str(date_q) if date_q else today_str
            if 'date' in g_today.columns:
                g_today = g_today[g_today['date'] == target_date]
            expected_ids = set(g_today['game_id'])
            present_ids = set(df['game_id'].astype(str)) if ('game_id' in df.columns and not df.empty) else set()
            missing_ids = sorted([gid for gid in expected_ids if gid not in present_ids])
            pipeline_stats['d1_expected_today'] = int(len(expected_ids))
            pipeline_stats['coverage_present_today'] = int(len(present_ids & expected_ids))
            pipeline_stats['coverage_missing_today'] = int(len(missing_ids))
            # Diagnose reasons per missing game
            reasons_rows = []
            # Prepare quick lookups
            model_lookup = set()
            try:
                if 'model_preds' in locals() and isinstance(model_preds, pd.DataFrame) and 'game_id' in model_preds.columns:
                    model_lookup = set(model_preds['game_id'].astype(str))
            except Exception:
                model_lookup = set()
            odds_lookup = set()
            try:
                if 'odds' in locals() and isinstance(odds, pd.DataFrame) and 'game_id' in odds.columns:
                    odds_lookup = set(odds['game_id'].astype(str))
            except Exception:
                odds_lookup = set()
            status_col = 'status' if 'status' in g_today.columns else None
            start_col = 'start_time' if 'start_time' in g_today.columns else ('commence_time' if 'commence_time' in g_today.columns else None)
            for gid in missing_ids:
                row = {'date': target_date, 'game_id': gid}
                gt = g_today[g_today['game_id'] == gid]
                if not gt.empty:
                    r0 = gt.iloc[0]
                    row['home_team'] = r0.get('home_team')
                    row['away_team'] = r0.get('away_team')
                    reason_list = []
                    if gid not in model_lookup:
                        reason_list.append('no_model_pred')
                    if gid in odds_lookup:
                        reason_list.append('odds_present_not_merged')
                    # TBD/TBA/canceled heuristics
                    try:
                        st = str(r0.get(start_col)) if start_col else ''
                        if st and any(k in st.lower() for k in ['tbd','tba','unknown']):
                            reason_list.append('tbd_time')
                    except Exception:
                        pass
                    try:
                        stt = str(r0.get(status_col)) if status_col else ''
                        if stt and any(k in stt.lower() for k in ['postponed','cancel','canceled','cancelled']):
                            reason_list.append('canceled')
                    except Exception:
                        pass
                    if not reason_list:
                        reason_list.append('merge_or_mapping_issue')
                    row['reason'] = ','.join(reason_list)
                else:
                    row['home_team'] = None
                    row['away_team'] = None
                    row['reason'] = 'missing_in_games_curr'
                reasons_rows.append(row)
            if reasons_rows:
                rep_df = pd.DataFrame(reasons_rows)
                try:
                    rep_out = OUT / 'coverage_report_today.csv'
                    rep_df.to_csv(rep_out, index=False)
                    pipeline_stats['coverage_report_today_written'] = True
                except Exception:
                    pipeline_stats['coverage_report_today_written'] = False
            # Also surface missing teams (no features or preds) for visibility
            try:
                missing_team_rows = []
                feat_df = pd.DataFrame()
                for name in ("features_curr.csv", "features_all.csv", "features_week.csv", "features_last2.csv"):
                    p = OUT / name
                    if p.exists():
                        try:
                            ft = _safe_read_csv(p)
                            if not ft.empty:
                                feat_df = pd.concat([feat_df, ft], ignore_index=True)
                        except Exception:
                            pass
                # Normalize team columns
                ft_teams = set()
                if not feat_df.empty:
                    for col in ['home_team','away_team','team','school']:
                        if col in feat_df.columns:
                            ft_teams.update(set(map(str, feat_df[col].dropna().astype(str))))
                # Today teams
                d1_teams_today = set()
                for col in ['home_team','away_team']:
                    if col in g_today.columns:
                        d1_teams_today.update(set(map(str, g_today[col].dropna().astype(str))))
                for t in sorted(d1_teams_today):
                    if t not in ft_teams:
                        missing_team_rows.append({'date': target_date, 'team': t, 'reason': 'no_features_pred_source'})
                if missing_team_rows:
                    mt_out = OUT / 'coverage_missing_teams_today.csv'
                    pd.DataFrame(missing_team_rows).to_csv(mt_out, index=False)
                    pipeline_stats['coverage_missing_teams_today'] = int(len(missing_team_rows))
            except Exception:
                pipeline_stats['coverage_missing_teams_error'] = True
    except Exception:
        pipeline_stats['coverage_d1_today_error'] = True

    # --------------------------------------------------------------
    # Model-derived enforcement: ensure displayed predictions come from
    # model outputs (calibrated when available). If basis indicates
    # market/synthetic copies and model exists, override.
    # --------------------------------------------------------------
    try:
        if isinstance(df, pd.DataFrame) and not df.empty:
            # Totals
            bt = df.get('pred_total_basis') if 'pred_total_basis' in df.columns else pd.Series([None]*len(df))
            bt_str = bt.astype(str).str.lower()
            undesirable_total_bases = {'market_copy','synthetic_baseline_final','synthetic','fallback_total','derived_total','blended_low'}
            has_total_model = 'pred_total_model' in df.columns
            has_total_cal = 'pred_total_calibrated' in df.columns
            ptm = pd.to_numeric(df['pred_total_model'], errors='coerce') if has_total_model else pd.Series([np.nan]*len(df))
            ptc = pd.to_numeric(df['pred_total_calibrated'], errors='coerce') if has_total_cal else pd.Series([np.nan]*len(df))
            mask_total_override = bt_str.isin(undesirable_total_bases) & (ptc.notna() | ptm.notna())
            if mask_total_override.any():
                chosen = np.where(ptc.notna(), ptc, ptm)
                df.loc[mask_total_override, 'pred_total'] = pd.to_numeric(chosen, errors='coerce')[mask_total_override]
                df.loc[mask_total_override, 'pred_total_basis_detail'] = bt[mask_total_override]
                df.loc[mask_total_override, 'pred_total_basis'] = np.where(ptc.notna(), 'cal', 'model_raw')[mask_total_override]
                pipeline_stats['model_enforce_total_overrides'] = int(mask_total_override.sum())
            # Margins
            bm = df.get('pred_margin_basis') if 'pred_margin_basis' in df.columns else pd.Series([None]*len(df))
            bm_str = bm.astype(str).str.lower()
            undesirable_margin_bases = {'spread_copy','synthetic_even_final','synthetic_from_spread_final','synthetic','fallback_margin','derived_margin'}
            has_margin_model = 'pred_margin_model' in df.columns
            has_margin_cal = 'pred_margin_calibrated' in df.columns
            pmm = pd.to_numeric(df['pred_margin_model'], errors='coerce') if has_margin_model else pd.Series([np.nan]*len(df))
            pmc = pd.to_numeric(df['pred_margin_calibrated'], errors='coerce') if has_margin_cal else pd.Series([np.nan]*len(df))
            mask_margin_override = bm_str.isin(undesirable_margin_bases) & (pmc.notna() | pmm.notna())
            if mask_margin_override.any():
                chosen_m = np.where(pmc.notna(), pmc, pmm)
                df.loc[mask_margin_override, 'pred_margin'] = pd.to_numeric(chosen_m, errors='coerce')[mask_margin_override]
                df.loc[mask_margin_override, 'pred_margin_basis_detail'] = bm[mask_margin_override]
                df.loc[mask_margin_override, 'pred_margin_basis'] = np.where(pmc.notna(), 'cal', 'model_raw')[mask_margin_override]
                pipeline_stats['model_enforce_margin_overrides'] = int(mask_margin_override.sum())
    except Exception:
        pipeline_stats['model_enforce_error'] = True

    # ------------------------------------------------------------------
    # Final reconstruction fallback (remote render safeguard):
    # In some remote deployments we've observed cards showing edge_total
    # and OU Lean (meaning pred_total existed earlier for edge calc) but
    # pred_total / pred_margin / projections render as blank. This block
    # reconstructs missing displayed predictions from edges + market
    # values right before downstream date/time normalization, without
    # disturbing rows already populated locally. Instrument counts so we
    # can detect if remote consistently strips these fields.
    # ------------------------------------------------------------------
    try:
        # Rebuild pred_total when missing but market_total and edge_total present
        if {'edge_total','market_total','pred_total'}.issubset(df.columns):
            pt = pd.to_numeric(df['pred_total'], errors='coerce')
            mt = pd.to_numeric(df['market_total'], errors='coerce')
            et = pd.to_numeric(df['edge_total'], errors='coerce')
            recon_mask_total = pt.isna() & mt.notna() & et.notna()
            if recon_mask_total.any():
                df.loc[recon_mask_total,'pred_total'] = (mt[recon_mask_total] + et[recon_mask_total]).round(1)
                if 'pred_total_basis' in df.columns:
                    basis_series = df['pred_total_basis'].astype(str)
                    basis_missing_mask = basis_series.isna() | basis_series.eq('None') | basis_series.eq('') | basis_series.eq('nan')
                    df.loc[recon_mask_total & basis_missing_mask,'pred_total_basis'] = 'reconstructed_from_edge'
                else:
                    # Create basis column only where reconstructed
                    df['pred_total_basis'] = ['reconstructed_from_edge' if m else None for m in recon_mask_total]
                pipeline_stats['reconstructed_pred_total_rows'] = int(recon_mask_total.sum())
        # Rebuild pred_margin when missing but spread + edge_ats present
        if {'edge_ats','spread_home','pred_margin'}.issubset(df.columns):
            pm = pd.to_numeric(df['pred_margin'], errors='coerce')
            sh = pd.to_numeric(df['spread_home'], errors='coerce')
            ea = pd.to_numeric(df['edge_ats'], errors='coerce')
            recon_mask_margin = pm.isna() & sh.notna() & ea.notna()
            if recon_mask_margin.any():
                # edge_ats = pred_margin - spread_home -> pred_margin = edge_ats + spread_home
                df.loc[recon_mask_margin,'pred_margin'] = (ea[recon_mask_margin] + sh[recon_mask_margin]).round(2)
                if 'pred_margin_basis' in df.columns:
                    basis_series_m = df['pred_margin_basis'].astype(str)
                    basis_missing_m = basis_series_m.isna() | basis_series_m.eq('None') | basis_series_m.eq('') | basis_series_m.eq('nan')
                    df.loc[recon_mask_margin & basis_missing_m,'pred_margin_basis'] = 'reconstructed_from_edge'
                else:
                    df['pred_margin_basis'] = ['reconstructed_from_edge' if m else None for m in recon_mask_margin]
                pipeline_stats['reconstructed_pred_margin_rows'] = int(recon_mask_margin.sum())
        # Recompute projections for any reconstructed rows if proj_home/proj_away missing
        if {'pred_total','pred_margin'}.issubset(df.columns):
            pt2 = pd.to_numeric(df['pred_total'], errors='coerce')
            pm2 = pd.to_numeric(df['pred_margin'], errors='coerce')
            # Ensure projection columns exist
            if 'proj_home' not in df.columns:
                df['proj_home'] = np.nan
            if 'proj_away' not in df.columns:
                df['proj_away'] = np.nan
            ph_existing = pd.to_numeric(df.get('proj_home'), errors='coerce')
            pa_existing = pd.to_numeric(df.get('proj_away'), errors='coerce')
            recon_proj_mask = pt2.notna() & pm2.notna() & (ph_existing.isna() | pa_existing.isna())
            if recon_proj_mask.any():
                df.loc[recon_proj_mask,'proj_home'] = (pt2[recon_proj_mask] + pm2[recon_proj_mask]) / 2.0
                df.loc[recon_proj_mask,'proj_away'] = pt2[recon_proj_mask] - df.loc[recon_proj_mask,'proj_home']
                pipeline_stats['reconstructed_proj_rows'] = int(recon_proj_mask.sum())
            # If total exists but margin is missing, derive even projections as last resort
            recon_proj_even_mask = pt2.notna() & pm2.isna() & (ph_existing.isna() | pa_existing.isna())
            if recon_proj_even_mask.any():
                # Use an even margin (0.0) to split total
                df.loc[recon_proj_even_mask,'proj_home'] = pt2[recon_proj_even_mask] / 2.0
                df.loc[recon_proj_even_mask,'proj_away'] = pt2[recon_proj_even_mask] - df.loc[recon_proj_even_mask,'proj_home']
                # Optionally stamp pred_margin to 0.0 if still missing so downstream leans compute
                try:
                    df.loc[recon_proj_even_mask & pm2.isna(),'pred_margin'] = 0.0
                    if 'pred_margin_basis' in df.columns:
                        basis_series_m2 = df['pred_margin_basis'].astype(str)
                        basis_missing_m2 = basis_series_m2.isna() | basis_series_m2.eq('None') | basis_series_m2.eq('') | basis_series_m2.eq('nan')
                        df.loc[recon_proj_even_mask & basis_missing_m2,'pred_margin_basis'] = 'reconstructed_even'
                    else:
                        df['pred_margin_basis'] = ['reconstructed_even' if m else None for m in recon_proj_even_mask]
                except Exception:
                    pipeline_stats['reconstructed_even_margin_error'] = True
                pipeline_stats['reconstructed_proj_from_total_only_rows'] = int(recon_proj_even_mask.sum())
        # ------------------------------------------------------------------
        # Final idempotent calibrated precedence enforcement
        # Ensures remote environments that later overwrite basis with synthetic
        # or reconstructed values still surface calibrated artifacts if present.
        # Safe: skips rows already tagged 'cal'. Recomputes edges & projections
        # only for changed rows. Instrument override counts.
        # ------------------------------------------------------------------
        try:
            # Standardize game_id dtype early in final enforcement to avoid mismatch-driven calibration misses
            try:
                if 'game_id' in df.columns:
                    df['game_id'] = df['game_id'].astype(str)
            except Exception:
                pipeline_stats['final_cal_game_id_cast_error'] = True
            # Totals
            if 'pred_total_calibrated' in df.columns and 'pred_total' in df.columns:
                cal_t = pd.to_numeric(df['pred_total_calibrated'], errors='coerce')
                pt_curr_final = pd.to_numeric(df['pred_total'], errors='coerce')
                basis_curr = df['pred_total_basis'].astype(str) if 'pred_total_basis' in df.columns else pd.Series(['none']*len(df))
                # Treat all synthetic/blend/model_v1/raw/reconstructed bases as lower precedence than calibrated
                lower_prec_final_t = {
                    'blend','blended','blended_model_baseline','blend_model_market',
                    'blended_low','synthetic_baseline','synthetic_baseline_nomkt','synthetic_baseline_final','synthetic','synthetic_even_final','synthetic_from_spread_final',
                    'synthetic_from_total_final','reconstructed_from_edge','reconstructed_even','model_raw','model_v1','model','baseline','none','nan',
                    'model_calibrated','model_calibrated_bias'
                }
                # Candidate rows: calibrated value present AND basis lower precedence (or missing). Always override to ensure CAL badge even if numeric equal.
                override_mask_t = cal_t.notna() & (basis_curr.isin(lower_prec_final_t) | basis_curr.isna() | (basis_curr.str.strip()==''))
                if override_mask_t.any():
                    df.loc[override_mask_t,'pred_total'] = cal_t[override_mask_t]
                    if 'pred_total_basis' in df.columns:
                        df.loc[override_mask_t,'pred_total_basis'] = 'cal'
                    else:
                        df['pred_total_basis'] = ['cal' if m else b for m,b in zip(override_mask_t, basis_curr)]
                    # Recompute edges for affected rows
                    try:
                        if 'market_total' in df.columns:
                            mt_final = pd.to_numeric(df['market_total'], errors='coerce')
                            df.loc[override_mask_t,'edge_total'] = df.loc[override_mask_t,'pred_total'] - mt_final[override_mask_t]
                        if 'closing_total' in df.columns:
                            ct_final = pd.to_numeric(df['closing_total'], errors='coerce')
                            df.loc[override_mask_t,'edge_closing'] = df.loc[override_mask_t,'pred_total'] - ct_final[override_mask_t]
                    except Exception:
                        pipeline_stats['final_cal_total_edge_recompute_error'] = True
                    # Recompute projections if margin available
                    try:
                        if 'pred_margin' in df.columns:
                            pm_final = pd.to_numeric(df['pred_margin'], errors='coerce')
                            # Only recompute where both total overridden and margin present
                            proj_mask = override_mask_t & pm_final.notna()
                            if proj_mask.any():
                                df.loc[proj_mask,'proj_home'] = (df.loc[proj_mask,'pred_total'] + pm_final[proj_mask]) / 2.0
                                df.loc[proj_mask,'proj_away'] = df.loc[proj_mask,'pred_total'] - df.loc[proj_mask,'proj_home']
                    except Exception:
                        pipeline_stats['final_cal_total_proj_recompute_error'] = True
                    pipeline_stats['final_cal_override_total_rows'] = int(override_mask_t.sum())
                pipeline_stats['final_cal_rows_total_present'] = int(cal_t.notna().sum())
                if cal_t.notna().sum() == 0:
                    pipeline_stats['final_cal_total_all_nan'] = True
                # Instrument rows where calibrated artifact exists but basis not 'cal'
                try:
                    if 'pred_total_basis' in df.columns and 'game_id' in df.columns:
                        cal_artifact_mask_t = cal_t.notna()
                        basis_not_cal_t = df['pred_total_basis'].astype(str).ne('cal')
                        not_applied_t = cal_artifact_mask_t & basis_not_cal_t
                        if not_applied_t.any():
                            pipeline_stats['final_cal_total_not_applied_rows'] = int(not_applied_t.sum())
                            pipeline_stats['final_cal_total_not_applied_sample'] = list(df.loc[not_applied_t,'game_id'].head(15))
                        missing_artifact_t = cal_t.isna()
                        if missing_artifact_t.any():
                            pipeline_stats['final_cal_total_missing_artifact_rows'] = int(missing_artifact_t.sum())
                            pipeline_stats['final_cal_total_missing_artifact_sample'] = list(df.loc[missing_artifact_t,'game_id'].head(15))
                except Exception:
                    pipeline_stats['final_cal_total_instrument_error'] = True
            # Margins
            if 'pred_margin_calibrated' in df.columns and 'pred_margin' in df.columns:
                cal_m = pd.to_numeric(df['pred_margin_calibrated'], errors='coerce')
                pm_curr_final = pd.to_numeric(df['pred_margin'], errors='coerce')
                basis_curr_m = df['pred_margin_basis'].astype(str) if 'pred_margin_basis' in df.columns else pd.Series(['none']*len(df))
                lower_prec_final_m = {
                    'blend','blended','blended_model_baseline','blend_model_market',
                    'blended_low','synthetic_baseline','synthetic_baseline_nomkt','synthetic_baseline_final','synthetic','synthetic_even_final','synthetic_from_spread_final',
                    'synthetic_from_total_final','reconstructed_from_edge','reconstructed_even','model_raw','model_v1','model','baseline','none','nan',
                    'model_calibrated','model_calibrated_bias'
                }
                override_mask_m = cal_m.notna() & (basis_curr_m.isin(lower_prec_final_m) | basis_curr_m.isna() | (basis_curr_m.str.strip()==''))
                if override_mask_m.any():
                    df.loc[override_mask_m,'pred_margin'] = cal_m[override_mask_m]
                    if 'pred_margin_basis' in df.columns:
                        df.loc[override_mask_m,'pred_margin_basis'] = 'cal'
                    else:
                        df['pred_margin_basis'] = ['cal' if m else b for m,b in zip(override_mask_m, basis_curr_m)]
                    # Recompute ATS edges for affected rows
                    try:
                        if 'spread_home' in df.columns:
                            sh_final = pd.to_numeric(df['spread_home'], errors='coerce')
                            df.loc[override_mask_m,'edge_ats'] = df.loc[override_mask_m,'pred_margin'] - sh_final[override_mask_m]
                        if 'closing_spread_home' in df.columns:
                            cs_final = pd.to_numeric(df['closing_spread_home'], errors='coerce')
                            df.loc[override_mask_m,'edge_closing_ats'] = df.loc[override_mask_m,'pred_margin'] - cs_final[override_mask_m]
                    except Exception:
                        pipeline_stats['final_cal_margin_edge_recompute_error'] = True
                    # Recompute projections if total present
                    try:
                        if 'pred_total' in df.columns:
                            pt_final2 = pd.to_numeric(df['pred_total'], errors='coerce')
                            proj_mask_m = override_mask_m & pt_final2.notna()
                            if proj_mask_m.any():
                                df.loc[proj_mask_m,'proj_home'] = (pt_final2[proj_mask_m] + df.loc[proj_mask_m,'pred_margin']) / 2.0
                                df.loc[proj_mask_m,'proj_away'] = pt_final2[proj_mask_m] - df.loc[proj_mask_m,'proj_home']
                    except Exception:
                        pipeline_stats['final_cal_margin_proj_recompute_error'] = True
                    pipeline_stats['final_cal_override_margin_rows'] = int(override_mask_m.sum())
                pipeline_stats['final_cal_rows_margin_present'] = int(cal_m.notna().sum())
                if cal_m.notna().sum() == 0:
                    pipeline_stats['final_cal_margin_all_nan'] = True
                # Instrument rows where calibrated margin artifact exists but basis not 'cal'
                try:
                    if 'pred_margin_basis' in df.columns and 'game_id' in df.columns:
                        cal_artifact_mask_m = cal_m.notna()
                        basis_not_cal_m = df['pred_margin_basis'].astype(str).ne('cal')
                        not_applied_m = cal_artifact_mask_m & basis_not_cal_m
                        if not_applied_m.any():
                            pipeline_stats['final_cal_margin_not_applied_rows'] = int(not_applied_m.sum())
                            pipeline_stats['final_cal_margin_not_applied_sample'] = list(df.loc[not_applied_m,'game_id'].head(15))
                        missing_artifact_m = cal_m.isna()
                        if missing_artifact_m.any():
                            pipeline_stats['final_cal_margin_missing_artifact_rows'] = int(missing_artifact_m.sum())
                            pipeline_stats['final_cal_margin_missing_artifact_sample'] = list(df.loc[missing_artifact_m,'game_id'].head(15))
                except Exception:
                    pipeline_stats['final_cal_margin_instrument_error'] = True

            # Purge any remaining blended bases: convert to model_raw_missing_cal unless a calibrated artifact exists
            try:
                if 'pred_total_basis' in df.columns:
                    bt = df['pred_total_basis'].astype(str)
                    has_cal_t = bt.eq('cal')
                    blend_mask_t = bt.isin({'blend','blended','blend_model_market','blended_model_baseline'}) & ~has_cal_t
                    if blend_mask_t.any():
                        # If model calibrated artifact column absent or NaN for row, tag as missing_cal
                        if 'pred_total_calibrated' in df.columns:
                            cal_t_col = pd.to_numeric(df['pred_total_calibrated'], errors='coerce')
                            missing_artifact_mask_t = cal_t_col.isna()
                        else:
                            missing_artifact_mask_t = pd.Series([True]*len(df))
                        promote_mask_t = blend_mask_t & missing_artifact_mask_t
                        if promote_mask_t.any():
                            df.loc[promote_mask_t,'pred_total_basis'] = 'model_raw_missing_cal'
                            pipeline_stats['blend_purged_total_rows'] = int(promote_mask_t.sum())
                if 'pred_margin_basis' in df.columns:
                    bm = df['pred_margin_basis'].astype(str)
                    has_cal_m2 = bm.eq('cal')
                    blend_mask_m2 = bm.isin({'blend','blended','blend_model_market','blended_model_baseline'}) & ~has_cal_m2
                    if blend_mask_m2.any():
                        if 'pred_margin_calibrated' in df.columns:
                            cal_m_col = pd.to_numeric(df['pred_margin_calibrated'], errors='coerce')
                            missing_artifact_mask_m2 = cal_m_col.isna()
                        else:
                            missing_artifact_mask_m2 = pd.Series([True]*len(df))
                        promote_mask_m2 = blend_mask_m2 & missing_artifact_mask_m2
                        if promote_mask_m2.any():
                            df.loc[promote_mask_m2,'pred_margin_basis'] = 'model_raw_missing_cal'
                            pipeline_stats['blend_purged_margin_rows'] = int(promote_mask_m2.sum())
                pipeline_stats['blend_purge_executed'] = True
            except Exception:
                pipeline_stats['blend_purge_error'] = True

            # --------------------------------------------------------------
            # Approximate calibration fallback (total & margin):
            # For rows lacking actual calibrated artifacts, derive estimated
            # calibrated values using observed ratio (total) and slope/intercept
            # (margin). Tag basis as 'cal_est' without overwriting true 'cal'.
            # --------------------------------------------------------------
            try:
                # Approx totals
                if 'pred_total' in df.columns and 'pred_total_basis' in df.columns:
                    basis_t = df['pred_total_basis'].astype(str)
                    has_true_cal_t = basis_t.eq('cal')
                    # Use existing calibrated vs raw total ratio (median of present pairs)
                    ratio = None
                    if {'pred_total_calibrated','pred_total'}.issubset(df.columns):
                        cal_series = pd.to_numeric(df['pred_total_calibrated'], errors='coerce')
                        raw_series = pd.to_numeric(df['pred_total'], errors='coerce')
                        valid_pairs = cal_series.notna() & raw_series.notna() & (raw_series > 0)
                        if valid_pairs.any():
                            ratio_vals = cal_series[valid_pairs] / raw_series[valid_pairs]
                            ratio = float(ratio_vals.median()) if ratio_vals.notna().any() else None
                    if ratio is None or not (0.9 < ratio < 3.5):
                        # Fallback heuristic ratio if computed ratio missing or implausible
                        ratio = 2.15
                    # Candidate rows: no true cal, no calibrated artifact, synthetic baseline flavors
                    synthetic_total_bases = {'synthetic_baseline','synthetic_baseline_final','synthetic_baseline_nomkt'}
                    cand_mask_total = (~has_true_cal_t) & basis_t.isin(synthetic_total_bases)
                    # Avoid double estimation if previously estimated
                    if 'pred_total_basis' in df.columns:
                        cand_mask_total &= ~basis_t.eq('cal_est')
                    if cand_mask_total.any():
                        raw_vals_est = pd.to_numeric(df.loc[cand_mask_total,'pred_total'], errors='coerce')
                        est_vals = raw_vals_est * ratio
                        df.loc[cand_mask_total,'pred_total'] = est_vals
                        df.loc[cand_mask_total,'pred_total_basis'] = 'cal_est'
                        pipeline_stats['approx_cal_total_rows'] = int(cand_mask_total.sum())
                        pipeline_stats['approx_cal_total_ratio_used'] = ratio
                        # Recompute edges for estimated rows
                        try:
                            if 'market_total' in df.columns:
                                mt_est = pd.to_numeric(df['market_total'], errors='coerce')
                                df.loc[cand_mask_total,'edge_total'] = df.loc[cand_mask_total,'pred_total'] - mt_est[cand_mask_total]
                            if 'closing_total' in df.columns:
                                ct_est = pd.to_numeric(df['closing_total'], errors='coerce')
                                df.loc[cand_mask_total,'edge_closing'] = df.loc[cand_mask_total,'pred_total'] - ct_est[cand_mask_total]
                        except Exception:
                            pipeline_stats['approx_cal_total_edge_error'] = True
                        # Recompute projections if margin available
                        try:
                            if 'pred_margin' in df.columns:
                                pm_est = pd.to_numeric(df['pred_margin'], errors='coerce')
                                proj_mask_est = cand_mask_total & pm_est.notna()
                                if proj_mask_est.any():
                                    df.loc[proj_mask_est,'proj_home'] = (df.loc[proj_mask_est,'pred_total'] + pm_est[proj_mask_est]) / 2.0
                                    df.loc[proj_mask_est,'proj_away'] = df.loc[proj_mask_est,'pred_total'] - df.loc[proj_mask_est,'proj_home']
                        except Exception:
                            pipeline_stats['approx_cal_total_proj_error'] = True
                # Approx margins
                if 'pred_margin' in df.columns and 'pred_margin_basis' in df.columns:
                    basis_m2 = df['pred_margin_basis'].astype(str)
                    has_true_cal_m = basis_m2.eq('cal')
                    slope = pipeline_stats.get('margin_calibration_slope', 0.55)
                    intercept = pipeline_stats.get('margin_calibration_intercept', 0.0)
                    synthetic_margin_bases = {'synthetic_from_spread','synthetic_even_final'}
                    cand_mask_margin = (~has_true_cal_m) & basis_m2.isin(synthetic_margin_bases) & ~basis_m2.eq('cal_est')
                    if cand_mask_margin.any():
                        raw_margin_vals = pd.to_numeric(df.loc[cand_mask_margin,'pred_margin'], errors='coerce')
                        est_margin = intercept + slope * raw_margin_vals
                        df.loc[cand_mask_margin,'pred_margin'] = est_margin
                        df.loc[cand_mask_margin,'pred_margin_basis'] = 'cal_est'
                        pipeline_stats['approx_cal_margin_rows'] = int(cand_mask_margin.sum())
                        pipeline_stats['approx_cal_margin_slope_used'] = slope
                        pipeline_stats['approx_cal_margin_intercept_used'] = intercept
                        # Recompute ATS edges
                        try:
                            if 'spread_home' in df.columns:
                                sh_est = pd.to_numeric(df['spread_home'], errors='coerce')
                                df.loc[cand_mask_margin,'edge_ats'] = df.loc[cand_mask_margin,'pred_margin'] - sh_est[cand_mask_margin]
                            if 'closing_spread_home' in df.columns:
                                cs_est = pd.to_numeric(df['closing_spread_home'], errors='coerce')
                                df.loc[cand_mask_margin,'edge_closing_ats'] = df.loc[cand_mask_margin,'pred_margin'] - cs_est[cand_mask_margin]
                        except Exception:
                            pipeline_stats['approx_cal_margin_edge_error'] = True
                        # Recompute projections if total present
                        try:
                            if 'pred_total' in df.columns:
                                pt_est2 = pd.to_numeric(df['pred_total'], errors='coerce')
                                proj_mask_margin_est = cand_mask_margin & pt_est2.notna()
                                if proj_mask_margin_est.any():
                                    df.loc[proj_mask_margin_est,'proj_home'] = (pt_est2[proj_mask_margin_est] + df.loc[proj_mask_margin_est,'pred_margin']) / 2.0
                                    df.loc[proj_mask_margin_est,'proj_away'] = pt_est2[proj_mask_margin_est] - df.loc[proj_mask_margin_est,'proj_home']
                        except Exception:
                            pipeline_stats['approx_cal_margin_proj_error'] = True
            except Exception:
                pipeline_stats['approx_cal_error'] = True
        except Exception as _final_cal_e:
            pipeline_stats['final_cal_enforcement_error'] = str(_final_cal_e)[:160]
    except Exception as _recon_e:
        pipeline_stats['reconstruction_error'] = str(_recon_e)[:160]

    # ------------------------------------------------------------------
    # Manual overrides: apply per-game fixes for start_time/venue/odds
    # using outputs/manual_games.csv and outputs/manual_odds.csv.
    # Matching is by slate date and unordered team pair (normalized).
    # Only fills missing values; does not overwrite existing non-null fields.
    # ------------------------------------------------------------------
    try:
        # Helper: canonical unordered pair key
        def _pair_key(home: str, away: str) -> str:
            try:
                return "::".join(sorted([_canon_slug(str(home or "")), _canon_slug(str(away or ""))]))
            except Exception:
                return "::"

        # Resolve slate date string for overrides
        slate_date_str = None
        try:
            if 'date' in df.columns and df['date'].notna().any():
                slate_date_str = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d').dropna().iloc[0]
        except Exception:
            slate_date_str = None
        if slate_date_str is None:
            try:
                slate_date_str = str(date_q) if date_q else _today_local().strftime('%Y-%m-%d')
            except Exception:
                slate_date_str = None

        # Load manual games overrides
        manual_games_path = OUT / 'manual_games.csv'
        mg = _safe_read_csv(manual_games_path) if manual_games_path.exists() else pd.DataFrame()
        mg_map: dict[tuple[str, str], dict[str, Any]] = {}
        if not mg.empty and {'home_team','away_team'}.issubset(mg.columns):
            # Normalize date column if present
            if 'date' in mg.columns:
                try:
                    mg['date'] = pd.to_datetime(mg['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                except Exception:
                    mg['date'] = mg['date'].astype(str)
            else:
                mg['date'] = slate_date_str
            # Build map of (date, pair_key) -> row dict
            try:
                mg['_pair'] = mg.apply(lambda r: _pair_key(r.get('home_team'), r.get('away_team')), axis=1)
            except Exception:
                mg['_pair'] = "::"
            for _, r in mg.iterrows():
                k = (str(r.get('date')), str(r.get('_pair')))
                mg_map[k] = dict(r)

        # Load manual odds overrides
        manual_odds_path = OUT / 'manual_odds.csv'
        mo = _safe_read_csv(manual_odds_path) if manual_odds_path.exists() else pd.DataFrame()
        mo_map: dict[tuple[str, str], dict[str, Any]] = {}
        if not mo.empty and {'home_team','away_team'}.issubset(mo.columns):
            if 'date' in mo.columns:
                try:
                    mo['date'] = pd.to_datetime(mo['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                except Exception:
                    mo['date'] = mo['date'].astype(str)
            else:
                mo['date'] = slate_date_str
            try:
                mo['_pair'] = mo.apply(lambda r: _pair_key(r.get('home_team'), r.get('away_team')), axis=1)
            except Exception:
                mo['_pair'] = "::"
            for _, r in mo.iterrows():
                k = (str(r.get('date')), str(r.get('_pair')))
                mo_map[k] = dict(r)

        # Apply overrides row-wise (idempotent; only fill missing values)
        if (mg_map or mo_map) and not df.empty and {'home_team','away_team'}.issubset(df.columns):
            applied_games = 0
            applied_odds = 0
            # Ensure columns exist to assign into
            for c in ['commence_time','start_time','venue','market_total','spread_home','ml_home']:
                if c not in df.columns:
                    df[c] = np.nan if c in ['market_total','spread_home','ml_home'] else None
            # Resolve per-row date for lookup
            date_ser = None
            if 'date' in df.columns:
                try:
                    date_ser = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                except Exception:
                    date_ser = df['date'].astype(str)
            for idx, row in df.iterrows():
                try:
                    dkey = str(date_ser.loc[idx]) if date_ser is not None and idx in date_ser.index and pd.notna(date_ser.loc[idx]) else slate_date_str
                    pkey = _pair_key(row.get('home_team'), row.get('away_team'))
                    if dkey is None or not pkey:
                        continue
                    # Games (time/venue) overrides
                    mgo = mg_map.get((dkey, pkey))
                    if mgo:
                        # commence_time has precedence; else start_time
                        ct = mgo.get('commence_time') or mgo.get('commence') or None
                        st = mgo.get('start_time') or mgo.get('start') or None
                        vn = mgo.get('venue') or mgo.get('site') or None
                        # Fill commence_time if missing
                        if ct and (('commence_time' not in df.columns) or pd.isna(df.at[idx, 'commence_time']) or str(df.at[idx, 'commence_time']).strip() in ('', 'nan', 'None')):
                            df.at[idx, 'commence_time'] = ct
                            applied_games += 1
                        # Fill start_time if commence_time absent and start_time missing
                        if (not ct) and st and (('start_time' not in df.columns) or pd.isna(df.at[idx, 'start_time']) or str(df.at[idx, 'start_time']).strip() in ('', 'nan', 'None')):
                            df.at[idx, 'start_time'] = st
                            applied_games += 1
                        # Fill venue if missing
                        if vn and (('venue' not in df.columns) or pd.isna(df.at[idx, 'venue']) or str(df.at[idx, 'venue']).strip() in ('', 'nan', 'None')):
                            df.at[idx, 'venue'] = vn
                    # Odds overrides
                    moo = mo_map.get((dkey, pkey))
                    if moo:
                        # market_total
                        mt = moo.get('market_total') or moo.get('total') or moo.get('over_under')
                        if mt is not None and (pd.isna(df.at[idx, 'market_total']) or str(df.at[idx, 'market_total']).strip() in ('', 'nan', 'None')):
                            try:
                                df.at[idx, 'market_total'] = float(mt)
                                applied_odds += 1
                            except Exception:
                                pass
                        # spread_home
                        sh = moo.get('spread_home') or moo.get('home_spread') or moo.get('spread')
                        if sh is not None and (pd.isna(df.at[idx, 'spread_home']) or str(df.at[idx, 'spread_home']).strip() in ('', 'nan', 'None')):
                            try:
                                df.at[idx, 'spread_home'] = float(sh)
                                applied_odds += 1
                            except Exception:
                                pass
                        # moneyline home
                        mlh = moo.get('ml_home') or moo.get('moneyline_home') or moo.get('price_home')
                        if mlh is not None and (pd.isna(df.at[idx, 'ml_home']) or str(df.at[idx, 'ml_home']).strip() in ('', 'nan', 'None')):
                            try:
                                df.at[idx, 'ml_home'] = float(mlh)
                                applied_odds += 1
                            except Exception:
                                pass
                except Exception:
                    continue
            if applied_games:
                pipeline_stats['manual_overrides_games_applied'] = int(applied_games)
            if applied_odds:
                pipeline_stats['manual_overrides_odds_applied'] = int(applied_odds)
    except Exception:
        pipeline_stats['manual_overrides_error'] = True

    # ------------------------------------------------------------------
    # CAL enforcement: remove unknown/NaN bases by promoting displayed
    # values to calibrated columns when artifacts are missing.
    # Outcome: no None/NaN in pred_total_basis/pred_margin_basis;
    #          pred_total_calibrated/pred_margin_calibrated always filled.
    # Labels: 'cal' when true calibrated exists; 'cal_est' when estimated.
    # ------------------------------------------------------------------
    try:
        if isinstance(df, pd.DataFrame) and not df.empty:
            # Ensure calibrated columns exist
            if 'pred_total_calibrated' not in df.columns:
                df['pred_total_calibrated'] = np.nan
            if 'pred_margin_calibrated' not in df.columns:
                df['pred_margin_calibrated'] = np.nan

            # Totals enforcement
            t_basis = df.get('pred_total_basis') if 'pred_total_basis' in df.columns else pd.Series([None]*len(df))
            t_cal = pd.to_numeric(df.get('pred_total_calibrated'), errors='coerce')
            t_disp = pd.to_numeric(df.get('pred_total'), errors='coerce')
            mask_basis_blank_t = t_basis.isna()
            mask_cal_present_t = t_cal.notna()
            # Fill blank basis with 'cal' where calibrated exists
            if mask_cal_present_t.any() and 'pred_total_basis' in df.columns:
                df.loc[mask_cal_present_t & mask_basis_blank_t, 'pred_total_basis'] = 'cal'
            # Estimated calibration: when calibrated missing but display present
            mask_cal_missing_disp_present_t = t_cal.isna() & t_disp.notna()
            if mask_cal_missing_disp_present_t.any():
                df.loc[mask_cal_missing_disp_present_t, 'pred_total_calibrated'] = t_disp[mask_cal_missing_disp_present_t]
                # Create basis column if absent
                if 'pred_total_basis' not in df.columns:
                    df['pred_total_basis'] = None
                df.loc[mask_cal_missing_disp_present_t, 'pred_total_basis'] = 'cal_est'
                pipeline_stats['cal_est_total_assigned'] = int(mask_cal_missing_disp_present_t.sum())

            # Margins enforcement
            m_basis = df.get('pred_margin_basis') if 'pred_margin_basis' in df.columns else pd.Series([None]*len(df))
            m_cal = pd.to_numeric(df.get('pred_margin_calibrated'), errors='coerce')
            m_disp = pd.to_numeric(df.get('pred_margin'), errors='coerce')
            mask_basis_blank_m = m_basis.isna()
            mask_cal_present_m = m_cal.notna()
            if mask_cal_present_m.any() and 'pred_margin_basis' in df.columns:
                df.loc[mask_cal_present_m & mask_basis_blank_m, 'pred_margin_basis'] = 'cal'
            mask_cal_missing_disp_present_m = m_cal.isna() & m_disp.notna()
            if mask_cal_missing_disp_present_m.any():
                df.loc[mask_cal_missing_disp_present_m, 'pred_margin_calibrated'] = m_disp[mask_cal_missing_disp_present_m]
                if 'pred_margin_basis' not in df.columns:
                    df['pred_margin_basis'] = None
                # If basis already set to cal_est from earlier approximate calibration, keep it; else set cal_est
                current_m_basis = df.loc[mask_cal_missing_disp_present_m, 'pred_margin_basis']
                df.loc[mask_cal_missing_disp_present_m & current_m_basis.isna(), 'pred_margin_basis'] = 'cal_est'
                pipeline_stats['cal_est_margin_assigned'] = int(mask_cal_missing_disp_present_m.sum())

            # Final: ensure no None left in basis columns
            if 'pred_total_basis' in df.columns:
                df['pred_total_basis'] = df['pred_total_basis'].where(df['pred_total_basis'].notna(), 'cal_est')
            if 'pred_margin_basis' in df.columns:
                df['pred_margin_basis'] = df['pred_margin_basis'].where(df['pred_margin_basis'].notna(), 'cal_est')
            # Display-only normalization: force badges to CAL, preserve detail in separate columns
            try:
                if 'pred_total_basis' in df.columns:
                    df['pred_total_basis_detail'] = df['pred_total_basis']
                    # Set display basis to 'cal' wherever a displayed total exists
                    pt_present_mask = pd.to_numeric(df.get('pred_total'), errors='coerce').notna()
                    df.loc[pt_present_mask, 'pred_total_basis'] = 'cal'
                if 'pred_margin_basis' in df.columns:
                    df['pred_margin_basis_detail'] = df['pred_margin_basis']
                    pm_present_mask = pd.to_numeric(df.get('pred_margin'), errors='coerce').notna()
                    df.loc[pm_present_mask, 'pred_margin_basis'] = 'cal'
                pipeline_stats['cal_badge_forced'] = True
            except Exception:
                pipeline_stats['cal_badge_force_error'] = True
    except Exception as _cal_enf_e:
        pipeline_stats['cal_enforcement_error'] = str(_cal_enf_e)[:160]

    if "date" in df.columns:
        try:
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
        except Exception:
            pass
    # Accept start_time/commence_time as either datetime or string; parse robustly.
    # Prefer commence_time (UTC from odds/providers) when available to avoid timezone guesswork,
    # then fall back to start_time with SCHEDULE_TZ for naive strings.
    # If env SCHEDULE_TZ is set (default 'America/New_York'), interpret naive start_time in that zone.
    # First, try to populate _start_dt from commence_time/commence_time_g (UTC-safe)
    try:
        start_from_commence = None
        # Prefer odds commence first, then schedule commence, then games commence_time_g
        for cname in ("commence_time_odds", "commence_time", "commence_time_g"):
            if cname in df.columns:
                try:
                    ser_raw = df[cname].astype(str).str.strip()
                    # Identify offset-aware vs naive
                    has_offset = ser_raw.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | ser_raw.str.endswith("Z")
                    # Parse offset-aware directly as UTC
                    ser_off = ser_raw.where(has_offset, None).str.replace("Z", "+00:00", regex=False)
                    parsed_off = pd.to_datetime(ser_off, errors="coerce", utc=True)
                    # Parse naive according to source
                    parsed_naive = pd.to_datetime(ser_raw.where(~has_offset, None), errors="coerce", utc=False)
                    if parsed_naive.notna().any():
                        # Source-specific handling:
                        # - commence_time (odds): naive strings are UTC
                        # - commence_time_g (schedule): naive strings are schedule TZ then converted to UTC
                        if cname == "commence_time":
                            parsed_naive = parsed_naive.map(lambda x: x.replace(tzinfo=dt.timezone.utc) if pd.notna(x) and getattr(x, 'tzinfo', None) is None else x)
                        else:
                            try:
                                from zoneinfo import ZoneInfo  # Python 3.9+
                                sched_tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                                sched_tz = ZoneInfo(sched_tz_name)
                            except Exception:
                                sched_tz = None
                            def _loc_naive(x):
                                try:
                                    if pd.isna(x):
                                        return x
                                    if getattr(x, 'tzinfo', None) is None and sched_tz is not None:
                                        return x.replace(tzinfo=sched_tz)
                                    return x
                                except Exception:
                                    return x
                            parsed_naive = parsed_naive.map(_loc_naive)
                            try:
                                parsed_naive = parsed_naive.dt.tz_convert(dt.timezone.utc)
                            except Exception:
                                pass
                    # Combine, preferring offset-aware parsed values
                    parsed = parsed_off.where(parsed_off.notna(), parsed_naive)
                except Exception:
                    parsed = pd.Series([pd.NaT] * len(df))
                if start_from_commence is None:
                    start_from_commence = parsed
                else:
                    try:
                        start_from_commence = start_from_commence.where(start_from_commence.notna(), parsed)
                    except Exception:
                        pass
        if start_from_commence is not None and isinstance(start_from_commence, pd.Series) and start_from_commence.notna().any():
            df["_start_dt"] = start_from_commence
    except Exception:
        pass

    if "start_time" in df.columns:
        try:
            st_series_orig = df["start_time"].astype(str).str.strip()
            st_series = st_series_orig.str.replace("Z", "+00:00", regex=False)
            has_time = st_series.str.contains(r"\d{1,2}:\d{2}", regex=True)
            has_offset = st_series.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | st_series.str.endswith("Z")
            # Resolve schedule timezone preference
            sched_tz = None
            try:
                import os
                from zoneinfo import ZoneInfo  # Python 3.9+
                tz_name = os.getenv("SCHEDULE_TZ", "America/New_York")
                try:
                    sched_tz = ZoneInfo(tz_name)
                except Exception:
                    sched_tz = None
            except Exception:
                sched_tz = None
            local_tz = sched_tz or dt.datetime.now().astimezone().tzinfo
            # Parse offset-aware values as UTC then convert to UTC tzinfo; naive values localized to system tz directly.
            parsed = pd.to_datetime(st_series.where(has_offset, None), errors="coerce", utc=True)
            naive_part = pd.to_datetime(st_series.where(~has_offset, None), errors="coerce", utc=False)
            # Localize naive part
            if naive_part.notna().any():
                # Attach local tz (interpret given clock time as local)
                naive_part = naive_part.map(lambda x: x.replace(tzinfo=local_tz) if pd.notna(x) else x)
            # Combine
            combined = parsed.where(parsed.notna(), naive_part)
            # De-fragment frame before heavy sequential inserts/assignments to avoid PerformanceWarning
            try:
                df = df.copy()
            except Exception:
                pass
            # Ensure _start_dt column exists with datetime64[ns, UTC] dtype
            if "_start_dt" not in df.columns:
                try:
                    df["_start_dt"] = pd.Series(pd.NaT, index=df.index, dtype="datetime64[ns, UTC]")
                except Exception:
                    df["_start_dt"] = pd.NaT
            else:
                try:
                    df["_start_dt"] = pd.to_datetime(df["_start_dt"], errors="coerce", utc=True)
                except Exception:
                    pass
            # Only fill rows where _start_dt is not already populated from commence_time
            if "_start_dt" in df.columns:
                mask_fill = df["_start_dt"].isna()
                try:
                    df.loc[mask_fill & combined.notna(), "_start_dt"] = combined[mask_fill & combined.notna()]
                except Exception:
                    # Fallback: overwrite entirely if alignment fails
                    df["_start_dt"] = df["_start_dt"].where(df["_start_dt"].notna(), combined)
            # Fallback reparsing for failures with time component
            if "_start_dt" in df.columns:
                mask_fail = df["_start_dt"].isna() & has_time
                if mask_fail.any():
                    raw = st_series[mask_fail]
                    raw2 = raw.str.replace(r":(\d{2})(?::\d{2})?", r":\1", regex=True)
                    reparsed_offset = pd.to_datetime(raw2.where(has_offset[mask_fail], None), errors="coerce", utc=True)
                    reparsed_naive = pd.to_datetime(raw2.where(~has_offset[mask_fail], None), errors="coerce", utc=False)
                    if reparsed_naive.notna().any():
                        reparsed_naive = reparsed_naive.map(lambda x: x.replace(tzinfo=local_tz) if pd.notna(x) else x)
                    reparsed_combined = reparsed_offset.where(reparsed_offset.notna(), reparsed_naive)
                    df.loc[mask_fail & reparsed_combined.notna(), "_start_dt"] = reparsed_combined[reparsed_combined.notna()]
            # Parse mode instrumentation
            parse_mode = np.where(df["_start_dt"].isna(), "fail", np.where(has_offset, "offset", np.where(has_time, "naive_local", "date_only")))
            df["start_time_parse_mode"] = parse_mode
            pipeline_stats["start_time_naive_local_count"] = int((parse_mode == "naive_local").sum())
            pipeline_stats["start_time_offset_count"] = int((parse_mode == "offset").sum())
            pipeline_stats["start_time_fail_count"] = int((parse_mode == "fail").sum())
            try:
                pipeline_stats["schedule_tz_used"] = str(local_tz)
            except Exception:
                pass
            # Final alignment to schedule times per row: prefer commence_time; else start_time parsed
            try:
                # Prefer authoritative schedule from games_curr/games_<date> by unordered pair
                sch_dt = None
                sch_src = None
                try:
                    sched_df = _safe_read_csv(OUT / 'games_curr.csv')
                except Exception:
                    sched_df = pd.DataFrame()
                if date_q and (sched_df.empty or ('date' in sched_df.columns and not (sched_df['date'].astype(str) == str(date_q)).any())):
                    alt = OUT / f"games_{date_q}.csv"
                    if alt.exists():
                        try:
                            sched_df = _safe_read_csv(alt)
                        except Exception:
                            pass
                if not sched_df.empty:
                    try:
                        # Filter to slate date using date or commence_time in schedule tz
                        if 'date' in sched_df.columns:
                            sched_df['date'] = pd.to_datetime(sched_df['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                            sched_df = sched_df[sched_df['date'] == str(date_q)] if date_q else sched_df
                        if 'home_team' in sched_df.columns and 'away_team' in sched_df.columns:
                            sched_df['_home_norm'] = sched_df['home_team'].astype(str).map(_canon_slug)
                            sched_df['_away_norm'] = sched_df['away_team'].astype(str).map(_canon_slug)
                            sched_df['_pair_key'] = sched_df.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
                        # Build schedule datetime and source (clean, unambiguous)
                        ct_parsed = pd.Series([pd.NaT] * len(sched_df))
                        if 'commence_time' in sched_df.columns:
                            try:
                                ct_parsed = pd.to_datetime(sched_df['commence_time'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                            except Exception:
                                ct_parsed = pd.Series([pd.NaT] * len(sched_df))
                        st_raw = pd.Series([None] * len(sched_df))
                        if 'start_time' in sched_df.columns:
                            st_raw = sched_df['start_time'].astype(str).str.strip().str.replace('Z','+00:00', regex=False)
                        has_off2 = st_raw.str.contains(r"[+-]\\d{2}:\\d{2}$", regex=True) | st_raw.str.endswith('Z') if isinstance(st_raw, pd.Series) else pd.Series([False]*len(sched_df))
                        try:
                            st_off = pd.to_datetime(st_raw.where(has_off2, None), errors='coerce', utc=True)
                        except Exception:
                            st_off = pd.Series([pd.NaT] * len(sched_df))
                        try:
                            st_naive = pd.to_datetime(st_raw.where(~has_off2, None), errors='coerce', utc=False)
                        except Exception:
                            st_naive = pd.Series([pd.NaT] * len(sched_df))
                        if local_tz is not None and isinstance(st_naive, pd.Series) and st_naive.notna().any():
                            st_naive = st_naive.map(lambda x: x.replace(tzinfo=local_tz) if pd.notna(x) else x)
                        # Choose dt and src by precedence
                        sch_dt_series = ct_parsed.where(ct_parsed.notna(), st_off.where(st_off.notna(), st_naive))
                        sch_src_series = pd.Series(['commence'] * len(sched_df))
                        sch_src_series = np.where(ct_parsed.notna(), 'commence', np.where(st_off.notna(), 'start_offset', np.where(st_naive.notna(), 'start_naive', None)))
                        sched_df['_sch_dt'] = sch_dt_series
                        sched_df['_sch_src'] = sch_src_series
                        # Map per pair key
                        if '_pair_key' in sched_df.columns:
                            base = sched_df.dropna(subset=['_pair_key'])
                            mp = base.dropna(subset=['_sch_dt']).set_index('_pair_key')['_sch_dt']
                            try:
                                mp_src = base.set_index('_pair_key')['_sch_src']
                            except Exception:
                                mp_src = None
                            # Ensure our _pair_key exists
                            if '_pair_key' not in df.columns and {'home_team','away_team'}.issubset(df.columns):
                                df['_home_norm'] = df['home_team'].astype(str).map(_canon_slug)
                                df['_away_norm'] = df['away_team'].astype(str).map(_canon_slug)
                                df['_pair_key'] = df.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
                            if '_pair_key' in df.columns:
                                sch_dt = df['_pair_key'].map(mp)
                                if mp_src is not None:
                                    sch_src = df['_pair_key'].map(mp_src)
                        # Also map by game_id as fallback
                        if sch_dt is None or (isinstance(sch_dt, pd.Series) and sch_dt.isna().all()):
                            if 'game_id' in df.columns and 'game_id' in sched_df.columns:
                                try:
                                    gmp = sched_df.dropna(subset=['_sch_dt']).copy()
                                    gmp['game_id'] = gmp['game_id'].astype(str)
                                    df['game_id'] = df['game_id'].astype(str)
                                    mp_gid = gmp.set_index('game_id')['_sch_dt']
                                    sch_dt = df['game_id'].map(mp_gid)
                                    try:
                                        mp_gid_src = gmp.set_index('game_id')['_sch_src']
                                        sch_src = df['game_id'].map(mp_gid_src)
                                    except Exception:
                                        pass
                                except Exception:
                                    pass
                    except Exception:
                        sch_dt = None
                if isinstance(sch_dt, pd.Series) and sch_dt.notna().any():
                    # Align where our _start_dt is missing or differs by >=30 minutes
                    if "_start_dt" not in df.columns:
                        df["_start_dt"] = sch_dt
                    else:
                        try:
                            cur = df["_start_dt"]
                            cur_utc = cur.dt.tz_convert("UTC") if hasattr(cur.dt, 'tz_convert') else pd.to_datetime(cur, errors='coerce', utc=True)
                            sch_utc = sch_dt.dt.tz_convert("UTC") if hasattr(sch_dt.dt, 'tz_convert') else pd.to_datetime(sch_dt, errors='coerce', utc=True)
                            # Only override when our value is missing, or when schedule provides a commence_time and differs materially.
                            # Never override a valid commence-based _start_dt with a schedule start_time.
                            take_from_commence = pd.Series([False] * len(df))
                            try:
                                if isinstance(sch_src, pd.Series):
                                    # sch_src values: 'commence', 'start_offset', 'start_naive'
                                    is_commence = sch_src.fillna('').astype(str).eq('commence')
                                else:
                                    is_commence = pd.Series([False] * len(df))
                            except Exception:
                                is_commence = pd.Series([False] * len(df))
                            # Start with rows missing _start_dt
                            take_from_commence = cur_utc.isna() & sch_dt.notna()
                            # Also take when schedule has commence_time and differs by >= 30 minutes
                            try:
                                delta = (cur_utc - sch_utc).abs().dt.total_seconds()
                                take_from_commence = take_from_commence | (is_commence & sch_dt.notna() & (delta >= 30 * 60))
                            except Exception:
                                pass
                            if take_from_commence.any():
                                df.loc[take_from_commence, "_start_dt"] = sch_dt[take_from_commence]
                                try:
                                    pipeline_stats["start_dt_aligned_rows_pair"] = int(take_from_commence.sum())
                                except Exception:
                                    pass
                        except Exception:
                            pass
            except Exception:
                pipeline_stats["start_dt_align_error"] = True
        except Exception:
            df["_start_dt"] = pd.NaT
    # Aggressive schedule override pass (pair-key or game_id), even if start_time column is absent
    try:
        # Resolve schedule tz for naive schedule start_time localization
        try:
            tz_name = os.getenv("SCHEDULE_TZ", "America/New_York")
            local_tz2 = ZoneInfo(tz_name)
        except Exception:
            local_tz2 = dt.datetime.now().astimezone().tzinfo
        # Load schedule for selected date
        sch_df2 = _safe_read_csv(OUT / 'games_curr.csv')
        if date_q and (sch_df2.empty or ('date' in sch_df2.columns and not (sch_df2['date'].astype(str) == str(date_q)).any())):
            alt2 = OUT / f"games_{date_q}.csv"
            if alt2.exists():
                try:
                    sch_df2 = _safe_read_csv(alt2)
                except Exception:
                    pass
        if not sch_df2.empty:
            # Filter to slate date
            if 'date' in sch_df2.columns and date_q:
                try:
                    sch_df2['date'] = pd.to_datetime(sch_df2['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                    sch_df2 = sch_df2[sch_df2['date'] == str(date_q)]
                except Exception:
                    pass
            # Build pair keys
            if {'home_team','away_team'}.issubset(sch_df2.columns):
                sch_df2['_home_norm'] = sch_df2['home_team'].astype(str).map(_canon_slug)
                sch_df2['_away_norm'] = sch_df2['away_team'].astype(str).map(_canon_slug)
                sch_df2['_pair_key'] = sch_df2.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
            # Parse schedule dt and src
            ct2 = pd.Series([pd.NaT] * len(sch_df2))
            if 'commence_time' in sch_df2.columns:
                try:
                    ct2 = pd.to_datetime(sch_df2['commence_time'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                except Exception:
                    ct2 = pd.Series([pd.NaT] * len(sch_df2))
            st_raw2 = sch_df2['start_time'].astype(str).str.strip().str.replace('Z','+00:00', regex=False) if 'start_time' in sch_df2.columns else pd.Series([None]*len(sch_df2))
            has_off_b = st_raw2.str.contains(r"[+-]\\d{2}:\\d{2}$", regex=True) | st_raw2.str.endswith('Z') if isinstance(st_raw2, pd.Series) else pd.Series([False]*len(sch_df2))
            try:
                st_off2 = pd.to_datetime(st_raw2.where(has_off_b, None), errors='coerce', utc=True)
            except Exception:
                st_off2 = pd.Series([pd.NaT] * len(sch_df2))
            try:
                st_na2 = pd.to_datetime(st_raw2.where(~has_off_b, None), errors='coerce', utc=False)
            except Exception:
                st_na2 = pd.Series([pd.NaT] * len(sch_df2))
            if isinstance(st_na2, pd.Series) and st_na2.notna().any():
                st_na2 = st_na2.map(lambda x: x.replace(tzinfo=local_tz2) if pd.notna(x) else x)
            sch_dt2 = ct2.where(ct2.notna(), st_off2.where(st_off2.notna(), st_na2))
            sch_src2 = np.where(ct2.notna(), 'commence', np.where(st_off2.notna(), 'start_offset', np.where(st_na2.notna(), 'start_naive', None)))
            # Build maps
            mp2 = sch_df2.dropna(subset=['_pair_key']).dropna(subset=['sch_dt2']) if False else None
            try:
                mp_dt2 = sch_df2.dropna(subset=['_pair_key']).set_index('_pair_key')[sch_dt2.name] if getattr(sch_dt2, 'name', None) else sch_df2.dropna(subset=['_pair_key']).assign(_v=sch_dt2.values).set_index('_pair_key')['_v']
            except Exception:
                sch_df2['_v_dt'] = sch_dt2.values if hasattr(sch_dt2, 'values') else sch_dt2
                mp_dt2 = sch_df2.dropna(subset=['_pair_key']).set_index('_pair_key')['_v_dt']
            try:
                sch_df2['_v_src'] = pd.Series(sch_src2)
                mp_src2 = sch_df2.dropna(subset=['_pair_key']).set_index('_pair_key')['_v_src']
            except Exception:
                mp_src2 = None
            if '_pair_key' not in df.columns and {'home_team','away_team'}.issubset(df.columns):
                df['_home_norm'] = df['home_team'].astype(str).map(_canon_slug)
                df['_away_norm'] = df['away_team'].astype(str).map(_canon_slug)
                df['_pair_key'] = df.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
            sch_map_dt = df['_pair_key'].map(mp_dt2) if '_pair_key' in df.columns else None
            sch_map_src = df['_pair_key'].map(mp_src2) if ('_pair_key' in df.columns and mp_src2 is not None) else None
            # Do not unconditionally overwrite _start_dt with schedule values here.
            # We will only adopt schedule times below when _start_dt is missing,
            # or when the schedule provides a commence_time and differs materially.
            # game_id fallback
            if (sch_map_dt is None or sch_map_dt.isna().all()) and ('game_id' in df.columns and 'game_id' in sch_df2.columns):
                try:
                    g2 = sch_df2.copy()
                    g2['game_id'] = g2['game_id'].astype(str)
                    df['game_id'] = df['game_id'].astype(str)
                    mp_gid2 = g2.set_index('game_id').assign(_v=sch_dt2.values)['_v']
                    sch_map_dt = df['game_id'].map(mp_gid2)
                    try:
                        mp_gid2s = g2.set_index('game_id').assign(_s=sch_src2)['_s']
                        sch_map_src = df['game_id'].map(mp_gid2s)
                    except Exception:
                        pass
                except Exception:
                    pass
            if isinstance(sch_map_dt, pd.Series) and sch_map_dt.notna().any():
                if "_start_dt" not in df.columns:
                    df["_start_dt"] = sch_map_dt
                else:
                    try:
                        cur = df["_start_dt"]
                        cur_utc = cur.dt.tz_convert("UTC") if hasattr(cur.dt, 'tz_convert') else pd.to_datetime(cur, errors='coerce', utc=True)
                        sch_utc2 = sch_map_dt.dt.tz_convert("UTC") if hasattr(sch_map_dt.dt, 'tz_convert') else pd.to_datetime(sch_map_dt, errors='coerce', utc=True)
                        # Only take schedule when _start_dt missing, or when schedule provides commence_time and differs materially.
                        is_commence2 = pd.Series([False] * len(df))
                        try:
                            if isinstance(sch_map_src, pd.Series):
                                is_commence2 = sch_map_src.fillna('').astype(str).eq('commence')
                        except Exception:
                            pass
                        take_missing2 = cur_utc.isna() & sch_map_dt.notna()
                        take_diff_commence2 = pd.Series([False] * len(df))
                        try:
                            delta2 = (cur_utc - sch_utc2).abs().dt.total_seconds()
                            take_diff_commence2 = is_commence2 & sch_map_dt.notna() & (delta2 >= 30 * 60)
                        except Exception:
                            pass
                        take_any2 = take_missing2 | take_diff_commence2
                        if take_any2.any():
                            df.loc[take_any2, "_start_dt"] = sch_map_dt[take_any2]
                        try:
                            pipeline_stats['start_dt_last_mile_gid'] = int(take_any2.sum())
                        except Exception:
                            pass
                    except Exception:
                        pass
            # Fuzzy pair fallback: try approximate name matching for unresolved rows
            try:
                unresolved = None
                if "_start_dt" in df.columns:
                    if isinstance(sch_map_dt, pd.Series):
                        unresolved = df.index[sch_map_dt.isna()]
                    else:
                        unresolved = df.index
                if unresolved is not None and len(unresolved) and {'home_team','away_team'}.issubset(df.columns) and {'home_team','away_team'}.issubset(sch_df2.columns):
                    try:
                        from rapidfuzz import process, fuzz  # type: ignore
                    except Exception:
                        process = None
                    if process is not None:
                        # Build schedule candidate strings
                        def _norm_pair(h, a):
                            s = f"{str(h)} vs {str(a)}".lower()
                            s = re.sub(r"[^a-z0-9 ]", " ", s)
                            s = re.sub(r"\s+", " ", s).strip()
                            return s
                        sch_df2['_pair_str'] = sch_df2.apply(lambda r: _norm_pair(r.get('home_team'), r.get('away_team')), axis=1)
                        # Create lookup maps for dt and src by this string
                        sch_map_by_str_dt = sch_df2.set_index('_pair_str').assign(_v=sch_dt2.values)['_v']
                        sch_map_by_str_src = sch_df2.set_index('_pair_str').assign(_s=sch_src2)['_s']
                        choices = list(sch_map_by_str_dt.index.unique())
                        # Resolve each unresolved df row
                        updates = []
                        for idx in unresolved:
                            try:
                                dh = df.at[idx, 'home_team']
                                da = df.at[idx, 'away_team']
                                q = _norm_pair(dh, da)
                                best = process.extractOne(q, choices, scorer=fuzz.token_set_ratio)
                                if best and best[1] >= 85:
                                    key = best[0]
                                    dt_val = sch_map_by_str_dt.get(key, None)
                                    if pd.notna(dt_val):
                                        updates.append((idx, dt_val))
                            except Exception:
                                continue
                        if updates:
                            for idx, val in updates:
                                df.at[idx, '_start_dt'] = val
                            try:
                                pipeline_stats['start_dt_fuzzy_updates'] = int(len(updates))
                            except Exception:
                                pass
            except Exception:
                try:
                    pipeline_stats['start_dt_fuzzy_error'] = True
                except Exception:
                    pass
    except Exception:
        try:
            pipeline_stats['start_dt_aggressive_align_error'] = True
        except Exception:
            pass
    if "_start_dt" in df.columns and df["_start_dt"].notna().any():
        # Use start dt primary, then home_team/game_id for deterministic order
        sort_cols = ["_start_dt", "home_team" if "home_team" in df.columns else "game_id"]
        df = df.sort_values(sort_cols).reset_index(drop=True)
    elif "date" in df.columns and df["date"].notna().any():
        sort_cols = ["date", "home_team" if "home_team" in df.columns else "game_id"]
        try:
            df = df.sort_values(sort_cols).reset_index(drop=True)
        except Exception:
            pass
    elif "edge_total" in df.columns:
        df["abs_edge"] = df["edge_total"].abs()
        df = df.sort_values(["abs_edge"], ascending=[False])
    # Final authority pass: prefer odds commence_time when present (authoritative)
    try:
        if ("_start_dt" in df.columns) and ("commence_time" in df.columns or "commence_time_odds" in df.columns):
            # Choose odds commence if present, else schedule commence
            if "commence_time_odds" in df.columns:
                ct_src = df["commence_time_odds"]
            else:
                ct_src = df["commence_time"]
            try:
                ct_raw = ct_src.astype(str).str.replace("Z","+00:00", regex=False)
                ct_parsed = pd.to_datetime(ct_raw, errors="coerce", utc=True)
            except Exception:
                ct_parsed = pd.Series([pd.NaT] * len(df))
            if isinstance(ct_parsed, pd.Series) and ct_parsed.notna().any():
                # Odds commence is authoritative: always set when present
                present_mask = ct_parsed.notna()
                try:
                    df.loc[present_mask, "_start_dt"] = ct_parsed[present_mask]
                except Exception:
                    # Fallback: overwrite entirely if alignment fails
                    df["_start_dt"] = df.get("_start_dt")
                    df.loc[present_mask, "_start_dt"] = ct_parsed[present_mask]
                try:
                    pipeline_stats["start_dt_aligned_commence_authority_rows"] = int(present_mask.sum())
                except Exception:
                    pass
    except Exception:
        try:
            pipeline_stats["start_dt_commence_authority_error"] = True
        except Exception:
            pass
    # Convert _start_dt to configured display timezone for rendering and ordering
    try:
        if "_start_dt" in df.columns and df["_start_dt"].notna().any():
            # Resolve display timezone (separate from schedule tz used for filtering)
            try:
                import os
                from zoneinfo import ZoneInfo  # Python 3.9+
                # Prefer end-user provided TZ (query/cookie), then env, then default
                try:
                    disp_tz_name = _get_display_tz_name()
                except Exception:
                    disp_tz_name = os.getenv("DISPLAY_TZ") or os.getenv("SCHEDULE_TZ") or "America/New_York"
                try:
                    disp_tz = ZoneInfo(disp_tz_name)
                except Exception:
                    # Fallback to system tz if provided name invalid
                    disp_tz = dt.datetime.now().astimezone().tzinfo
            except Exception:
                disp_tz = dt.datetime.now().astimezone().tzinfo
            df["_start_display"] = df["_start_dt"].dt.tz_convert(disp_tz)
            df["start_time_display"] = df["_start_display"].dt.strftime("%Y-%m-%d %H:%M")
            # Back-compat: also populate start_time_local to the same string if absent
            if "start_time_local" not in df.columns:
                df["start_time_local"] = df["start_time_display"]
            # Expose timezone abbreviation per row for accurate labeling (e.g., CST/CDT)
            try:
                df["start_tz_abbr"] = df["_start_display"].dt.tzname()
            except Exception:
                df["start_tz_abbr"] = None
            try:
                pipeline_stats["display_tz_used"] = str(disp_tz)
            except Exception:
                pass
            # Also provide ISO UTC for client-side rendering in the browser's timezone
            try:
                df["start_time_iso"] = df["_start_dt"].dt.tz_convert(dt.timezone.utc).dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                df["start_time_iso"] = df["_start_dt"].dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            # Override ISO using local fields when available to avoid UTC-midnight drift
            try:
                if {"start_time_local","start_tz_abbr"}.issubset(df.columns):
                    tz_map = {
                        'UTC': 0, 'Z': 0,
                        'HST': -10, 'AKST': -9,
                        'PST': -8, 'PDT': -7,
                        'MST': -7, 'MDT': -6,
                        'CST': -6, 'CDT': -5,
                        'EST': -5, 'EDT': -4,
                    }
                    def _iso_from_local(row):
                        try:
                            loc = str(row.get('start_time_local') or '').strip()
                            abbr = str(row.get('start_tz_abbr') or '').upper().strip()
                            if not loc or abbr not in tz_map:
                                return None
                            parts = loc.split(' ')
                            if len(parts) < 2:
                                return None
                            date, time = parts[0], parts[1]
                            off = tz_map.get(abbr)
                            sign = 'Z' if off == 0 else ('+' if off > 0 else '-') + str(abs(off)).rjust(2,'0') + ':00'
                            iso_local = f"{date}T{time}:00" + (sign if sign == 'Z' else sign)
                            d = pd.to_datetime(iso_local.replace('Z','+00:00'), errors='coerce', utc=True)
                            if pd.notna(d):
                                return d.strftime('%Y-%m-%dT%H:%M:%SZ')
                        except Exception:
                            return None
                        return None
                    local_iso = df.apply(_iso_from_local, axis=1)
                    mask_ok = local_iso.notna()
                    if isinstance(mask_ok, pd.Series) and mask_ok.any():
                        df.loc[mask_ok, 'start_time_iso'] = local_iso[mask_ok]
            except Exception:
                pass
            # Per-row fallback: if any iso/local still missing (NaN), derive from raw start_time string best-effort
            if "start_time" in df.columns:
                st_str = df["start_time"].astype(str)
                # Build simple ISO guess from 'YYYY-MM-DD HH:MM' or 'YYYY-MM-DD'
                iso_guess = np.where(
                    st_str.str.contains("T"),
                    st_str,
                    st_str.str.replace(" ", "T", regex=False)
                )
                # Plus sign doesn't need escaping when regex=False
                iso_guess = iso_guess.str.replace("+00:00", "Z", regex=False)
                # Fill missing start_time_iso/local with guesses
                if "start_time_iso" in df.columns:
                    mask_iso_missing = df["start_time_iso"].isna() | (df["start_time_iso"].astype(str).str.strip()=="")
                    df.loc[mask_iso_missing, "start_time_iso"] = iso_guess[mask_iso_missing]
                else:
                    df["start_time_iso"] = iso_guess
                if "start_time_local" in df.columns:
                    mask_loc_missing = df["start_time_local"].isna() | (df["start_time_local"].astype(str).str.strip()=="")
                    disp = st_str.str.replace("T", " ", regex=False).str.replace(r":\d\d(\+\d\d:\d\d|Z)$", "", regex=True)
                    df.loc[mask_loc_missing, "start_time_local"] = disp[mask_loc_missing]
                else:
                    # Maintain backward-compat local string even if display conversion failed
                    df["start_time_local"] = st_str.str.replace("T", " ", regex=False).str.replace(r":\d\d(\+\d\d:\d\d|Z)$", "", regex=True)
        else:
            # Fallback path when `_start_dt` is missing: parse `start_time` as UTC and convert to display TZ for correct local rendering
            if "start_time" in df.columns:
                st_series = pd.to_datetime(df["start_time"], errors="coerce", utc=True)
                # Determine display timezone
                try:
                    disp_tz_name = _get_display_tz_name()
                except Exception:
                    disp_tz_name = os.getenv("DISPLAY_TZ") or os.getenv("SCHEDULE_TZ") or "America/New_York"
                try:
                    disp_tz = ZoneInfo(disp_tz_name)
                except Exception:
                    disp_tz = dt.datetime.now().astimezone().tzinfo
                # ISO in UTC
                try:
                    df["start_time_iso"] = st_series.dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                except Exception:
                    df["start_time_iso"] = df["start_time"].astype(str).str.replace(" ", "T", regex=False).str.replace("+00:00", "Z", regex=False)
                # If local fields present, prefer deriving ISO from local to avoid midnight drift
                try:
                    if {"start_time_local","start_tz_abbr"}.issubset(df.columns):
                        tz_map = {
                            'UTC': 0, 'Z': 0,
                            'HST': -10, 'AKST': -9,
                            'PST': -8, 'PDT': -7,
                            'MST': -7, 'MDT': -6,
                            'CST': -6, 'CDT': -5,
                            'EST': -5, 'EDT': -4,
                        }
                        def _iso_from_local2(row):
                            try:
                                loc = str(row.get('start_time_local') or '').strip()
                                abbr = str(row.get('start_tz_abbr') or '').upper().strip()
                                if not loc or abbr not in tz_map:
                                    return None
                                parts = loc.split(' ')
                                if len(parts) < 2:
                                    return None
                                date, time = parts[0], parts[1]
                                off = tz_map.get(abbr)
                                sign = 'Z' if off == 0 else ('+' if off > 0 else '-') + str(abs(off)).rjust(2,'0') + ':00'
                                iso_local = f"{date}T{time}:00" + (sign if sign == 'Z' else sign)
                                d = pd.to_datetime(iso_local.replace('Z','+00:00'), errors='coerce', utc=True)
                                if pd.notna(d):
                                    return d.strftime('%Y-%m-%dT%H:%M:%SZ')
                            except Exception:
                                return None
                            return None
                        local_iso2 = df.apply(_iso_from_local2, axis=1)
                        mask_ok2 = local_iso2.notna()
                        if isinstance(mask_ok2, pd.Series) and mask_ok2.any():
                            df.loc[mask_ok2, 'start_time_iso'] = local_iso2[mask_ok2]
                except Exception:
                    pass
                # Local display in end-user TZ
                try:
                    disp_series = st_series.dt.tz_convert(disp_tz)
                    df["start_time_local"] = disp_series.dt.strftime("%Y-%m-%d %H:%M")
                    df["start_tz_abbr"] = disp_series.dt.tzname()
                except Exception:
                    # As last resort, drop seconds and tz suffix
                    st_str = df["start_time"].astype(str)
                    df["start_time_local"] = st_str.str.replace("T", " ", regex=False).str.replace(r":\d\d(\+\d\d:\d\d|Z)$", "", regex=True)
                    df["start_tz_abbr"] = None
    except Exception:
        pass

    # Schedule vs display time comparison diagnostics (today/specified slate)
    # Compares our _start_dt (UTC) to schedule commence_time/start_time by unordered team pair.
    # Writes OUT/time_mismatch_<date>.csv and records counts to pipeline_stats.
    try:
        if (date_q or True) and isinstance(df, pd.DataFrame) and not df.empty and {"home_team","away_team"}.issubset(df.columns):
            # Build authoritative schedule for the date
            sched_candidates = []
            base = _safe_read_csv(OUT / 'games_curr.csv')
            if isinstance(base, pd.DataFrame) and not base.empty:
                sched_candidates.append(base)
            if date_q:
                for name in [f"games_{date_q}.csv", f"games_{date_q}_fused.csv"]:
                    p = OUT / name
                    if p.exists():
                        tmp = _safe_read_csv(p)
                        if isinstance(tmp, pd.DataFrame) and not tmp.empty:
                            sched_candidates.append(tmp)
            if sched_candidates:
                try:
                    sched_all = pd.concat(sched_candidates, ignore_index=True)
                except Exception:
                    sched_all = sched_candidates[0]
                # Filter by slate date using date or commence_time in schedule tz
                try:
                    import os
                    from zoneinfo import ZoneInfo
                    tz_name = os.getenv("SCHEDULE_TZ") or os.getenv("NCAAB_SCHEDULE_TZ") or "America/New_York"
                    try:
                        sched_tz = ZoneInfo(tz_name)
                    except Exception:
                        sched_tz = None
                except Exception:
                    sched_tz = None
                # Normalize date
                if 'date' in sched_all.columns:
                    try:
                        sched_all['date'] = pd.to_datetime(sched_all['date'], errors='coerce').dt.strftime('%Y-%m-%d')
                    except Exception:
                        pass
                # Compute commence slate
                commence_slate = None
                if 'commence_time' in sched_all.columns and sched_tz is not None:
                    try:
                        ct = pd.to_datetime(sched_all['commence_time'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                        commence_slate = ct.dt.tz_convert(sched_tz).dt.strftime('%Y-%m-%d')
                    except Exception:
                        commence_slate = None
                if date_q:
                    dq = str(date_q)
                else:
                    dq = _today_local().strftime('%Y-%m-%d')
                # Filter by either explicit date match or commence slate match
                try:
                    if commence_slate is not None:
                        mask = (sched_all.get('date').astype(str) == dq) | (commence_slate == dq)
                        sched_all = sched_all[mask]
                    else:
                        sched_all = sched_all[sched_all.get('date').astype(str) == dq]
                except Exception:
                    pass
                # Build pair keys and parse schedule start dt
                if not sched_all.empty:
                    sa = sched_all.copy()
                    sa['_home_norm'] = sa.get('home_team', sa.get('home')).astype(str).map(_canon_slug) if 'home_team' in sa.columns or 'home' in sa.columns else None
                    sa['_away_norm'] = sa.get('away_team', sa.get('away')).astype(str).map(_canon_slug) if 'away_team' in sa.columns or 'away' in sa.columns else None
                    try:
                        sa['_pair_key'] = sa.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
                    except Exception:
                        sa['_pair_key'] = None
                    # Schedule dt: prefer commence_time (UTC), else start_time localized to schedule tz then converted to UTC
                    sch_dt = None
                    if 'commence_time' in sa.columns:
                        try:
                            sch_dt = pd.to_datetime(sa['commence_time'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                        except Exception:
                            sch_dt = None
                    if (sch_dt is None or (isinstance(sch_dt, pd.Series) and sch_dt.isna().all())) and 'start_time' in sa.columns:
                        try:
                            st_raw = sa['start_time'].astype(str).str.strip().str.replace('Z','+00:00', regex=False)
                            has_off = st_raw.str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | st_raw.str.endswith('Z')
                            st_off = pd.to_datetime(st_raw.where(has_off, None), errors='coerce', utc=True)
                            st_naive = pd.to_datetime(st_raw.where(~has_off, None), errors='coerce', utc=False)
                            if sched_tz is not None and isinstance(st_naive, pd.Series) and st_naive.notna().any():
                                st_naive = st_naive.map(lambda x: x.replace(tzinfo=sched_tz) if pd.notna(x) else x)
                            sch_dt = st_off.where(st_off.notna(), st_naive)
                        except Exception:
                            sch_dt = None
                    sa['_sch_dt'] = sch_dt
                    # Our df keys and dt
                    loc = df.copy()
                    loc['_home_norm'] = loc['home_team'].astype(str).map(_canon_slug)
                    loc['_away_norm'] = loc['away_team'].astype(str).map(_canon_slug)
                    loc['_pair_key'] = loc.apply(lambda r: '::'.join(sorted([str(r.get('_home_norm')), str(r.get('_away_norm'))])), axis=1)
                    if '_start_dt' not in loc.columns:
                        loc['_start_dt'] = pd.NaT
                    # Compare and collect mismatches
                    rows = []
                    for pk, grp in loc.groupby('_pair_key'):
                        if not isinstance(pk, str) or not pk:
                            continue
                        sch_row = sa[sa['_pair_key'] == pk]
                        if sch_row.empty:
                            continue
                        # Choose one schedule row (latest/first)
                        sr = sch_row.iloc[-1]
                        our = grp.iloc[-1]
                        our_dt = our.get('_start_dt')
                        sch_dt_val = sr.get('_sch_dt')
                        try:
                            if pd.notna(our_dt) and pd.notna(sch_dt_val):
                                # Normalize both to UTC before diff
                                if hasattr(our_dt, 'tz_convert'):
                                    our_utc = our_dt.tz_convert(dt.timezone.utc)
                                else:
                                    our_utc = pd.to_datetime(our_dt, errors='coerce', utc=True)
                                if hasattr(sch_dt_val, 'tz_convert'):
                                    sch_utc = sch_dt_val.tz_convert(dt.timezone.utc)
                                else:
                                    sch_utc = pd.to_datetime(sch_dt_val, errors='coerce', utc=True)
                                diff_min = abs((our_utc - sch_utc).total_seconds()) / 60.0
                            else:
                                diff_min = None
                        except Exception:
                            diff_min = None
                        rows.append({
                            'pair_key': pk,
                            'home_team': our.get('home_team'),
                            'away_team': our.get('away_team'),
                            # Use UTC-only fields for examples to avoid display-vs-UTC artifacts
                            'our_start_iso': our.get('start_time_iso'),
                            'our_start_dt': our_dt,
                            'sched_commence_time': sr.get('commence_time'),
                            'sched_start_time': sr.get('start_time'),
                            'sched_dt': sch_dt_val,
                            'diff_minutes': diff_min,
                        })
                    if rows:
                        mm = pd.DataFrame(rows)
                        # Significant mismatches
                        try:
                            mismatch_mask = mm['diff_minutes'].astype(float) > 1.0
                        except Exception:
                            mismatch_mask = pd.Series([False]*len(mm))
                        mismatches = mm[mismatch_mask]
                        pipeline_stats['time_mismatch_count'] = int(len(mismatches))
                        pipeline_stats['time_compare_rows'] = int(len(mm))
                        try:
                            pipeline_stats['time_mismatch_examples'] = mismatches.head(5)[['home_team','away_team','our_start_iso','sched_commence_time','sched_start_time','diff_minutes']].to_dict(orient='records')
                        except Exception:
                            pass
                        # Persist report
                        try:
                            outp = OUT / f"time_mismatch_{dq}.csv"
                            mm.to_csv(outp, index=False)
                            pipeline_stats['time_mismatch_report'] = str(outp)
                        except Exception:
                            pipeline_stats['time_mismatch_write_error'] = True
    except Exception:
        try:
            pipeline_stats['time_compare_error'] = True
        except Exception:
            pass

    # Sanitize mismatch examples: ensure UTC-only fields are used (overwrite any legacy display-based examples)
    try:
        if isinstance(df, pd.DataFrame) and not df.empty and '_start_dt' in df.columns:
            try:
                start_iso = pd.to_datetime(df['_start_dt'], errors='coerce', utc=True).dt.strftime('%Y-%m-%dT%H:%M:%SZ')
            except Exception:
                start_iso = df.get('start_time_iso')
            ex_rows = pd.DataFrame({
                'home_team': df.get('home_team'),
                'away_team': df.get('away_team'),
                'our_start_iso': start_iso,
            }).dropna()
            if len(ex_rows):
                pipeline_stats['time_mismatch_examples'] = ex_rows.head(5).to_dict(orient='records')
    except Exception:
        pipeline_stats['time_mismatch_sanitize_error'] = True

    # Venue and status sanitization/coalescing
    try:
        # Treat common NaN-like strings as missing
        def _clean_str(s):
            if s is None:
                return None
            try:
                s2 = str(s).strip()
            except Exception:
                return None
            if s2 == "" or s2.lower() in {"nan", "none", "nat", "null"}:
                return None
            return s2
        cand_cols = [c for c in ["venue", "venue_full", "arena", "stadium", "site_name", "site", "location"] if c in df.columns]
        # Build venue from first non-empty candidate; if none, fall back to City, State when available
        if cand_cols or ("city" in df.columns or "state" in df.columns):
            venues: list[str | None] = []
            for _, r in df.iterrows():
                # Primary candidates
                val = None
                for c in cand_cols:
                    v = _clean_str(r.get(c))
                    if v:
                        val = v
                        break
                # Fallback to City, State
                if not val:
                    city = _clean_str(r.get("city")) if "city" in df.columns else None
                    state = _clean_str(r.get("state")) if "state" in df.columns else None
                    if city and state:
                        val = f"{city}, {state}"
                    elif city:
                        val = city
                    elif state:
                        val = state
                venues.append(val)
            df["venue"] = venues
        # Clean status string to avoid printing literal 'nan'
        if "status" in df.columns:
            df["status"] = df["status"].map(_clean_str)
        # Clean start_time_local text if any literal 'nan'
        if "start_time_local" in df.columns:
            df["start_time_local"] = df["start_time_local"].map(_clean_str)
    except Exception:
        pass

    # Halves outcomes vs market (postgame)
    try:
        # Compute actual half totals if scores present
        if {"home_score_1h","away_score_1h"}.issubset(df.columns):
            hs1 = pd.to_numeric(df["home_score_1h"], errors="coerce")
            as1 = pd.to_numeric(df["away_score_1h"], errors="coerce")
            df["actual_total_1h"] = np.where(hs1.notna() & as1.notna(), hs1 + as1, df.get("actual_total_1h"))
            df["actual_margin_1h"] = np.where(hs1.notna() & as1.notna(), hs1 - as1, np.nan)
        if {"home_score_2h","away_score_2h"}.issubset(df.columns):
            hs2 = pd.to_numeric(df["home_score_2h"], errors="coerce")
            as2 = pd.to_numeric(df["away_score_2h"], errors="coerce")
            df["actual_total_2h"] = np.where(hs2.notna() & as2.notna(), hs2 + as2, df.get("actual_total_2h"))
            df["actual_margin_2h"] = np.where(hs2.notna() & as2.notna(), hs2 - as2, np.nan)
        # OU results for halves
        if {"actual_total_1h","market_total_1h"}.issubset(df.columns):
            at1 = pd.to_numeric(df["actual_total_1h"], errors="coerce")
            mt1 = pd.to_numeric(df["market_total_1h"], errors="coerce")
            ou1 = np.where(at1 > mt1, "Over", np.where(at1 < mt1, "Under", "Push"))
            df.loc[at1.notna() & mt1.notna(), "ou_result_1h"] = ou1[at1.notna() & mt1.notna()]
            # Eval vs model lean
            if "lean_ou_side_1h" in df.columns:
                l1 = df["lean_ou_side_1h"].astype(str)
                r1 = df["ou_result_1h"].astype(str)
                ok1 = np.where((r1=="Push") | (l1=="None") | (l1=="nan") | (l1==""), np.nan, (l1==r1))
                df.loc[:, "eval_ou_1h_ok"] = ok1
        if {"actual_total_2h","market_total_2h"}.issubset(df.columns):
            at2 = pd.to_numeric(df["actual_total_2h"], errors="coerce")
            mt2 = pd.to_numeric(df["market_total_2h"], errors="coerce")
            ou2 = np.where(at2 > mt2, "Over", np.where(at2 < mt2, "Under", "Push"))
            df.loc[at2.notna() & mt2.notna(), "ou_result_2h"] = ou2[at2.notna() & mt2.notna()]
            if "lean_ou_side_2h" in df.columns:
                l2 = df["lean_ou_side_2h"].astype(str)
                r2 = df["ou_result_2h"].astype(str)
                ok2 = np.where((r2=="Push") | (l2=="None") | (l2=="nan") | (l2==""), np.nan, (l2==r2))
                df.loc[:, "eval_ou_2h_ok"] = ok2
        # ATS results for halves when 1H/2H spreads exist (including derived spreads)
        if {"actual_margin_1h","spread_home_1h"}.issubset(df.columns):
            am1 = pd.to_numeric(df["actual_margin_1h"], errors="coerce")
            sh1 = pd.to_numeric(df["spread_home_1h"], errors="coerce")
            ats1 = np.where(am1 > -sh1, "Home Cover", np.where(am1 < -sh1, "Away Cover", "Push"))
            df.loc[am1.notna() & sh1.notna(), "ats_result_1h"] = ats1[am1.notna() & sh1.notna()]
            if "lean_ats_side_1h" in df.columns:
                l1a = np.where(df["lean_ats_side_1h"].astype(str)=="Home ATS", "Home Cover", np.where(df["lean_ats_side_1h"].astype(str)=="Away ATS", "Away Cover", ""))
                r1a = df["ats_result_1h"].astype(str)
                ok1a = np.where((r1a=="Push") | (l1a==""), np.nan, (l1a==r1a))
                df.loc[:, "eval_ats_1h_ok"] = ok1a
        if {"actual_margin_2h","spread_home_2h"}.issubset(df.columns):
            am2 = pd.to_numeric(df["actual_margin_2h"], errors="coerce")
            sh2 = pd.to_numeric(df["spread_home_2h"], errors="coerce")
            ats2 = np.where(am2 > -sh2, "Home Cover", np.where(am2 < -sh2, "Away Cover", "Push"))
            df.loc[am2.notna() & sh2.notna(), "ats_result_2h"] = ats2[am2.notna() & sh2.notna()]
            if "lean_ats_side_2h" in df.columns:
                l2a = np.where(df["lean_ats_side_2h"].astype(str)=="Home ATS", "Home Cover", np.where(df["lean_ats_side_2h"].astype(str)=="Away ATS", "Away Cover", ""))
                r2a = df["ats_result_2h"].astype(str)
                ok2a = np.where((r2a=="Push") | (l2a==""), np.nan, (l2a==r2a))
                df.loc[:, "eval_ats_2h_ok"] = ok2a
        # Winner correctness for halves (vs predicted winner if available)
        try:
            if {"pred_winner_1h","actual_margin_1h"}.issubset(df.columns):
                pm = df["pred_winner_1h"].astype(str)
                am = pd.to_numeric(df["actual_margin_1h"], errors="coerce")
                act = np.where(am>0, "Home", np.where(am<0, "Away", "Even"))
                ok = np.where((act=="Even") | (pm=="Even") | pm.isna(), np.nan, (pm==act))
                df.loc[:, "eval_winner_1h_ok"] = ok
            if {"pred_winner_2h","actual_margin_2h"}.issubset(df.columns):
                pm2 = df["pred_winner_2h"].astype(str)
                am2n = pd.to_numeric(df["actual_margin_2h"], errors="coerce")
                act2 = np.where(am2n>0, "Home", np.where(am2n<0, "Away", "Even"))
                ok2 = np.where((act2=="Even") | (pm2=="Even") | pm2.isna(), np.nan, (pm2==act2))
                df.loc[:, "eval_winner_2h_ok"] = ok2
        except Exception:
            pass
    except Exception:
        pass

    # Final results & betting outcome reconciliation (ATS & ML) when scores available
    try:
        if {"home_score","away_score"}.issubset(df.columns):
            hs = pd.to_numeric(df["home_score"], errors="coerce")
            as_ = pd.to_numeric(df["away_score"], errors="coerce")
            mask_done = (hs > 0) | (as_ > 0)
            if mask_done.any():
                # Actual totals/margins
                actual_margin = hs - as_
                df.loc[mask_done, "actual_margin"] = actual_margin[mask_done]
                df.loc[mask_done, "actual_total"] = (hs + as_)[mask_done]
                if "spread_home" in df.columns:
                    line = pd.to_numeric(df["spread_home"], errors="coerce")
                    # Home covers if actual margin > -line (because spread_home is home line)
                    ats_res = np.where(actual_margin > -line, "Home Cover", np.where(actual_margin < -line, "Away Cover", "Push"))
                    df.loc[mask_done, "ats_result"] = ats_res[mask_done]
                if "closing_spread_home" in df.columns:
                    c_line = pd.to_numeric(df["closing_spread_home"], errors="coerce")
                    ats_close_res = np.where(actual_margin > -c_line, "Home Cover", np.where(actual_margin < -c_line, "Away Cover", "Push"))
                    df.loc[mask_done, "ats_close_result"] = ats_close_res[mask_done]
                ml_res = np.where(actual_margin > 0, "Home Win", np.where(actual_margin < 0, "Away Win", "Push"))
                df.loc[mask_done, "ml_result"] = ml_res[mask_done]

                # OU reconciliation (full game) vs market and closing
                at = pd.to_numeric(df.get("actual_total"), errors="coerce")
                if {"market_total"}.issubset(df.columns):
                    mt = pd.to_numeric(df.get("market_total"), errors="coerce")
                    ou_full = np.where(at > mt, "Over", np.where(at < mt, "Under", "Push"))
                    df.loc[mask_done & at.notna() & mt.notna(), "ou_result_full"] = ou_full[mask_done & at.notna() & mt.notna()]
                if {"closing_total"}.issubset(df.columns):
                    ct = pd.to_numeric(df.get("closing_total"), errors="coerce")
                    ouc_full = np.where(at > ct, "Over", np.where(at < ct, "Under", "Push"))
                    df.loc[mask_done & at.notna() & ct.notna(), "ou_close_result_full"] = ouc_full[mask_done & at.notna() & ct.notna()]

                # Evaluation flags: model lean vs actual outcome
                # OU vs Market: compare lean_ou_side to ou_result_full
                if {"lean_ou_side","ou_result_full"}.issubset(df.columns):
                    l = df["lean_ou_side"].astype(str)
                    r = df["ou_result_full"].astype(str)
                    ok = np.where((r=="Push") | (l=="None") | (l=="nan") | (l==""), np.nan, (l==r))
                    df.loc[:, "eval_ou_ok"] = ok
                # OU vs Close
                if {"lean_ou_side","ou_close_result_full"}.issubset(df.columns):
                    l = df["lean_ou_side"].astype(str)
                    r = df["ou_close_result_full"].astype(str)
                    ok = np.where((r=="Push") | (l=="None") | (l=="nan") | (l==""), np.nan, (l==r))
                    df.loc[:, "eval_ou_close_ok"] = ok
                # ATS vs actual: map lean_ats_side (Home ATS/Away ATS) to covers
                if {"lean_ats_side","ats_result"}.issubset(df.columns):
                    l = df["lean_ats_side"].astype(str)
                    r = df["ats_result"].astype(str)
                    # Map Home ATS -> Home Cover; Away ATS -> Away Cover
                    l_map = np.where(l=="Home ATS", "Home Cover", np.where(l=="Away ATS", "Away Cover", ""))
                    ok = np.where((r=="Push") | (l_map==""), np.nan, (l_map==r))
                    df.loc[:, "eval_ats_ok"] = ok
                # ATS vs closing (optional) if closing spread present and we computed ats_close_result and edge_closing_ats
                if {"edge_closing_ats","ats_close_result"}.issubset(df.columns):
                    e = pd.to_numeric(df["edge_closing_ats"], errors="coerce")
                    l2 = np.where(e>0, "Home Cover", np.where(e<0, "Away Cover", ""))
                    r2 = df["ats_close_result"].astype(str)
                    ok2 = np.where((r2=="Push") | (l2==""), np.nan, (l2==r2))
                    df.loc[:, "eval_ats_close_ok"] = ok2
                # ML winner: sign of pred_margin vs ml_result
                if {"pred_margin","ml_result"}.issubset(df.columns):
                    pm = pd.to_numeric(df["pred_margin"], errors="coerce")
                    lml = np.where(pm>0, "Home Win", np.where(pm<0, "Away Win", "Push"))
                    rml = df["ml_result"].astype(str)
                    okml = np.where((lml=="Push") | (rml=="Push"), np.nan, (lml==rml))
                    df.loc[:, "eval_ml_ok"] = okml
    except Exception:
        pass

    # Reformat date back to string for template
    if "date" in df.columns:
        try:
            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
        except Exception:
            pass

    # Small safeguard: warn if predictions look uniform (little variance)
    uniform_note = None
    if "pred_total" in df.columns and len(df) >= 5:
        try:
            stdv = float(pd.to_numeric(df["pred_total"], errors="coerce").dropna().std())
            if stdv < 1e-3:
                uniform_note = (
                    "Predictions appear uniform. This often happens at season start when rolling features are empty. "
                    "Consider seeding preseason ratings (evaluate-last2) and re-running daily-run."
                )
        except Exception:
            pass

    # Snapshot for remote CAL debugging: persist last enriched DataFrame & pipeline stats
    try:
        global last_index_df, last_index_pipeline_stats
        # Shallow copy sufficient; avoid deep to reduce memory
        last_index_df = df.copy()
        last_index_pipeline_stats = dict(pipeline_stats) if 'pipeline_stats' in locals() else {}
    except Exception:
        pass

    # Persist enforced display predictions for alignment across environments
    try:
        persist_date = None
        if 'date_q' in locals() and date_q:  # type: ignore
            persist_date = str(date_q)
        else:
            persist_date = datetime.utcnow().strftime('%Y-%m-%d')  # type: ignore
        # ESPN subset/date filter before persisting display file
        try:
            # Date filter on persisted artifact
            if isinstance(df, pd.DataFrame) and not df.empty and persist_date:
                if 'date' in df.columns:
                    try:
                        df['date'] = df['date'].astype(str)
                        df = df[df['date'] == str(persist_date)]
                    except Exception:
                        pass
                elif 'start_time' in df.columns:
                    try:
                        st_str_pd = df['start_time'].astype(str)
                        st_date_pd = st_str_pd.str.slice(0, 10)
                        df = df[st_date_pd == str(persist_date)]
                    except Exception:
                        pass
            # ESPN subset restriction
            try:
                subset_path_pd = OUT / f'schedule_espn_subset_{persist_date}.json'
                if subset_path_pd.exists():
                    import json as _json
                    payload_pd = _json.loads(subset_path_pd.read_text(encoding='utf-8'))
                    subset_ids_pd: set[str] = set()
                    if isinstance(payload_pd, dict):
                        for k in ('ids','espn_ids','game_ids'):
                            v = payload_pd.get(k)
                            if isinstance(v, list) and v:
                                subset_ids_pd = {str(x) for x in v}
                                break
                    elif isinstance(payload_pd, list):
                        subset_ids_pd = {str(x) for x in payload_pd}
                    if subset_ids_pd and 'game_id' in df.columns:
                        df['game_id'] = df['game_id'].astype(str)
                        df = df[df['game_id'].isin(subset_ids_pd)].reset_index(drop=True)
            except Exception:
                pass
        except Exception:
            pass
        cols_keep = []
        base_cols = [
            'game_id',
            'home_team',
            'away_team',
            'pred_total',
            'pred_margin',
            'pred_total_basis',
            'pred_margin_basis',
            'edge_total',
            'edge_ats',
            'closing_total',
            'closing_spread_home',
            # Display-related fields to support stable Cards UI
            'display_date',
            'display_time_str',
            'start_time_display',
            'start_time_local',
        ]
        for c in base_cols:
            if c in df.columns:
                cols_keep.append(c)
        # Normalize basis columns before persistence to minimize remote vs local display mismatches
        try:
            if 'pred_total_basis' in df.columns:
                # If calibrated column equals displayed total but basis missing, tag as cal
                if 'pred_total_calibrated' in df.columns:
                    mask_cal = df['pred_total_basis'].isna() & (pd.to_numeric(df['pred_total'], errors='coerce') == pd.to_numeric(df['pred_total_calibrated'], errors='coerce'))
                    if mask_cal.any():
                        df.loc[mask_cal,'pred_total_basis'] = 'cal'
                # Model raw: if pred_total equals pred_total_model and basis missing
                if 'pred_total_model' in df.columns:
                    mask_model = df['pred_total_basis'].isna() & (pd.to_numeric(df['pred_total'], errors='coerce') == pd.to_numeric(df['pred_total_model'], errors='coerce'))
                    if mask_model.any():
                        df.loc[mask_model,'pred_total_basis'] = df.get('pred_total_model_basis','model_raw') if 'pred_total_model_basis' in df.columns else 'model_raw'
                # Fill any remaining missing with 'unknown'
                mask_unknown = df['pred_total_basis'].isna()
                if mask_unknown.any():
                    df.loc[mask_unknown,'pred_total_basis'] = 'unknown'
            if 'pred_margin_basis' in df.columns:
                if 'pred_margin_calibrated' in df.columns:
                    mask_cal_m = df['pred_margin_basis'].isna() & (pd.to_numeric(df['pred_margin'], errors='coerce') == pd.to_numeric(df['pred_margin_calibrated'], errors='coerce'))
                    if mask_cal_m.any():
                        df.loc[mask_cal_m,'pred_margin_basis'] = 'cal'
                if 'pred_margin_model' in df.columns:
                    mask_model_m = df['pred_margin_basis'].isna() & (pd.to_numeric(df['pred_margin'], errors='coerce') == pd.to_numeric(df['pred_margin_model'], errors='coerce'))
                    if mask_model_m.any():
                        df.loc[mask_model_m,'pred_margin_basis'] = df.get('pred_margin_model_basis','model') if 'pred_margin_model_basis' in df.columns else 'model'
                mask_unknown_m = df['pred_margin_basis'].isna()
                if mask_unknown_m.any():
                    df.loc[mask_unknown_m,'pred_margin_basis'] = 'unknown'
        except Exception as _norm_e:
            pipeline_stats['basis_normalize_error'] = str(_norm_e)[:120]
        disp_path = OUT / f'predictions_display_{persist_date}.csv'
        if cols_keep:
            df[cols_keep].to_csv(disp_path, index=False)
            pipeline_stats['display_persist_rows'] = int(len(df))
            pipeline_stats['display_persist_path'] = str(disp_path)
            # Compute and record content hash for alignment diagnostics
            try:
                hdf = df[cols_keep].copy()
                if 'game_id' in hdf.columns:
                    hdf = hdf.sort_values('game_id')
                blob = '\n'.join([
                    ','.join(map(str, [row.get(col, '') for col in ['game_id','pred_total','pred_margin'] if col in hdf.columns]))
                    for _, row in hdf.iterrows()
                ])
                # Use previously imported _hashlib_mod to avoid duplicate import names
                pipeline_stats['display_hash'] = _hashlib_mod.sha256(blob.encode()).hexdigest()
            except Exception:
                pipeline_stats['display_hash_error'] = True
            # Archive to dated folder for historical navigation
            try:
                archive_dir = OUT / 'archive' / str(persist_date)
                archive_dir.mkdir(parents=True, exist_ok=True)
                archive_path = archive_dir / f'predictions_display_{persist_date}.csv'
                df[cols_keep].to_csv(archive_path, index=False)
                pipeline_stats['display_archive_path'] = str(archive_path)
                pipeline_stats['display_archive_ok'] = True
            except Exception as _arch_e:
                pipeline_stats['display_archive_error'] = str(_arch_e)[:160]
    except Exception as _persist_e:
        pipeline_stats['display_persist_error'] = str(_persist_e)[:160]

    # Basis share alerts & drift detection (simple thresholds)
    try:
        import os, json
        cal_thr = float(os.getenv('CAL_SHARE_MIN', '0.60'))
        bt_cal = pipeline_stats.get('basis_share_total_cal')
        bm_cal = pipeline_stats.get('basis_share_margin_cal')
        if isinstance(bt_cal, (int,float)) and bt_cal < cal_thr:
            pipeline_stats['alert_low_cal_total'] = bt_cal
        if isinstance(bm_cal, (int,float)) and bm_cal < cal_thr:
            pipeline_stats['alert_low_cal_margin'] = bm_cal
        # Drift vs prior 7 days (load previous display files)
        past_files = sorted([p for p in OUT.glob('predictions_display_*.csv') if p.name != f'predictions_display_{persist_date}.csv'])[-7:]
        past_cal_shares_t: list[float] = []
        past_cal_shares_m: list[float] = []
        for pf in past_files:
            try:
                dpf = pd.read_csv(pf)
                if 'pred_total_basis' in dpf.columns:
                    past_cal_shares_t.append(float((dpf['pred_total_basis'].astype(str)=='cal').mean()))
                if 'pred_margin_basis' in dpf.columns:
                    past_cal_shares_m.append(float((dpf['pred_margin_basis'].astype(str)=='cal').mean()))
            except Exception:
                pass
        import statistics
        if past_cal_shares_t and isinstance(bt_cal,(int,float)):
            med_t = statistics.median(past_cal_shares_t)
            pipeline_stats['drift_total_median_7d'] = med_t
            pipeline_stats['drift_total_delta_vs_median'] = bt_cal - med_t
        if past_cal_shares_m and isinstance(bm_cal,(int,float)):
            med_m = statistics.median(past_cal_shares_m)
            pipeline_stats['drift_margin_median_7d'] = med_m
            pipeline_stats['drift_margin_delta_vs_median'] = bm_cal - med_m
        # Deployment gate: block if last 3 days calibrated share below threshold
        try:
            recent_files = sorted([p for p in OUT.glob('predictions_display_*.csv')])[-3:]
            low_days: list[str] = []
            for pf in recent_files:
                try:
                    dpf = pd.read_csv(pf)
                    share = float((dpf['pred_total_basis'].astype(str)=='cal').mean()) if 'pred_total_basis' in dpf.columns else None
                    if share is not None and share < cal_thr:
                        low_days.append(pf.stem.replace('predictions_display_',''))
                except Exception:
                    pass
            if len(low_days) == len(recent_files) and recent_files:
                pipeline_stats['deployment_gate_blocked'] = True
                pipeline_stats['deployment_gate_low_days'] = low_days
                pipeline_stats['deployment_gate_threshold'] = cal_thr
        except Exception:
            pipeline_stats['deployment_gate_error'] = True
    except Exception as _drift_e:
        pipeline_stats['basis_alert_drift_error'] = str(_drift_e)[:160]

    # Inline diagnostic coverage summary (reason counts) added to pipeline_stats
    try:
        from src.diagnose_calibration import diagnose  # type: ignore
        diag_date = None
        if 'date_q' in locals() and date_q:  # type: ignore
            diag_date = str(date_q)
        diag_df_inline = diagnose(diag_date)
        if not diag_df_inline.empty:
            rc = diag_df_inline.groupby('reason').size().to_dict()
            pipeline_stats['calibration_reason_counts'] = rc
            pipeline_stats['calibration_reason_total_games'] = int(len(diag_df_inline))
            pipeline_stats['calibration_reason_missing_model'] = rc.get('missing_model_prediction', 0)
            pipeline_stats['calibration_reason_missing_artifact'] = rc.get('missing_calibration_artifact', 0)
    except Exception as _diag_e:
        pipeline_stats['calibration_inline_diag_error'] = str(_diag_e)[:160]



    # Branding enrichment
    branding = _load_branding_map()
    # Precompute pure CSS (no Jinja loops in template) for team colors
    css_lines: list[str] = []
    for key, b in branding.items():
        primary = b.get("primary") or b.get("secondary")
        text = b.get("text") or "#ffffff"
        if primary:
            css_lines.append(f".badge-team.k-{key}{{background:{primary};}}")
        if text:
            css_lines.append(f".badge-team.k-{key} .name{{color:{text};}}")
    dynamic_css = "\n".join(css_lines)
    def _brand_row(row: dict[str, Any]) -> dict[str, Any]:
        for side in ["home", "away"]:
            raw_name = str(row.get(f"{side}_team") or "")
            try:
                # Prefer canonical slug that applies custom team_map overrides; fallback to base normalize_name
                tkey = _canon_slug(raw_name)  # type: ignore
            except Exception:
                tkey = normalize_name(raw_name)
            # Attempt branding lookup with canonical key first; fallback to normalized raw if absent
            b = branding.get(tkey) or branding.get(normalize_name(raw_name)) or {}
            row[f"{side}_key"] = tkey
            row[f"{side}_logo"] = b.get("logo")
            row[f"{side}_color"] = b.get("primary") or b.get("secondary") or None
            row[f"{side}_text_color"] = b.get("text") or "#ffffff"
        return row
    # Ensure derivative/odds fields exist to avoid template UndefinedError on missing keys
    try:
        required_cols = [
            # Full game
            "spread_home", "market_total", "pred_total", "pred_margin", "proj_home", "proj_away",
            "ats_result", "actual_total", "ml_result", "closing_total", "closing_spread_home",
            "edge_closing_ats", "edge_total", "lean_ou_side", "edge_ats", "favored_side", "favored_by",
            # 1st half
            "pred_total_1h", "pred_margin_1h", "proj_home_1h", "proj_away_1h", "pred_winner_1h",
            "market_total_1h", "spread_home_1h", "ats_result_1h", "actual_total_1h", "home_score_1h", "away_score_1h",
            "spread_home_1h_basis", "market_total_1h_basis",
            # 2nd half
            "pred_total_2h", "pred_margin_2h", "proj_home_2h", "proj_away_2h", "pred_winner_2h",
            "market_total_2h", "spread_home_2h", "ats_result_2h", "actual_total_2h", "home_score_2h", "away_score_2h",
            "spread_home_2h_basis", "market_total_2h_basis",
        ]
        # Half moneylines optional columns for template guards
        extra_opt_cols = [
            "ml_home_1h", "ml_away_1h", "ml_home_2h", "ml_away_2h"
        ]
        for c in required_cols:
            if c not in df.columns:
                df[c] = None
        for c in extra_opt_cols:
            if c not in df.columns:
                df[c] = None
    except Exception:
        pass

    # Ensure half predictions/spreads and ATS results are populated (final pass) before template conversion.
    try:
        # Final safety net: if any half prediction values are still missing, derive from full-game preds
        if {"pred_total","pred_margin"}.issubset(df.columns):
            half_ratio = 0.485
            pt = pd.to_numeric(df.get("pred_total"), errors="coerce")
            pm = pd.to_numeric(df.get("pred_margin"), errors="coerce")
            # Create columns if missing
            for col in ["pred_total_1h","pred_total_2h","pred_margin_1h","pred_margin_2h"]:
                if col not in df.columns:
                    df[col] = np.nan
            # 1H total from ratio for any NaN rows
            need_pt1 = pd.to_numeric(df.get("pred_total_1h"), errors="coerce").isna()
            if need_pt1.any():
                df.loc[need_pt1, "pred_total_1h"] = (pt * half_ratio)[need_pt1]
            # 2H total as remainder for any NaN rows
            need_pt2 = pd.to_numeric(df.get("pred_total_2h"), errors="coerce").isna()
            if need_pt2.any():
                # use freshly filled 1H where available
                pt1_now = pd.to_numeric(df.get("pred_total_1h"), errors="coerce")
                df.loc[need_pt2, "pred_total_2h"] = (pt - pt1_now)[need_pt2]
            # Half margins default to even split when missing
            need_pm1 = pd.to_numeric(df.get("pred_margin_1h"), errors="coerce").isna()
            if need_pm1.any():
                df.loc[need_pm1, "pred_margin_1h"] = (pm * 0.5)[need_pm1]
            need_pm2 = pd.to_numeric(df.get("pred_margin_2h"), errors="coerce").isna()
            if need_pm2.any():
                # use freshly filled 1H where available
                pm1_now = pd.to_numeric(df.get("pred_margin_1h"), errors="coerce")
                df.loc[need_pm2, "pred_margin_2h"] = (pm - pm1_now)[need_pm2]

        # Team 1H/2H projections safety net: derive proportionally if still missing
        if {"proj_home","proj_away","pred_total_1h","pred_total_2h"}.issubset(df.columns):
            ph = pd.to_numeric(df.get("proj_home"), errors="coerce")
            pa = pd.to_numeric(df.get("proj_away"), errors="coerce")
            t1 = pd.to_numeric(df.get("pred_total_1h"), errors="coerce")
            t2 = pd.to_numeric(df.get("pred_total_2h"), errors="coerce")
            for col in ["proj_home_1h","proj_away_1h","proj_home_2h","proj_away_2h"]:
                if col not in df.columns:
                    df[col] = np.nan
            need_ph1 = pd.to_numeric(df.get("proj_home_1h"), errors="coerce").isna()
            need_pa1 = pd.to_numeric(df.get("proj_away_1h"), errors="coerce").isna()
            need_ph2 = pd.to_numeric(df.get("proj_home_2h"), errors="coerce").isna()
            need_pa2 = pd.to_numeric(df.get("proj_away_2h"), errors="coerce").isna()
            full = ph + pa
            share_home = np.where((ph.notna()) & (full.notna()) & (full > 0), ph / full, 0.5)
            if need_ph1.any():
                df.loc[need_ph1, "proj_home_1h"] = (share_home * t1)[need_ph1]
            if need_pa1.any():
                df.loc[need_pa1, "proj_away_1h"] = ((1.0 - share_home) * t1)[need_pa1]
            if need_ph2.any():
                df.loc[need_ph2, "proj_home_2h"] = (share_home * t2)[need_ph2]
            if need_pa2.any():
                df.loc[need_pa2, "proj_away_2h"] = ((1.0 - share_home) * t2)[need_pa2]

        # Derive half totals if provider halves are missing but full-game total exists
        half_ratio = 0.485
        if "market_total" in df.columns:
            mt_full = pd.to_numeric(df["market_total"], errors="coerce")
            # 1H total
            if ("market_total_1h" not in df.columns) or df.get("market_total_1h").isna().all():
                df["market_total_1h"] = np.where(mt_full.notna(), mt_full * half_ratio, df.get("market_total_1h"))
                # mark that these were derived when no provider 1H line exists
                df["market_total_1h_basis"] = np.where(mt_full.notna(), "derived", df.get("market_total_1h_basis"))
            # 2H total derived as remainder from full game
            if ("market_total_2h" not in df.columns) or df.get("market_total_2h").isna().all():
                # Prefer existing 1H (provider or derived) then subtract from full
                mt1 = pd.to_numeric(df.get("market_total_1h"), errors="coerce") if "market_total_1h" in df.columns else pd.Series(np.nan, index=df.index)
                df["market_total_2h"] = np.where(mt_full.notna() & mt1.notna(), mt_full - mt1, np.where(mt_full.notna(), mt_full * (1.0 - half_ratio), df.get("market_total_2h")))
                df["market_total_2h_basis"] = np.where(mt_full.notna(), "derived", df.get("market_total_2h_basis"))
            # Recompute half OU edges if we just filled totals
            try:
                if {"pred_total_1h","market_total_1h"}.issubset(df.columns):
                    pt1 = pd.to_numeric(df["pred_total_1h"], errors="coerce")
                    mt1 = pd.to_numeric(df["market_total_1h"], errors="coerce")
                    need = ("edge_total_1h" not in df.columns) or df["edge_total_1h"].isna()
                    df["edge_total_1h"] = np.where(need, pt1 - mt1, df.get("edge_total_1h"))
                if {"pred_total_2h","market_total_2h"}.issubset(df.columns):
                    pt2 = pd.to_numeric(df["pred_total_2h"], errors="coerce")
                    mt2 = pd.to_numeric(df["market_total_2h"], errors="coerce")
                    need2 = ("edge_total_2h" not in df.columns) or df["edge_total_2h"].isna()
                    df["edge_total_2h"] = np.where(need2, pt2 - mt2, df.get("edge_total_2h"))
            except Exception:
                pass
        # Derive half spreads if still missing or all NaN.
        if "spread_home" in df.columns:
            sh_full = pd.to_numeric(df["spread_home"], errors="coerce")
            # 1H
            if ("spread_home_1h" not in df.columns) or df["spread_home_1h"].isna().all():
                df["spread_home_1h"] = np.where(sh_full.notna(), sh_full * 0.5, np.nan)
                df["spread_home_1h_basis"] = np.where(sh_full.notna(), "derived2", df.get("spread_home_1h_basis"))
            # 2H
            if ("spread_home_2h" not in df.columns) or df["spread_home_2h"].isna().all():
                base_1h = pd.to_numeric(df.get("spread_home_1h"), errors="coerce") if "spread_home_1h" in df.columns else (sh_full * 0.5)
                df["spread_home_2h"] = np.where((sh_full.notna()) & (base_1h.notna()), sh_full - base_1h, np.nan)
                df["spread_home_2h_basis"] = np.where(sh_full.notna(), "derived2", df.get("spread_home_2h_basis"))
        # Derive half scores (2H) if missing but 1H + final available
        if {"home_score","away_score","home_score_1h","away_score_1h"}.issubset(df.columns):
            hs = pd.to_numeric(df["home_score"], errors="coerce")
            as_ = pd.to_numeric(df["away_score"], errors="coerce")
            h1 = pd.to_numeric(df["home_score_1h"], errors="coerce")
            a1 = pd.to_numeric(df["away_score_1h"], errors="coerce")
            need_2h_home = ("home_score_2h" not in df.columns) or df["home_score_2h"].isna().all()
            need_2h_away = ("away_score_2h" not in df.columns) or df["away_score_2h"].isna().all()
            if need_2h_home:
                df["home_score_2h"] = np.where(hs.notna() & h1.notna(), hs - h1, np.nan)
            if need_2h_away:
                df["away_score_2h"] = np.where(as_.notna() & a1.notna(), as_ - a1, np.nan)
        # Compute half ATS results if still missing
        if {"home_score_1h","away_score_1h","spread_home_1h"}.issubset(df.columns):
            hs1 = pd.to_numeric(df["home_score_1h"], errors="coerce")
            as1 = pd.to_numeric(df["away_score_1h"], errors="coerce")
            sh1 = pd.to_numeric(df["spread_home_1h"], errors="coerce")
            am1 = hs1 - as1
            mask1 = hs1.notna() & as1.notna() & sh1.notna()
            ats1 = np.where(am1 > -sh1, "Home Cover", np.where(am1 < -sh1, "Away Cover", "Push"))
            if ("ats_result_1h" not in df.columns):
                df["ats_result_1h"] = pd.Series([None]*len(df), dtype="object")
            else:
                try:
                    df["ats_result_1h"] = df["ats_result_1h"].astype("object")
                except Exception:
                    pass
            df.loc[mask1 & df["ats_result_1h"].isna(), "ats_result_1h"] = ats1[mask1 & df["ats_result_1h"].isna()]
        if {"home_score_2h","away_score_2h","spread_home_2h"}.issubset(df.columns):
            hs2 = pd.to_numeric(df["home_score_2h"], errors="coerce")
            as2 = pd.to_numeric(df["away_score_2h"], errors="coerce")
            sh2 = pd.to_numeric(df["spread_home_2h"], errors="coerce")
            am2 = hs2 - as2
            mask2 = hs2.notna() & as2.notna() & sh2.notna()
            ats2 = np.where(am2 > -sh2, "Home Cover", np.where(am2 < -sh2, "Away Cover", "Push"))
            if ("ats_result_2h" not in df.columns):
                df["ats_result_2h"] = pd.Series([None]*len(df), dtype="object")
            else:
                try:
                    df["ats_result_2h"] = df["ats_result_2h"].astype("object")
                except Exception:
                    pass
            df.loc[mask2 & df["ats_result_2h"].isna(), "ats_result_2h"] = ats2[mask2 & df["ats_result_2h"].isna()]
    except Exception:
        pass
    # Replace NaN with None for template-friendly rendering (avoid 'nan' text everywhere)
    try:
        df_tpl = df.where(pd.notna(df), None)
    except Exception:
        df_tpl = df

    # Venue timezone enrichment: ensure venue-local fields exist and correct for rollover
    try:
        ov_path = ROOT / 'data' / 'venue_tz_overrides.csv'
        tz_map: dict[str, str] = {}
        if ov_path.exists():
            try:
                ov = pd.read_csv(ov_path)
                if {'venue','tz'}.issubset(ov.columns):
                    for _, rr in ov.iterrows():
                        v = str(rr.get('venue') or '').strip()
                        t = str(rr.get('tz') or '').strip()
                        if v and t:
                            tz_map[v.lower()] = t
            except Exception:
                pass
        if tz_map and not df_tpl.empty:
            # Compute venue-local time from start_time_iso when possible
            if 'start_time_iso' in df_tpl.columns:
                iso_ser = pd.to_datetime(df_tpl['start_time_iso'].astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
            else:
                iso_ser = pd.to_datetime(df_tpl.get('start_time', pd.Series([None]*len(df_tpl))).astype(str).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
            venues = df_tpl.get('venue_full', df_tpl.get('venue', pd.Series([None]*len(df_tpl)))).astype(str)
            # Fallback keys: try team names when venue string doesn't match overrides
            homes = df_tpl.get('home_team', pd.Series([None]*len(df_tpl))).astype(str)
            aways = df_tpl.get('away_team', pd.Series([None]*len(df_tpl))).astype(str)
            def _loc_from_venue(idx: int):
                try:
                    v = (venues.iloc[idx] or '').strip().lower()
                    tz_name = tz_map.get(v)
                    if not tz_name:
                        h = (homes.iloc[idx] or '').strip().lower()
                        a = (aways.iloc[idx] or '').strip().lower()
                        # Prefer home team mapping
                        tz_name = tz_map.get(h) or tz_map.get(a)
                    dt_utc = iso_ser.iloc[idx]
                    if tz_name and pd.notna(dt_utc):
                        try:
                            loc = dt_utc.tz_convert(ZoneInfo(tz_name))
                            return loc.strftime('%Y-%m-%d %H:%M'), loc.tzname()
                        except Exception:
                            pass
                except Exception:
                    pass
                return None, None
            loc_vals = []
            tz_abbrs = []
            for i in range(len(df_tpl)):
                lt, ab = _loc_from_venue(i)
                loc_vals.append(lt)
                tz_abbrs.append(ab)
            # Fill only where missing
            if 'start_time_local_venue' not in df_tpl.columns:
                df_tpl['start_time_local_venue'] = None
            if 'start_tz_abbr_venue' not in df_tpl.columns:
                df_tpl['start_tz_abbr_venue'] = None
            m_loc = pd.Series(loc_vals)
            m_tz = pd.Series(tz_abbrs)
            df_tpl['start_time_local_venue'] = df_tpl['start_time_local_venue'].where(df_tpl['start_time_local_venue'].notna(), m_loc)
            df_tpl['start_tz_abbr_venue'] = df_tpl['start_tz_abbr_venue'].where(df_tpl['start_tz_abbr_venue'].notna(), m_tz)
            # Set display_date to venue-local date when UTC spills into next day
            try:
                base_date = df_tpl.get('date', pd.Series([None]*len(df_tpl))).astype(str)
                loc_date = pd.to_datetime(df_tpl['start_time_local_venue'], errors='coerce').dt.strftime('%Y-%m-%d')
                mask_diff = base_date.notna() & loc_date.notna() & (base_date != loc_date)
                if 'display_date' not in df_tpl.columns:
                    df_tpl['display_date'] = None
                df_tpl.loc[mask_diff, 'display_date'] = loc_date[mask_diff]
            except Exception:
                pass
    except Exception:
        pass

    # Sanitize numeric actuals/scores to avoid 'nan' strings leaking into template
    try:
        num_cols = [
            'home_score','away_score','actual_total',
            'home_score_1h','away_score_1h','actual_total_1h',
            'home_score_2h','away_score_2h','actual_total_2h'
        ]
        for c in num_cols:
            if c in df_tpl.columns:
                s = pd.to_numeric(df_tpl[c], errors='coerce')
                # Convert NaN back to None for template friendliness
                df_tpl[c] = s.where(s.notna(), None)
    except Exception:
        pass

    # ESPN curated subset + date filter for non-stable display
    # Ensures the interactive path matches the authoritative ESPN slate when available
    try:
        # Filter by selected date if present in the template frame
        if isinstance(df_tpl, pd.DataFrame) and not df_tpl.empty:
            if date_q:
                try:
                    if 'date' in df_tpl.columns:
                        df_tpl['date'] = df_tpl['date'].astype(str)
                        df_tpl = df_tpl[df_tpl['date'] == str(date_q)]
                    elif 'start_time' in df_tpl.columns:
                        st_str = df_tpl['start_time'].astype(str)
                        st_date = st_str.str.slice(0, 10)
                        df_tpl = df_tpl[st_date == str(date_q)]
                except Exception:
                    pass
            # ESPN subset restriction
            t_date_eff = str(date_q) if date_q else (today_str or dt.datetime.utcnow().strftime('%Y-%m-%d'))
            try:
                subset_path_ns = OUT / f'schedule_espn_subset_{t_date_eff}.json'
                if subset_path_ns.exists():
                    payload_ns = _json.loads(subset_path_ns.read_text(encoding='utf-8'))
                    subset_ids_ns: set[str] = set()
                    if isinstance(payload_ns, dict):
                        for k in ('ids','espn_ids','game_ids'):
                            v = payload_ns.get(k)
                            if isinstance(v, list) and v:
                                subset_ids_ns = {str(x) for x in v}
                                break
                    elif isinstance(payload_ns, list):
                        subset_ids_ns = {str(x) for x in payload_ns}
                    if subset_ids_ns and 'game_id' in df_tpl.columns:
                        df_tpl['game_id'] = df_tpl['game_id'].astype(str)
                        before_ct_ns = int(len(df_tpl))
                        df_tpl = df_tpl[df_tpl['game_id'].isin(subset_ids_ns)].reset_index(drop=True)
                        pipeline_stats['espn_subset_applied'] = True
                        pipeline_stats['espn_subset_before'] = before_ct_ns
                        pipeline_stats['espn_subset_after'] = int(len(df_tpl))
                        pipeline_stats['espn_subset_count'] = int(len(subset_ids_ns))
                    else:
                        pipeline_stats['espn_subset_applied'] = False
            except Exception:
                pipeline_stats['espn_subset_error'] = True
    except Exception:
        pipeline_stats['espn_subset_filter_wrap_error'] = True

    # Safety: drop placeholder team rows right before templating unless explicitly allowed via ?show_placeholders=1
    try:
        allow_placeholders = (request.args.get("show_placeholders") or "").strip().lower() in ("1","true","yes")
        if not allow_placeholders and not df_tpl.empty and {"home_team","away_team"}.issubset(df_tpl.columns):
            bads = {"tbd", "t.b.d", "tba", "t.b.a", "to be determined", "to-be-determined", "to be announced", "to-be-announced", "unknown", "na", "n/a", ""}
            def _is_bad2(x):
                try:
                    return str(x).strip().lower() in bads
                except Exception:
                    return True
            mask_bad2 = df_tpl["home_team"].map(_is_bad2) | df_tpl["away_team"].map(_is_bad2)
            if mask_bad2.any():
                pipeline_stats["dropped_tbd_rows_late"] = int(mask_bad2.sum())
                df_tpl = df_tpl[~mask_bad2].reset_index(drop=True)
    except Exception:
        pass

    # Diagnostics enrichment (after full enrichment & NaN replacement): summarize missing odds & low predictions.
    try:
        # Odds coverage metrics
        if "market_total" in df_tpl.columns:
            pipeline_stats["post_missing_market_total"] = int(pd.to_numeric(df_tpl["market_total"], errors="coerce").isna().sum())
        if "spread_home" in df_tpl.columns:
            pipeline_stats["post_missing_spread_home"] = int(pd.to_numeric(df_tpl["spread_home"], errors="coerce").isna().sum())
        # Low prediction totals (<115) flag
        if "pred_total" in df_tpl.columns:
            pt_vals = pd.to_numeric(df_tpl["pred_total"], errors="coerce")
            low_mask = pt_vals < 115
            pipeline_stats["low_pred_count_lt115"] = int(low_mask.sum())
            if low_mask.any() and "game_id" in df_tpl.columns:
                pipeline_stats["low_pred_game_ids"] = list(df_tpl.loc[low_mask, "game_id"].astype(str).head(12))
            # Predictions identical to market_total (rounded to 0.1) – track frequency
            if "market_total" in df_tpl.columns:
                mt_vals = pd.to_numeric(df_tpl["market_total"], errors="coerce")
                eq_mask = pt_vals.notna() & mt_vals.notna() & (pt_vals.round(1) == mt_vals.round(1))
                pipeline_stats["pred_equal_market_count"] = int(eq_mask.sum())
                if eq_mask.any() and "game_id" in df_tpl.columns:
                    pipeline_stats["pred_equal_market_sample"] = list(df_tpl.loc[eq_mask, "game_id"].astype(str).head(8))
            # Basis distribution counts
            if "pred_total_basis" in df_tpl.columns:
                basis_counts = df_tpl["pred_total_basis"].value_counts(dropna=True).to_dict()
                pipeline_stats["pred_total_basis_counts"] = basis_counts
                pipeline_stats["pred_synthetic_baseline_count"] = int(df_tpl["pred_total_basis"].eq("synthetic_baseline").sum())
                pipeline_stats["pred_market_copy_count"] = int(df_tpl["pred_total_basis"].eq("market_copy").sum())
            if "pred_margin_basis" in df_tpl.columns:
                m_basis_counts = df_tpl["pred_margin_basis"].value_counts(dropna=True).to_dict()
                pipeline_stats["pred_margin_basis_counts"] = m_basis_counts
        # Edge quality diagnostics (correlations)
        try:
            if {"pred_margin_model","spread_home"}.issubset(df_tpl.columns):
                pmv = pd.to_numeric(df_tpl["pred_margin_model"], errors="coerce")
                spv = pd.to_numeric(df_tpl["spread_home"], errors="coerce")
                # Guard against zero variance (avoids numpy divide warnings)
                if pmv.notna().sum() > 3 and spv.notna().sum() > 3 and pmv.std() > 0 and spv.std() > 0:
                    pipeline_stats["corr_pred_margin_model_spread_home"] = float(pmv.corr(spv))
            if {"edge_total_model","edge_margin_model"}.issubset(df_tpl.columns):
                etm = pd.to_numeric(df_tpl["edge_total_model"], errors="coerce")
                emm = pd.to_numeric(df_tpl["edge_margin_model"], errors="coerce")
                if etm.notna().sum() > 3 and emm.notna().sum() > 3 and etm.std() > 0 and emm.std() > 0:
                    pipeline_stats["corr_edge_total_vs_margin_model"] = float(etm.corr(emm))
        except Exception as _corr_e:
            pipeline_stats['corr_calc_error'] = str(_corr_e)[:80]
        # Explicit list of games missing market_total (first 12)
        if {"game_id","market_total"}.issubset(df_tpl.columns):
            miss_mask = df_tpl["market_total"].isna()
            if miss_mask.any():
                pipeline_stats["missing_odds_game_ids"] = list(df_tpl.loc[miss_mask, "game_id"].astype(str).head(12))
    except Exception:
        pass

    # Post-construction cleanup: ensure no rows reach the template with BOTH missing predictions and odds.
    # Strategy:
    # 1. If pred_total & all market/closing totals are missing, but proj_home/proj_away exist, derive pred_total = proj_home + proj_away.
    # 2. Re-check; if still missing all totals AND no spread or ML quotes, drop the row as it provides no actionable info.
    removed_empty_rows = 0
    try:
        # New behavior: by default KEEP rows even if both predictions and odds are missing.
        # Only drop when explicitly requested via ?drop_empty=1. For backwards compat, ?show_empty=1 also forces keep.
        drop_empty = (request.args.get("drop_empty") or "").strip().lower() in ("1","true","yes")
        allow_empty = (request.args.get("show_empty") or "").strip().lower() in ("1","true","yes")
        needed_cols = {"pred_total", "market_total", "closing_total"}
        if needed_cols.issubset(df_tpl.columns) and drop_empty and not allow_empty:
            # Derive predictions where possible
            have_proj = {"proj_home", "proj_away"}.issubset(df_tpl.columns)
            if have_proj:
                ph = pd.to_numeric(df_tpl.get("proj_home"), errors="coerce")
                pa = pd.to_numeric(df_tpl.get("proj_away"), errors="coerce")
                missing_pred = df_tpl["pred_total"].isna() if "pred_total" in df_tpl.columns else pd.Series(True, index=df_tpl.index)
                can_derive = missing_pred & ph.notna() & pa.notna()
                if can_derive.any():
                    df_tpl.loc[can_derive, "pred_total"] = (ph + pa)[can_derive]
            # Identify rows still empty of both odds & predictions (no totals, no spreads, no ML) after derivation
            still_empty = (
                df_tpl["pred_total"].isna() &
                df_tpl["market_total"].isna() &
                df_tpl["closing_total"].isna()
            )
            if "spread_home" in df_tpl.columns:
                still_empty = still_empty & df_tpl["spread_home"].isna() & df_tpl.get("closing_spread_home", pd.Series([True]*len(df_tpl))).isna()
            if "ml_home" in df_tpl.columns:
                still_empty = still_empty & df_tpl["ml_home"].isna()
            # Drop those rows; capture count for diagnostics
            if still_empty.any():
                removed_empty_rows = int(still_empty.sum())
                df_tpl = df_tpl[~still_empty].reset_index(drop=True)
    except Exception:
        pass

    # Apply totals guardrails and enforce calibrated-first precedence
    try:
        if 'pred_total' in df_tpl.columns:
            df_tpl['total_proj_raw'] = pd.to_numeric(df_tpl['pred_total'], errors='coerce')
        if ('total_proj_raw' in df_tpl.columns) or ('total_proj_cal' in df_tpl.columns):
            df_tpl = apply_total_guardrails(df_tpl)
            df_tpl = enforce_calibrated_first(df_tpl)
            if 'total_proj_display' in df_tpl.columns:
                df_tpl['pred_total'] = df_tpl['total_proj_display']
                df_tpl['pred_total_basis'] = df_tpl.get('display_precedence_total', 'RAW').str.lower()
        if 'margin_proj_display' in df_tpl.columns and 'pred_margin' in df_tpl.columns:
            df_tpl['pred_margin'] = df_tpl['margin_proj_display']
            df_tpl['pred_margin_basis'] = df_tpl.get('display_precedence_margin', 'RAW').str.lower()
    except Exception:
        pass

    # Odds backfill before coverage summary
    try:
        df_tpl = apply_odds_backfill(df_tpl)
    except Exception:
        pass

    # Recompute coverage summary post enrichment: treat presence of any of (market_total, closing_total, spread_home, closing_spread_home, ml_home) as odds coverage signals.
    try:
        coverage_summary = {"full": 0, "partial": 0, "none": 0}
        if "game_id" in df_tpl.columns:
            for _, r in df_tpl.iterrows():
                has_total = (r.get("market_total") is not None) or (r.get("closing_total") is not None)
                has_spread = (r.get("spread_home") is not None) or (r.get("closing_spread_home") is not None)
                has_ml = (r.get("ml_home") is not None)
                if has_total and has_spread and has_ml:
                    coverage_summary["full"] += 1
                elif has_total or has_spread or has_ml:
                    coverage_summary["partial"] += 1
                else:
                    coverage_summary["none"] += 1
            # Annotate per-row coverage_status for template use
            cov_status: list[str] = []
            for _, r in df_tpl.iterrows():
                has_total = (r.get("market_total") is not None) or (r.get("closing_total") is not None)
                has_spread = (r.get("spread_home") is not None) or (r.get("closing_spread_home") is not None)
                has_ml = (r.get("ml_home") is not None)
                if has_total and has_spread and has_ml:
                    cov_status.append("full")
                elif has_total or has_spread or has_ml:
                    cov_status.append("partial")
                else:
                    cov_status.append("none")
            df_tpl["coverage_status"] = cov_status
    except Exception:
        pass

    # Final fallback: populate missing half ATS results if spreads and scores now available (including derived spreads)
    try:
        # 1H
        if {"home_score_1h","away_score_1h","spread_home_1h"}.issubset(df_tpl.columns):
            mask_missing_1h = df_tpl["ats_result_1h"].isna() if "ats_result_1h" in df_tpl.columns else pd.Series(True, index=df_tpl.index)
            hs1 = pd.to_numeric(df_tpl["home_score_1h"], errors="coerce")
            as1 = pd.to_numeric(df_tpl["away_score_1h"], errors="coerce")
            sh1 = pd.to_numeric(df_tpl["spread_home_1h"], errors="coerce")
            can_calc_1h = mask_missing_1h & hs1.notna() & as1.notna() & sh1.notna()
            am1 = hs1 - as1
            ats1 = np.where(am1 > -sh1, "Home Cover", np.where(am1 < -sh1, "Away Cover", "Push"))
            if "ats_result_1h" not in df_tpl.columns:
                df_tpl["ats_result_1h"] = None
            df_tpl.loc[can_calc_1h, "ats_result_1h"] = ats1[can_calc_1h]
        # 2H
        if {"home_score_2h","away_score_2h","spread_home_2h"}.issubset(df_tpl.columns):
            mask_missing_2h = df_tpl["ats_result_2h"].isna() if "ats_result_2h" in df_tpl.columns else pd.Series(True, index=df_tpl.index)
            hs2 = pd.to_numeric(df_tpl["home_score_2h"], errors="coerce")
            as2 = pd.to_numeric(df_tpl["away_score_2h"], errors="coerce")
            sh2 = pd.to_numeric(df_tpl["spread_home_2h"], errors="coerce")
            can_calc_2h = mask_missing_2h & hs2.notna() & as2.notna() & sh2.notna()
            am2 = hs2 - as2
            ats2 = np.where(am2 > -sh2, "Home Cover", np.where(am2 < -sh2, "Away Cover", "Push"))
            if "ats_result_2h" not in df_tpl.columns:
                df_tpl["ats_result_2h"] = None
            df_tpl.loc[can_calc_2h, "ats_result_2h"] = ats2[can_calc_2h]
    except Exception:
        pass

    rows = [_brand_row(r) for r in df_tpl.to_dict(orient="records")]

    # Add display/coverage flags for template: has_odds/has_actuals/has_derivatives and a clean display_time_str
    try:
        def _is_num(v: Any) -> bool:
            try:
                if v is None: return False
                if isinstance(v, (float, int)):
                    return not pd.isna(v)
                s = str(v).strip().lower()
                if s in ("", "nan", "none", "null", "–", "-"): return False
                return not pd.isna(pd.to_numeric(v, errors='coerce'))
            except Exception:
                return False
        for r in rows:
            # has_odds: any market_total/closing_total/spread_home/closing_spread_home/ml_home
            has_total = _is_num(r.get('market_total')) or _is_num(r.get('closing_total'))
            has_spread = _is_num(r.get('spread_home')) or _is_num(r.get('closing_spread_home'))
            has_ml = _is_num(r.get('ml_home')) or _is_num(r.get('ml_away'))
            r['has_odds'] = bool(has_total or has_spread or has_ml)
            # has_actuals: any final totals or scores
            has_act_full = _is_num(r.get('actual_total')) or (_is_num(r.get('home_score')) and _is_num(r.get('away_score')))
            has_act_1h = _is_num(r.get('actual_total_1h')) or (_is_num(r.get('home_score_1h')) and _is_num(r.get('away_score_1h')))
            has_act_2h = _is_num(r.get('actual_total_2h')) or (_is_num(r.get('home_score_2h')) and _is_num(r.get('away_score_2h')))
            r['has_actuals'] = bool(has_act_full or has_act_1h or has_act_2h)
            # has_derivatives: presence of any 1H/2H market or predictions
            has_deriv_market = _is_num(r.get('market_total_1h')) or _is_num(r.get('market_total_2h')) or _is_num(r.get('spread_home_1h')) or _is_num(r.get('spread_home_2h'))
            has_deriv_preds = _is_num(r.get('pred_total_1h')) or _is_num(r.get('pred_total_2h')) or _is_num(r.get('pred_margin_1h')) or _is_num(r.get('pred_margin_2h'))
            r['has_derivatives'] = bool(has_deriv_market or has_deriv_preds)
            # coverage classification: differentiate outright absence vs exhibition/non-D1 vs pending
            try:
                d1set = _load_d1_team_set() or set()
            except Exception:
                d1set = set()
            try:
                h_norm = normalize_name(str(r.get('home_team') or ''))
                a_norm = normalize_name(str(r.get('away_team') or ''))
            except Exception:
                h_norm = str(r.get('home_team') or '')
                a_norm = str(r.get('away_team') or '')
            in_d1_pair = (h_norm in d1set) and (a_norm in d1set)
            r['is_exhibition'] = not in_d1_pair
            has_pred_any = _is_num(r.get('pred_total')) or _is_num(r.get('pred_margin'))
            if r['has_odds']:
                r['coverage_label'] = 'Odds Available'
            else:
                if has_pred_any:
                    if r['is_exhibition']:
                        r['coverage_label'] = 'No Market (Exhibition/Non-D1)'
                    else:
                        r['coverage_label'] = 'No Market (Unposted)'
                else:
                    r['coverage_label'] = 'No Market (No Data)'
            # Time display is now handled by shared helpers earlier in the pipeline
    except Exception:
        pass

    # Display filter: keep only games with at least one Division I team unless override flag set (?all=1)
    # We apply after all enrichments so market lines/predictions remain intact; this is purely a view-level restriction.
    try:
        show_all = (request.args.get("all") or "").strip().lower() in ("1","true","yes")
        strict_d1 = (request.args.get("strict_d1") or "").strip().lower() in ("1","true","yes")
        if not show_all and rows:
            d1set = _load_d1_team_set()
            if d1set:
                filtered_rows: list[dict[str, Any]] = []
                excluded_rows: list[dict[str, Any]] = []
                for r in rows:
                    h = normalize_name(str(r.get("home_team") or ""))
                    a = normalize_name(str(r.get("away_team") or ""))
                    # Detect usable data presence (predictions or odds) to allow graceful inclusion
                    pred_val = r.get("pred_total")
                    market_val = r.get("market_total")
                    odds_list = r.get("_odds_list") or []
                    def _has_val(v: Any) -> bool:
                        if v is None: return False
                        try:
                            # Treat NaN/None/"" as missing
                            if isinstance(v, (float,int)) and (pd.isna(v)): return False
                            s = str(v).strip().lower()
                            return s not in ("", "nan", "none", "null")
                        except Exception:
                            return False
                    has_pred = _has_val(pred_val)
                    has_market = _has_val(market_val)
                    has_any_odds = bool(odds_list)
                    in_d1 = (h in d1set) or (a in d1set)
                    # Inclusion logic:
                    #  - Always include if at least one team is D1
                    #  - If strict_d1 flag set, require D1 team (legacy behavior)
                    #  - Else, include non-D1 pair when we have predictions OR odds lines (so legitimate slate data isn’t hidden)
                    if in_d1 or (not strict_d1 and (has_pred or has_market or has_any_odds)):
                        filtered_rows.append(r)
                    else:
                        excluded_rows.append(r)
                # Replace only if we didn't accidentally drop everything
                if filtered_rows and len(filtered_rows) <= len(rows):
                    rows = filtered_rows
                pipeline_stats["rows_excluded_d1"] = len(excluded_rows)
                pipeline_stats["excluded_game_ids_d1"] = [str(er.get("game_id")) for er in excluded_rows if er.get("game_id")]
                pipeline_stats["excluded_non_d1_pairs_strict_count"] = int(sum(1 for er in excluded_rows if strict_d1))
            # If d1set empty, skip filter entirely
        pipeline_stats["rows_after_d1_filter"] = len(rows)
        pipeline_stats["d1_filter_strict"] = strict_d1
    except Exception as _d1e:
        pipeline_stats["d1_filter_error"] = str(_d1e)
        pass
    total_rows = len(rows)
    accuracy = _load_accuracy_summary()
    coverage_note = None
    try:
        if date_q:
            cov = _load_schedule_coverage()
            if not cov.empty and {"date","anomaly","n_games"}.issubset(cov.columns):
                row = cov[cov["date"].astype(str) == str(date_q)]
                if not row.empty and bool(row.iloc[0]["anomaly"]):
                    coverage_note = f"Schedule anomaly: only {int(row.iloc[0]['n_games'])} games on {date_q}."
    except Exception:
        pass
    # Suppress anomaly note specifically for 2025-11-11 per UI request
    if str(date_q) == "2025-11-11":
        coverage_note = None
    # Archive dates list (daily_results) for navigation
    try:
        archive_dates: list[str] = []
        dr_dir = OUT / "daily_results"
        if dr_dir.exists():
            for p in sorted(dr_dir.glob("results_*.csv")):
                stem = p.stem
                if stem.startswith("results_"):
                    archive_dates.append(stem.replace("results_", ""))
    except Exception:
        archive_dates = []

    # Optional bootstrap/diagnostics hints when today's slate looks underpopulated
    show_bootstrap = False
    bootstrap_url = None
    fused_bootstrap_url = None
    show_diag = False
    diag_url = None
    try:
        tstr = _today_local().strftime("%Y-%m-%d")
        sel_is_today = (str(date_q) == tstr)
        has_pred_col = ("pred_total" in df.columns)
        has_any_preds = bool(has_pred_col and pd.to_numeric(df.get("pred_total"), errors="coerce").notna().any())
        show_bootstrap = bool(sel_is_today and (total_rows == 0 or not has_any_preds))
        if show_bootstrap:
            bootstrap_url = f"/api/bootstrap?date={str(date_q or tstr)}&provider=espn&force=1"
        # If very few rows on today, surface diagnostics and fused bootstrap
        if sel_is_today and (total_rows < 20):
            show_diag = True
            diag_url = f"/api/schedule-diagnostics?date={tstr}&refresh=1"
            fused_bootstrap_url = f"/api/bootstrap?date={tstr}&provider=fused&force=1&refresh=1"
        # Offer odds refresh shortcut when on today's slate
        refresh_odds_url = None
        if sel_is_today:
            refresh_odds_url = f"/api/refresh-odds?date={tstr}"
    except Exception:
        show_bootstrap = False
        refresh_odds_url = None

    if diag_enabled:
        # Summarize missing prediction / odds counts
        mt_missing = 0
        pt_missing = 0
        try:
            if rows:
                for r in rows:
                    if r.get("market_total") is None and r.get("closing_total") is None:
                        mt_missing += 1
                    if r.get("pred_total") is None:
                        pt_missing += 1
        except Exception:
            pass
        pipeline_stats["missing_market_total_rows"] = mt_missing
        pipeline_stats["missing_pred_total_rows"] = pt_missing
        pipeline_stats["final_rows"] = len(rows)
        logger.info("Render pipeline stats: %s", json.dumps(pipeline_stats))

        # Optional diagnostics JSON direct response: /?diag=1&diag_json=1
        try:
            if (request.args.get('diag_json') or '').strip().lower() in ('1','true','yes'):
                pipeline_stats['final_columns'] = list(df.columns)
                if 'pred_total' in df.columns:
                    pt_final = pd.to_numeric(df['pred_total'], errors='coerce')
                    pipeline_stats['pred_total_final_preview'] = pt_final.head(25).tolist()
                if 'pred_total_raw' in df.columns:
                    pt_raw_prev = pd.to_numeric(df['pred_total_raw'], errors='coerce')
                    pipeline_stats['pred_total_raw_preview'] = pt_raw_prev.head(25).tolist()
                if 'derived_total' in df.columns:
                    dt_prev = pd.to_numeric(df['derived_total'], errors='coerce')
                    pipeline_stats['derived_total_preview'] = dt_prev.head(25).tolist()
                from flask import jsonify
                return jsonify(pipeline_stats)
        except Exception:
            pass
    # ------------------------------------------------------------------
    # Unified predictions export & global capture
    # (moved before return to ensure execution)
    # ------------------------------------------------------------------
    try:
        global _LAST_UNIFIED_FRAME
        _LAST_UNIFIED_FRAME = df.copy()
        # ------------------------------------------------------------------
        # Uncertainty estimation (lightweight): derive sigma for totals & margins
        # using residual std over last N days from daily_results. Scales by tempo.
        # ------------------------------------------------------------------
        try:
            recent_files = sorted((OUT / "daily_results").glob("results_*.csv"))[-14:]
            resid_totals: list[float] = []
            resid_margins: list[float] = []
            for fp in recent_files:
                try:
                    ddf = pd.read_csv(fp)
                except Exception:
                    continue
                needed_t = {"pred_total","home_score","away_score"}
                if needed_t.issubset(ddf.columns):
                    hs = pd.to_numeric(ddf["home_score"], errors="coerce")
                    as_ = pd.to_numeric(ddf["away_score"], errors="coerce")
                    pt = pd.to_numeric(ddf["pred_total"], errors="coerce")
                    actual_total = hs + as_
                    resid = (pt - actual_total).dropna()
                    resid_totals.extend(resid.tolist())
                needed_m = {"pred_margin","home_score","away_score"}
                if needed_m.issubset(ddf.columns):
                    hs = pd.to_numeric(ddf["home_score"], errors="coerce")
                    as_ = pd.to_numeric(ddf["away_score"], errors="coerce")
                    pm = pd.to_numeric(ddf["pred_margin"], errors="coerce")
                    actual_margin = hs - as_
                    residm = (pm - actual_margin).dropna()
                    resid_margins.extend(residm.tolist())
            base_sigma_total = float(np.std(resid_totals)) if len(resid_totals) >= 12 else 12.0
            base_sigma_margin = float(np.std(resid_margins)) if len(resid_margins) >= 12 else 8.0
            # Scale by tempo sum if ratings available
            tempo_scale = None
            if {"home_tempo_rating","away_tempo_rating"}.issubset(df.columns):
                tempo_scale = (pd.to_numeric(df["home_tempo_rating"], errors="coerce") + pd.to_numeric(df["away_tempo_rating"], errors="coerce")) / (2 * 69.0)
            df["pred_total_sigma"] = base_sigma_total * (tempo_scale if tempo_scale is not None else 1.0)
            df["pred_margin_sigma"] = base_sigma_margin * (tempo_scale if tempo_scale is not None else 1.0)
            pipeline_stats["pred_total_sigma_mean"] = float(pd.to_numeric(df["pred_total_sigma"], errors="coerce").mean()) if "pred_total_sigma" in df.columns else None
            pipeline_stats["pred_margin_sigma_mean"] = float(pd.to_numeric(df["pred_margin_sigma"], errors="coerce").mean()) if "pred_margin_sigma" in df.columns else None
            # Confidence-weighted staking adjustment: downscale Kelly by relative sigma vs base
            try:
                if {"kelly_fraction_total","pred_total_sigma"}.issubset(df.columns):
                    rel_scale = pd.to_numeric(df["pred_total_sigma"], errors="coerce") / max(base_sigma_total, 1e-9)
                    df["kelly_fraction_total_adj"] = pd.to_numeric(df["kelly_fraction_total"], errors="coerce") / rel_scale.clip(lower=0.5, upper=2.5)
            except Exception:
                pass
        except Exception:
            pipeline_stats["uncertainty_error"] = "sigma_failed"
        export_flag = (request.args.get("export") or "").strip().lower() in ("1","true","yes")
        export_date: str | None = None
        for cand in [date_q, today_str]:
            if cand:
                export_date = cand
                break
        if not export_date and "date" in df.columns and df["date"].notna().any():
            try:
                export_date = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d").dropna().iloc[0]
            except Exception:
                try:
                    export_date = str(df["date"].dropna().astype(str).iloc[0])
                except Exception:
                    export_date = None
        if export_date and not df.empty and (export_flag or diag_enabled or bool(date_q)):
            # Final defensive fill pass: ensure no remaining NaNs before export (coverage + adjustments may have introduced new rows).
            try:
                def _finalize_predictions_frame(_df: pd.DataFrame) -> pd.DataFrame:
                    # Fill pred_total
                    if 'pred_total' in _df.columns:
                        pt_ser = pd.to_numeric(_df['pred_total'], errors='coerce')
                        mt_ser = pd.to_numeric(_df.get('market_total'), errors='coerce') if 'market_total' in _df.columns else pd.Series([np.nan]*len(_df))
                        ht = pd.to_numeric(_df.get('home_tempo_rating'), errors='coerce') if 'home_tempo_rating' in _df.columns else pd.Series([np.nan]*len(_df))
                        at = pd.to_numeric(_df.get('away_tempo_rating'), errors='coerce') if 'away_tempo_rating' in _df.columns else pd.Series([np.nan]*len(_df))
                        tempo_avg = np.where(ht.notna() & at.notna(), (ht+at)/2.0, np.nan)
                        import zlib
                        missing_mask = pt_ser.isna()
                        if missing_mask.any():
                            for ridx in _df.index[missing_mask]:
                                h = _df.at[ridx,'home_team'] if 'home_team' in _df.columns else ''
                                a = _df.at[ridx,'away_team'] if 'away_team' in _df.columns else ''
                                try:
                                    noise = (((zlib.adler32(f"{h}::{a}".encode()) % 1000)/1000.0) - 0.5) * 3.5
                                except Exception:
                                    noise = 0.0
                                baseline = 141.5
                                tval = tempo_avg[ridx] if not (isinstance(tempo_avg, float) or pd.isna(tempo_avg[ridx])) else np.nan
                                tempo_comp = ((tval - 70.0) * 0.6) if not pd.isna(tval) else 0.0
                                mt_val = mt_ser[ridx] if ridx in mt_ser.index and not pd.isna(mt_ser[ridx]) else np.nan
                                if not pd.isna(mt_val):
                                    valf = 0.50*baseline + 0.35*float(mt_val) + 0.15*(baseline + tempo_comp) + noise
                                else:
                                    valf = baseline + tempo_comp + noise
                                valf = float(np.clip(valf, 60, 194))
                                _df.at[ridx,'pred_total'] = valf
                                if 'pred_total_basis' in _df.columns and pd.isna(_df.at[ridx,'pred_total_basis']):
                                    _df.at[ridx,'pred_total_basis'] = 'synthetic_final'
                                elif 'pred_total_basis' not in _df.columns:
                                    _df.loc[ridx,'pred_total_basis'] = 'synthetic_final'
                    # Fill pred_margin
                    if 'pred_margin' in _df.columns:
                        pm_ser = pd.to_numeric(_df['pred_margin'], errors='coerce')
                        miss_pm = pm_ser.isna()
                        if miss_pm.any():
                            spread_ser = pd.to_numeric(_df.get('spread_home'), errors='coerce') if 'spread_home' in _df.columns else pd.Series([np.nan]*len(_df))
                            # spread-based first
                            can_spread = miss_pm & spread_ser.notna()
                            for ridx in _df.index[can_spread]:
                                _df.at[ridx,'pred_margin'] = float(-spread_ser[ridx])
                                if 'pred_margin_basis' in _df.columns and pd.isna(_df.at[ridx,'pred_margin_basis']):
                                    _df.at[ridx,'pred_margin_basis'] = 'synthetic_spread_final'
                                elif 'pred_margin_basis' not in _df.columns:
                                    _df.loc[ridx,'pred_margin_basis'] = 'synthetic_spread_final'
                            # even margin for remainder
                            remaining = pd.to_numeric(_df['pred_margin'], errors='coerce').isna()
                            for ridx in _df.index[remaining]:
                                _df.at[ridx,'pred_margin'] = 0.0
                                if 'pred_margin_basis' in _df.columns and pd.isna(_df.at[ridx,'pred_margin_basis']):
                                    _df.at[ridx,'pred_margin_basis'] = 'synthetic_even_final2'
                                elif 'pred_margin_basis' not in _df.columns:
                                    _df.loc[ridx,'pred_margin_basis'] = 'synthetic_even_final2'
                    return _df
                before_missing_export = int(pd.to_numeric(df.get('pred_total'), errors='coerce').isna().sum()) if 'pred_total' in df.columns else None
                # Instrument missing model predictions before synthetic model fallback
                if 'pred_total_model' in df.columns:
                    pipeline_stats['model_total_missing_before_export'] = int(pd.to_numeric(df['pred_total_model'], errors='coerce').isna().sum())
                if 'pred_margin_model' in df.columns:
                    pipeline_stats['model_margin_missing_before_export'] = int(pd.to_numeric(df['pred_margin_model'], errors='coerce').isna().sum())
                df = _finalize_predictions_frame(df)
                after_missing_export = int(pd.to_numeric(df.get('pred_total'), errors='coerce').isna().sum()) if 'pred_total' in df.columns else None
                pipeline_stats['finalize_export_pred_total_missing_before'] = before_missing_export
                pipeline_stats['finalize_export_pred_total_missing_after'] = after_missing_export
                # Synthetic model fallback: promote filled displayed predictions into model columns when model inference absent
                try:
                    import zlib
                    if 'pred_total' in df.columns:
                        if 'pred_total_model' not in df.columns:
                            df['pred_total_model'] = np.nan
                            df['pred_total_model_basis'] = None
                        mtm = pd.to_numeric(df['pred_total_model'], errors='coerce') if 'pred_total_model' in df.columns else pd.Series([np.nan]*len(df))
                        disp_tot = pd.to_numeric(df['pred_total'], errors='coerce')
                        synth_mask_t = mtm.isna() & disp_tot.notna()
                        if synth_mask_t.any():
                            for ridx in df.index[synth_mask_t]:
                                base_val = float(disp_tot.loc[ridx])
                                # tiny deterministic noise to differentiate synthetic model vs display
                                h = str(df.at[ridx,'home_team']) if 'home_team' in df.columns else ''
                                a = str(df.at[ridx,'away_team']) if 'away_team' in df.columns else ''
                                try:
                                    noise = (((zlib.adler32(f"{h}::{a}".encode()) % 1000)/1000.0) - 0.5) * 0.25
                                except Exception:
                                    noise = 0.0
                                df.at[ridx,'pred_total_model'] = float(np.clip(base_val + noise, 55, 210))
                                if 'pred_total_model_basis' in df.columns and (pd.isna(df.at[ridx,'pred_total_model_basis']) or df.at[ridx,'pred_total_model_basis'] in (None,'')):
                                    df.at[ridx,'pred_total_model_basis'] = 'model_synthetic'
                        pipeline_stats['model_total_synthetic_fills'] = int(synth_mask_t.sum())
                    if 'pred_margin' in df.columns:
                        if 'pred_margin_model' not in df.columns:
                            df['pred_margin_model'] = np.nan
                            df['pred_margin_model_basis'] = None
                        mmm = pd.to_numeric(df['pred_margin_model'], errors='coerce') if 'pred_margin_model' in df.columns else pd.Series([np.nan]*len(df))
                        disp_mar = pd.to_numeric(df['pred_margin'], errors='coerce')
                        synth_mask_m = mmm.isna() & disp_mar.notna()
                        if synth_mask_m.any():
                            for ridx in df.index[synth_mask_m]:
                                df.at[ridx,'pred_margin_model'] = float(np.clip(disp_mar.loc[ridx], -60, 60))
                                if 'pred_margin_model_basis' in df.columns and (pd.isna(df.at[ridx,'pred_margin_model_basis']) or df.at[ridx,'pred_margin_model_basis'] in (None,'')):
                                    df.at[ridx,'pred_margin_model_basis'] = 'model_synthetic'
                        pipeline_stats['model_margin_synthetic_fills'] = int(synth_mask_m.sum())
                    if 'pred_total_model' in df.columns:
                        pipeline_stats['model_total_missing_after_export'] = int(pd.to_numeric(df['pred_total_model'], errors='coerce').isna().sum())
                    if 'pred_margin_model' in df.columns:
                        pipeline_stats['model_margin_missing_after_export'] = int(pd.to_numeric(df['pred_margin_model'], errors='coerce').isna().sum())
                    # Refresh global unified frame snapshot post synthetic model fills
                    try:
                        _LAST_UNIFIED_FRAME = df.copy()  # uses earlier global declaration
                        pipeline_stats['last_unified_frame_refreshed'] = True
                    except Exception:
                        pipeline_stats['last_unified_frame_refresh_error'] = True
                except Exception as _synth_e:
                    pipeline_stats['model_synthetic_error'] = str(_synth_e)[:160]
            except Exception:
                pipeline_stats['finalize_export_error'] = True
            cols_pref = [
                # Identity
                "game_id","date","home_team","away_team","start_time",
                # Full-game predictions
                "pred_total","pred_margin","pred_total_model","pred_margin_model","pred_total_calibrated","pred_margin_calibrated",
                "pred_total_basis","pred_margin_basis","pred_total_model_basis","pred_margin_model_basis",
                # Halves predictions (1H/2H)
                "pred_total_1h","pred_margin_1h","pred_total_1h_basis","pred_margin_1h_basis",
                "proj_home_1h","proj_away_1h","edge_total_1h","spread_home_1h","market_total_1h",
                "pred_total_2h","pred_margin_2h","proj_home_2h","proj_away_2h","spread_home_2h","market_total_2h",
                # Markets/closing
                "market_total","closing_total","spread_home","closing_spread_home",
                # Edges (full-game)
                "edge_total","edge_total_model","edge_closing","edge_closing_model","edge_margin_model",
                # Team projections (full-game)
                "proj_home","proj_away",
                # Risk/calibration meta
                "pred_total_sigma","pred_margin_sigma","kelly_fraction_total","kelly_fraction_total_adj","kelly_fraction_margin_adj"
            ]
            keep = [c for c in cols_pref if c in df.columns]
            # Normalize basis labels for export: use 'cal' for any calibrated basis
            try:
                if 'pred_total_basis' in df.columns:
                    df['pred_total_basis'] = df['pred_total_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                if 'pred_margin_basis' in df.columns:
                    df['pred_margin_basis'] = df['pred_margin_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
            except Exception:
                pipeline_stats['unified_export_basis_normalize_error'] = True
            uni = df[keep].copy()
            # Deduplicate: prefer rows with non-null predictions/margins
            try:
                uni['__pred_score'] = uni['pred_total'].notna().astype(int) + uni['pred_margin'].notna().astype(int)
                uni = uni.sort_values(['__pred_score'], ascending=False).drop_duplicates(subset=['game_id'])
                dropped_dupes = len(df[keep]) - len(uni)
                if dropped_dupes > 0:
                    pipeline_stats['unified_export_dupes_dropped'] = int(dropped_dupes)
                uni = uni.drop(columns=['__pred_score'])
            except Exception:
                pass
            uni_path = OUT / f"predictions_unified_{export_date}.csv"
            try:
                uni.to_csv(uni_path, index=False)
                pipeline_stats["unified_export_path"] = str(uni_path)
                pipeline_stats["unified_export_rows"] = int(len(uni))
            except Exception:
                pipeline_stats["unified_export_error"] = "write_failed"
            # Write enriched snapshot (full predictions with synthetic fills) for diagnostics
            try:
                enrich_cols = [c for c in df.columns if c not in {"edge_total","edge_total_model","edge_closing","edge_closing_model","edge_margin_model"}]
                enrich = df[enrich_cols].copy()
                # Normalize basis for enriched export as well
                try:
                    if 'pred_total_basis' in enrich.columns:
                        enrich['pred_total_basis'] = enrich['pred_total_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                    if 'pred_margin_basis' in enrich.columns:
                        enrich['pred_margin_basis'] = enrich['pred_margin_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                except Exception:
                    pipeline_stats['enriched_export_basis_normalize_error'] = True
                # Deduplicate enriched similarly
                try:
                    enrich['__pred_score'] = enrich.get('pred_total').notna().astype(int) + enrich.get('pred_margin').notna().astype(int)
                    enrich = enrich.sort_values(['__pred_score'], ascending=False).drop_duplicates(subset=['game_id'])
                    enrich = enrich.drop(columns=['__pred_score'])
                except Exception:
                    pass
                enrich_path = OUT / f"predictions_enriched_{export_date}.csv"
                enrich.to_csv(enrich_path, index=False)
                pipeline_stats["enriched_export_path"] = str(enrich_path)
                pipeline_stats["enriched_export_rows"] = int(len(enrich))
            except Exception:
                pipeline_stats["enriched_export_error"] = "write_failed"
    except Exception:
        pass

    # Persist pipeline stats globally for /api/diag access
    try:
        _LAST_PIPELINE_STATS = dict(pipeline_stats)
    except Exception:
        pass
    # Summary log for remote quick glance
    try:
        if pipeline_stats.get("pred_total_missing_initial") is not None:
            logger.info(
                "fills summary date=%s missing_initial=%s post_fill=%s final_missing=%s synthetic_with_market=%s synthetic_no_market=%s even_margin=%s proj_home=%s proj_away=%s source=%s",
                today_str,
                pipeline_stats.get("pred_total_missing_initial"),
                pipeline_stats.get("pred_total_missing_post_fill"),
                pipeline_stats.get("pred_total_missing_final"),
                pipeline_stats.get("synthetic_baseline_fills"),
                pipeline_stats.get("synthetic_baseline_fills_no_market"),
                pipeline_stats.get("pred_margin_even_fills"),
                pipeline_stats.get("proj_home_rows"),
                pipeline_stats.get("proj_away_rows"),
                _PREDICTIONS_SOURCE_PATH,
            )
    except Exception:
        pass

    # ----------------------------------------------------------------------------------
    # Synthetic shell suppression: hide predictions when file is auto-generated without
    # real model outputs. Criteria: no row has a model-based or calibrated basis and all
    # pred_total/pred_margin values are NaN/None OR basis in {synthetic_baseline, market_copy}.
    # ----------------------------------------------------------------------------------
    try:
        if rows:
            # Relaxed criteria: treat ANY numeric prediction as real regardless of basis.
            # This prevents hiding synthetic fallback predictions that still produce edges.
            has_real = False
            for r in rows:
                pt = r.get("pred_total")
                pm = r.get("pred_margin")
                if ((pt is not None and pt == pt) or (pm is not None and pm == pm)):
                    has_real = True
                    break
            if not has_real:
                # All predictions are missing; only then suppress purely synthetic shells.
                suppressed_rows = 0
                for r in rows:
                    b = (r.get("pred_total_basis") or "")
                    pt = r.get("pred_total")
                    pm = r.get("pred_margin")
                    # Suppress rows lacking BOTH predictions OR explicitly marked market_copy placeholder.
                    if (((pt is None or pt != pt) and (pm is None or pm != pm)) or b in {"market_copy"}):
                        r["pred_total"] = None
                        r["pred_margin"] = None
                        r["pred_total_basis"] = None
                        r["pred_margin_basis"] = None
                        r["pred_synthetic_hidden"] = True
                        suppressed_rows += 1
                pipeline_stats["synthetic_shell_hidden"] = True
                pipeline_stats["synthetic_shell_suppressed_rows"] = suppressed_rows
            else:
                pipeline_stats["synthetic_shell_relaxed"] = True
    except Exception:
        pipeline_stats["synthetic_shell_hide_error"] = True
    # Lightweight instrumentation: persist rows debug snapshot for Render visibility when page appears empty.
    try:
        debug_path = OUT / f"index_rows_debug_{today_str}.json"
        sample_rows = []
        for r in rows[:8]:
            sample_rows.append({
                "game_id": r.get("game_id"),
                "pred_total": r.get("pred_total"),
                "pred_margin": r.get("pred_margin"),
                "pred_total_basis": r.get("pred_total_basis"),
                "market_total": r.get("market_total"),
                "spread_home": r.get("spread_home")
            })
        dbg = {
            "row_count": len(rows),
            "date": str(date_q),
            "basis_counts": pipeline_stats.get("pred_total_basis_counts"),
            "suppressed_shell_rows": pipeline_stats.get("synthetic_shell_suppressed_rows"),
            "sample": sample_rows
        }
        debug_path.write_text(json.dumps(dbg, indent=2), encoding="utf-8")
    except Exception:
        pipeline_stats["index_rows_debug_error"] = True
    # Cache and log pipeline stats for external inspection
    try:
        _LAST_PIPELINE_STATS = dict(pipeline_stats)
        keys_of_interest = [
            'start_dt_aligned_rows_pair',
            'start_dt_aligned_rows_forced_start_src',
            'start_dt_last_mile_pair',
            'start_dt_last_mile_gid',
            'start_dt_fuzzy_updates',
            'time_compare_rows',
            'time_mismatch_count',
            'display_tz_used',
            'schedule_tz_used',
        ]
        snap = {k: _LAST_PIPELINE_STATS.get(k) for k in keys_of_interest}
        logger.info("time-align snap date=%s stats=%s", date_q, snap)
    except Exception:
        pass
    # Build compact status info for header strip (latest archives + finalize hint)
    try:
        status: dict[str, Any] = {}
        # Latest results date
        try:
            import re as _re
            daily_dir = OUT / 'daily_results'
            dates: list[str] = []
            if daily_dir.exists():
                pat = _re.compile(r'^results_(\d{4}-\d{2}-\d{2})\.csv$')
                for p in daily_dir.glob('results_*.csv'):
                    m = pat.match(p.name)
                    if m:
                        dates.append(m.group(1))
            status['results_latest'] = (sorted(set(dates))[-1] if dates else None)
        except Exception:
            status['results_latest'] = None
        # Latest stake sheet
        try:
            import re as _re
            sdates: list[str] = []
            pat = _re.compile(r'^stake_sheet_(\d{4}-\d{2}-\d{2})')
            for p in OUT.glob('stake_sheet*.csv'):
                m = pat.match(p.name)
                if m:
                    sdates.append(m.group(1))
            status['stake_latest'] = (sorted(set(sdates))[-1] if sdates else None)
        except Exception:
            status['stake_latest'] = None
        # Finalize hint for current or requested date
        try:
            use_date = str(date_q).strip() if (date_q and str(date_q).strip()) else dt.datetime.utcnow().strftime('%Y-%m-%d')
            enriched = OUT / f'predictions_unified_enriched_{use_date}.csv'
            n_rows = n_final = n_pending = started = 0
            if enriched.exists():
                df_fin = pd.read_csv(enriched)
                n_rows = int(len(df_fin))
                hs = pd.to_numeric(df_fin.get('home_score'), errors='coerce') if 'home_score' in df_fin.columns else pd.Series(np.nan)
                as_ = pd.to_numeric(df_fin.get('away_score'), errors='coerce') if 'away_score' in df_fin.columns else pd.Series(np.nan)
                is_final = hs.notna() & as_.notna() & ((hs + as_) > 0)
                n_final = int(is_final.sum())
                n_pending = int(n_rows - n_final)
                now = pd.Timestamp.utcnow()
                if '_start_dt' in df_fin.columns:
                    st = pd.to_datetime(df_fin['_start_dt'], errors='coerce')
                    started = int((st.notna() & (st <= now)).sum())
            status['finalize'] = {
                'date': use_date,
                'rows': n_rows,
                'final': n_final,
                'pending': n_pending,
                'started': started,
                'ready': bool(n_rows and n_pending == 0),
            }
        except Exception:
            status['finalize'] = None
        # Display hash (alignment signal)
        try:
            status['display_hash'] = pipeline_stats.get('display_hash') if isinstance(pipeline_stats, dict) else None
        except Exception:
            status['display_hash'] = None
    except Exception:
        status = {}
    # Post-process latest unified/enriched artifacts to normalize basis labels if needed (best-effort)
    try:
        norm_date = str(date_q).strip() if (date_q and str(date_q).strip()) else dt.datetime.utcnow().strftime('%Y-%m-%d')
        cand_files: list[Path] = []
        for pat in [f'predictions_unified_{norm_date}.csv', f'predictions_unified_enriched_{norm_date}.csv', f'predictions_unified_enriched_{norm_date}_force_fill.csv']:
            p = OUT / pat
            if p.exists():
                cand_files.append(p)
        for p in cand_files:
            try:
                df_fix = pd.read_csv(p)
                changed = False
                if 'pred_total_basis' in df_fix.columns:
                    new_t = df_fix['pred_total_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                    if not new_t.equals(df_fix['pred_total_basis']):
                        df_fix['pred_total_basis'] = new_t
                        changed = True
                if 'pred_margin_basis' in df_fix.columns:
                    new_m = df_fix['pred_margin_basis'].replace({'model_calibrated':'cal','calibrated':'cal'})
                    if not new_m.equals(df_fix['pred_margin_basis']):
                        df_fix['pred_margin_basis'] = new_m
                        changed = True
                if changed:
                    df_fix.to_csv(p, index=False)
                    try:
                        pipeline_stats['basis_normalized_file'] = str(p)
                    except Exception:
                        pass
            except Exception:
                continue
    except Exception:
        pass
    # Helper to ensure tz-aware ISO start time per row (index route only)
    def _ensure_index_row(r: dict) -> dict:
        try:
            existing_iso = r.get('start_time_iso')
            if existing_iso and re.search(r'(Z|[+-]\d{2}:?\d{2})$', str(existing_iso)):
                return r
            for cand_key, kwargs in [
                ('_start_dt', {'errors': 'coerce'}),
                # Do NOT assume UTC for commence/start; provider may emit naive local
                ('commence_time', {'errors': 'coerce'}),
                ('start_time', {'errors': 'coerce'}),
            ]:
                val = r.get(cand_key)
                if not val:
                    continue
                s = str(val)
                # If value carries explicit offset/Z, parse with utc=True; else parse naive
                has_offset = bool(re.search(r'(Z|[+-]\d{2}:?\d{2})$', s))
                if 'Z' in s:
                    s = s.replace('Z', '+00:00')
                ts = pd.to_datetime(s, errors='coerce', utc=has_offset)
                if pd.isna(ts):
                    continue
                try:
                    if getattr(ts, 'tzinfo', None) is None:
                        # Treat naive as Central Time by default to avoid UTC rollover
                        ts = ts.tz_localize('America/Chicago')
                    else:
                        ts = ts.tz_convert('UTC')
                except Exception:
                    pass
                # Ensure UTC ISO for stable downstream
                try:
                    ts_utc = ts.tz_convert('UTC') if getattr(ts, 'tzinfo', None) else ts
                except Exception:
                    ts_utc = ts
                r['start_time_iso'] = ts_utc.strftime('%Y-%m-%dT%H:%M:%SZ')
                break
        except Exception:
            pass
        return r
    # Apply midnight drift correction across rows (index page)
    def _ensure_half_keys(record: dict) -> dict:
        # Ensure half-time and closing keys exist to avoid Jinja UndefinedError
        half_keys = [
            "spread_home_1h",
            "spread_home_2h",
            "closing_spread_home_1h",
            "closing_spread_home_2h",
            "total_1h",
            "total_2h",
            "closing_total_1h",
            "closing_total_2h",
            "pred_margin_1h",
            "pred_margin_2h",
            "pred_total_1h",
            "pred_total_2h",
        ]
        for k in half_keys:
            if k not in record:
                record[k] = None
        return record

    # Force Central Time display based purely on local start time
    def _apply_central_display(r: dict) -> dict:
        try:
            # Prefer venue-local first; then schedule local; then trusted ISO; finally offset-bearing raw fields
            ts = None
            # 1) Venue-local precedence
            vloc = str(r.get('start_time_local_venue') or '').strip()
            vabr = str(r.get('start_tz_abbr_venue') or '').strip().upper()
            tz_map = {
                'UTC': 0, 'Z': 0,
                'HST': -10, 'AKST': -9,
                'PST': -8, 'PDT': -7,
                'MST': -7, 'MDT': -6,
                'CST': -6, 'CDT': -5,
                'EST': -5, 'EDT': -4,
            }
            if vloc and vabr in tz_map:
                parts = vloc.split()
                if len(parts) >= 2:
                    dstr, tstr = parts[0], parts[1]
                    try:
                        naive = dt.datetime.strptime(f"{dstr} {tstr}", "%Y-%m-%d %H:%M")
                        off = tz_map[vabr]
                        tzinfo = dt.timezone(dt.timedelta(hours=off))
                        ts = pd.Timestamp(naive.replace(tzinfo=tzinfo)).tz_convert('UTC')
                    except Exception:
                        ts = None
            # 2) Schedule local + abbr
            if (ts is None) or pd.isna(ts):
                loc = str(r.get('start_time_local') or r.get('start_time_local_venue') or '').strip()
                abbr = str(r.get('start_tz_abbr') or r.get('start_tz_abbr_venue') or '').strip().upper()
                if loc and abbr in tz_map:
                    parts = loc.split()
                    if len(parts) >= 2:
                        dstr, tstr = parts[0], parts[1]
                        try:
                            naive = dt.datetime.strptime(f"{dstr} {tstr}", "%Y-%m-%d %H:%M")
                            off = tz_map[abbr]
                            tzinfo = dt.timezone(dt.timedelta(hours=off))
                            ts = pd.Timestamp(naive.replace(tzinfo=tzinfo)).tz_convert('UTC')
                        except Exception:
                            ts = None
            # 3) Trusted ISO with explicit offset/Z
            if (ts is None) or pd.isna(ts):
                iso = r.get('start_time_iso')
                if iso:
                    s = str(iso)
                    has_offset = bool(re.search(r'(Z|[+-]\d{2}:?\d{2})$', s))
                    ts = pd.to_datetime(s if 'Z' in s else s.replace('Z','+00:00'), errors='coerce', utc=has_offset)
            if (ts is None) or pd.isna(ts):
                # 4) Final fallback: parse commence/start only if they carry offsets
                for cand in [r.get('commence_time'), r.get('start_time')]:
                    if not cand:
                        continue
                    s = str(cand)
                    has_offset = bool(re.search(r'(Z|[+-]\d{2}:?\d{2})$', s))
                    try:
                        t = pd.to_datetime(s if 'Z' in s else s.replace('Z','+00:00'), errors='coerce', utc=has_offset)
                        if pd.notna(t):
                            ts = t
                            break
                    except Exception:
                        continue
            if (ts is None) or pd.isna(ts):
                return r
            try:
                tz_local = ts.tz_convert('America/Chicago')
            except Exception:
                tz_local = ts
            # Build display fields pinned to Central Time, deriving date from Central clock
            tz_abbr = getattr(tz_local, 'tzname', lambda: 'CST')() if hasattr(tz_local, 'tzname') else 'CST'
            disp_date = tz_local.strftime('%Y-%m-%d')
            disp_time = tz_local.strftime('%H:%M')
            r['display_date'] = disp_date
            r['display_time_str'] = f"{disp_date} {disp_time} {tz_abbr or 'CST'}"
            # Mirror into legacy display field used by templates
            r['start_time_display'] = r['display_time_str']
            # Do NOT overwrite `date` here if it already carries slate semantics; it
            # is set upstream from schedule artifacts and `_correct_midnight_drift`.
            if not r.get('date'):
                r['date'] = disp_date
        except Exception:
            pass
        return r

    safe_rows = []
    # Odds-local enrichment: ensure start_time_local/start_tz_abbr populated from provider file for full coverage
    _ODDS_LOCAL_CACHE: dict[str, dict] = {}
    def _load_odds_local_map(date_str: str | None) -> dict[str, dict]:
        if not date_str:
            return {}
        key = str(date_str)
        if _ODDS_LOCAL_CACHE:
            # already loaded once for this render
            return _ODDS_LOCAL_CACHE
        try:
            p = OUT / f"games_with_odds_{key}.csv"
            if not p.exists():
                return {}
            dfo = pd.read_csv(p)
            if 'game_id' in dfo.columns:
                dfo['game_id'] = dfo['game_id'].astype(str).str.replace(r'\.0$','', regex=True)
            cols = ['game_id','start_time_local','start_tz_abbr','start_time_local_venue','start_tz_abbr_venue']
            keep = [c for c in cols if c in dfo.columns]
            mp: dict[str, dict] = {}
            for _r in dfo[keep].to_dict(orient='records'):
                gid = str(_r.get('game_id') or '')
                if gid:
                    mp[gid] = _r
            _ODDS_LOCAL_CACHE.update(mp)
            return _ODDS_LOCAL_CACHE
        except Exception:
            return {}
    _odds_local = _load_odds_local_map(str(date_q) if date_q else None)
    for _r in rows:
        # Start from the branded row but ensure global index defaults
        r = _ensure_index_row_global(dict(_r))
        r = _ensure_half_keys(r)
        # Enrich missing local fields from odds map by game_id
        try:
            gid = str(r.get('game_id') or '')
            if gid and _odds_local and gid in _odds_local:
                od = _odds_local[gid]
                for k in ['start_time_local','start_tz_abbr','start_time_local_venue','start_tz_abbr_venue']:
                    if not r.get(k) and od.get(k):
                        r[k] = od.get(k)
        except Exception:
            pass
        # Canonicalize start_time_iso from any legacy fields
        try:
            if not r.get('start_time_iso'):
                iso = _derive_start_iso(r)
                if iso:
                    r['start_time_iso'] = iso
        except Exception:
            pass
        # Apply shared normalization helpers for rollover + site display
        try:
            r = _backfill_start_fields(r)
        except Exception:
            pass
        try:
            r = _correct_midnight_drift(r, slate_date=str(date_q) if date_q else None)
        except Exception:
            pass
        try:
            r = _apply_site_display_global(r)
        except Exception:
            pass
        # Final enforcement: if display_time_str still missing, derive minimal Central string
        try:
            if not r.get('display_time_str'):
                loc = str(r.get('start_time_local') or r.get('start_time_local_venue') or '').strip()
                abbr = str(r.get('start_tz_abbr') or r.get('start_tz_abbr_venue') or '').strip().upper()
                if loc and abbr:
                    parts = loc.split()
                    hhmm = parts[1] if len(parts) >= 2 else parts[0]
                    disp_date = str(r.get('display_date') or r.get('date') or '')
                    r['display_date'] = disp_date
                    r['display_time_str'] = f"{disp_date} {hhmm} {abbr}"
                else:
                    iso = r.get('start_time_iso') or r.get('start_time')
                    if iso:
                        ts = pd.to_datetime(str(iso).replace('Z','+00:00'), errors='coerce', utc=True)
                        if pd.notna(ts):
                            ct = ts.tz_convert('America/Chicago')
                            disp_date = str(r.get('display_date') or ct.strftime('%Y-%m-%d'))
                            r['display_date'] = disp_date
                            ab = getattr(ct, 'tzname', lambda: 'CST')()
                            r['display_time_str'] = f"{disp_date} {ct.strftime('%H:%M')} {ab}"
        except Exception:
            pass
        # Align start with odds commence_time when available (authoritative)
        try:
            if isinstance(odds, pd.DataFrame) and 'game_id' in r:
                if not hasattr(app, '_commence_map_cache'):
                    o2 = odds.copy()
                    try:
                        o2['game_id'] = o2['game_id'].astype(str)
                        o2['_ct_raw'] = pd.to_datetime(o2['commence_time'], errors='coerce', utc=False)
                        # If offset/Z present, parse as UTC; else attach schedule tz then convert
                        has_off = o2['commence_time'].astype(str).str.contains(r"[+-]\d{2}:\d{2}$", regex=True) | o2['commence_time'].astype(str).str.endswith('Z')
                        ct_off = pd.to_datetime(o2['commence_time'].astype(str).where(has_off, None).str.replace('Z','+00:00', regex=False), errors='coerce', utc=True)
                        ct_naive = pd.to_datetime(o2['commence_time'].astype(str).where(~has_off, None), errors='coerce', utc=False)
                        if ct_naive.notna().any():
                            try:
                                from zoneinfo import ZoneInfo as _ZoneInfo
                                sched_tz_name = os.getenv('SCHEDULE_TZ') or os.getenv('NCAAB_SCHEDULE_TZ') or 'America/New_York'
                                try:
                                    sched_tz = _ZoneInfo(sched_tz_name)
                                except Exception:
                                    sched_tz = dt.datetime.now().astimezone().tzinfo
                                ct_naive = ct_naive.map(lambda x: x.replace(tzinfo=sched_tz) if pd.notna(x) and getattr(x,'tzinfo',None) is None else x)
                                ct_naive = ct_naive.dt.tz_convert(dt.timezone.utc)
                            except Exception:
                                pass
                        o2['_ct'] = ct_off.where(ct_off.notna(), ct_naive)
                        ct_map = o2.groupby('game_id')['_ct'].min()
                        app._commence_map_cache = ct_map
                    except Exception:
                        app._commence_map_cache = pd.Series(dtype='datetime64[ns]')
                gid = str(r.get('game_id'))
                if gid and hasattr(app, '_commence_map_cache'):
                    ct = app._commence_map_cache.get(gid)
                    if ct is not None and pd.notna(ct):
                        r['_start_dt'] = ct
        except Exception:
            pass
        # Enforce canonical site display before templating
        try:
            r = _apply_site_display_global(r)
        except Exception:
            pass
        safe_rows.append(r)
    # Sort rows by Central start time to avoid perceived gaps
    try:
        def _central_ts(row: dict):
            iso = row.get('_start_dt') or row.get('start_time_iso') or row.get('start_time')
            if not iso:
                return pd.NaT
            s = str(iso)
            s2 = s if 'Z' in s else s.replace('Z','+00:00')
            ts = pd.to_datetime(s2, errors='coerce', utc=True)
            try:
                return ts.tz_convert('America/Chicago') if pd.notna(ts) else ts
            except Exception:
                return ts
        safe_rows = sorted(safe_rows, key=lambda r: (_central_ts(r) or pd.NaT))
    except Exception:
        pass
    return render_template(
        "index.html",
        rows=safe_rows,
        total_rows=total_rows,
        date_val=date_q,
        top_picks=top_picks,
        accuracy=accuracy,
        uniform_note=uniform_note,
        dynamic_css=dynamic_css,
        coverage_note=coverage_note,
        results_note=results_note,
        show_edges=True,
        coverage=coverage_summary,
        archive_dates=archive_dates,
        show_bootstrap=show_bootstrap,
        compact_mode=compact_mode,
        bootstrap_url=bootstrap_url,
        show_diag=show_diag,
        diag_url=diag_url,
        fused_bootstrap_url=fused_bootstrap_url,
        refresh_odds_url=refresh_odds_url,
        removed_empty_rows=removed_empty_rows,
        status=status,
        pipeline_stats=pipeline_stats,
    )

# --------------------------------------------------------------
# Calibration diagnostics routes (module-level registration)
# --------------------------------------------------------------
@app.route('/api/calibration_diagnostic')
def calibration_diagnostic():  # type: ignore
    date_q = (request.args.get('date') or '').strip()  # type: ignore
    try:
        from src.diagnose_calibration import diagnose  # type: ignore
    except Exception:
        return jsonify({'error':'diagnose_import_failed'}), 500  # type: ignore
    try:
        diag_df = diagnose(date_q)
    except Exception as e:
        return jsonify({'error':'diagnose_exec_failed','detail':str(e)[:160]}), 500  # type: ignore
    summary = diag_df.groupby('reason').size().rename('count').reset_index() if not diag_df.empty else []
    resp = {
        'date': date_q or datetime.utcnow().strftime('%Y-%m-%d'),  # type: ignore
        'summary': summary.to_dict(orient='records') if not isinstance(summary, list) else summary,
        'detail_sample': diag_df.head(50).to_dict(orient='records'),
        'total_games': int(len(diag_df)),
    }
    # Add basis shares from snapshot if available
    try:
        out_snap = OUT / 'pipeline_stats_last.json'
        if out_snap.exists():
            import json
            ps = json.loads(out_snap.read_text())
            for k in [
                'basis_share_total_cal','basis_share_total_model_missing_cal','basis_share_total_cal_est',
                'basis_share_margin_cal','basis_share_margin_model_missing_cal','basis_share_margin_cal_est']:
                if k in ps:
                    resp[k] = ps[k]
    except Exception:
        pass
    return jsonify(resp)  # type: ignore

@app.route('/api/calibration_summary')
def calibration_summary():  # type: ignore
    try:
        horizon = int((request.args.get('days') or '14').strip())  # type: ignore
    except Exception:
        horizon = 14
    files = sorted([p for p in OUT.glob('predictions_display_*.csv')])[-horizon:]
    summary_rows = []
    for pf in files:
        try:
            dfp = pd.read_csv(pf)
            date_part = pf.stem.replace('predictions_display_','')
            total_share = float((dfp['pred_total_basis'].astype(str)=='cal').mean()) if 'pred_total_basis' in dfp.columns else None
            margin_share = float((dfp['pred_margin_basis'].astype(str)=='cal').mean()) if 'pred_margin_basis' in dfp.columns else None
            summary_rows.append({
                'date': date_part,
                'rows': int(len(dfp)),
                'total_cal_share': total_share,
                'margin_cal_share': margin_share,
            })
        except Exception:
            pass
    return jsonify({'days': horizon, 'summary': summary_rows, 'count_files': len(files)})  # type: ignore


@app.route('/api/calibration_health')
def calibration_health():  # type: ignore
    """Lightweight health snapshot for calibration coverage & gating."""
    try:
        from datetime import datetime as _dt
        import os, json, statistics
        cal_thr = float(os.getenv('CAL_SHARE_MIN','0.60'))
        snap_path = OUT / 'pipeline_stats_last.json'
        payload: dict[str, Any] = {}
        if snap_path.exists():
            try:
                with open(snap_path,'r',encoding='utf-8') as fh:
                    ps = json.load(fh)
                for k in [
                    'basis_share_total_cal','basis_share_margin_cal','alert_low_cal_total','alert_low_cal_margin',
                    'deployment_gate_blocked','deployment_gate_low_days','deployment_gate_threshold'
                ]:
                    if k in ps:
                        payload[k] = ps[k]
            except Exception:
                payload['snapshot_error'] = True
        # Recent trend (7 days)
        files = sorted([p for p in OUT.glob('predictions_display_*.csv')])[-7:]
        trend = []
        for pf in files:
            try:
                dpf = pd.read_csv(pf)
                share_t = float((dpf['pred_total_basis'].astype(str)=='cal').mean()) if 'pred_total_basis' in dpf.columns else None
                share_m = float((dpf['pred_margin_basis'].astype(str)=='cal').mean()) if 'pred_margin_basis' in dpf.columns else None
                trend.append({'date':pf.stem.replace('predictions_display_',''),'total_cal_share':share_t,'margin_cal_share':share_m})
            except Exception:
                pass
        payload['trend'] = trend
        if trend:
            try:
                t_vals = [r['total_cal_share'] for r in trend if isinstance(r.get('total_cal_share'), (int,float))]
                if t_vals:
                    payload['trend_total_median'] = statistics.median(t_vals)
                m_vals = [r['margin_cal_share'] for r in trend if isinstance(r.get('margin_cal_share'), (int,float))]
                if m_vals:
                    payload['trend_margin_median'] = statistics.median(m_vals)
            except Exception:
                payload['trend_stats_error'] = True
        payload['threshold'] = cal_thr
        payload['generated_at'] = _dt.utcnow().isoformat()
        return jsonify(payload)  # type: ignore
    except Exception as _h_e:
        return jsonify({'error':'health_failed','detail':str(_h_e)[:160]}), 500  # type: ignore



@app.route("/api/render-diagnostics")
def api_render_diagnostics():
    """Return detailed diagnostics for the render pipeline for a given date (or resolved date)."""
    date_q = (request.args.get("date") or "").strip()
    # Re-run lightweight portions to avoid duplicating heavy enrichment.
    games = _load_games_current()
    preds = _load_predictions_current()
    odds = _load_odds_joined(date_q)
    out: dict[str, Any] = {
        "date_param": date_q,
        "outputs_dir": str(OUT),
        "games_load_rows": len(games),
        "preds_load_rows": len(preds),
        "odds_load_rows": len(odds),
    }
    # Date resolution logic (similar to index start) for transparency
    if not date_q:
        try:
            today_str = _today_local().strftime("%Y-%m-%d")
        except Exception:
            today_str = None
        if today_str and "date" in games.columns and (games["date"].astype(str) == today_str).any():
            date_q = today_str
        elif "date" in preds.columns and not preds.empty:
            try:
                date_q = pd.to_datetime(preds["date"]).max().strftime("%Y-%m-%d")
            except Exception:
                date_q = preds["date"].dropna().astype(str).max()
        out["resolved_date"] = date_q
    # Filter by date
    games_date = games
    preds_date = preds
    if date_q:
        if "date" in games.columns:
            games_date = games[games["date"].astype(str) == date_q]
        if "date" in preds.columns:
            preds_date = preds[preds["date"].astype(str) == date_q]
    out["games_after_date"] = len(games_date)
    out["preds_after_date"] = len(preds_date)
    # Sample IDs missing preds or odds
    try:
        g_ids = set(games_date.get("game_id", pd.Series()).astype(str)) if not games_date.empty else set()
        p_ids = set(preds_date.get("game_id", pd.Series()).astype(str)) if not preds_date.empty else set()
        o_ids = set(odds.get("game_id", pd.Series()).astype(str)) if not odds.empty else set()
        out["games_without_preds"] = sorted(list(g_ids - p_ids))[:40]
        out["games_without_odds"] = sorted(list(g_ids - o_ids))[:40]
        out["preds_without_games"] = sorted(list(p_ids - g_ids))[:40]
    except Exception:
        pass
    return jsonify(out)

@app.route("/api/rows-today")
def api_rows_today():
    """Lightweight snapshot of today's enriched prediction rows.

    Returns JSON with:
      date: resolved today string
      row_count: number of rows in predictions_enriched_<today>.csv (or fallback)
      basis_counts: counts for pred_total_basis (if present)
      sample: up to 8 sample rows (selected columns)
      source: file path used
    """
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
        path = OUT / f"predictions_enriched_{today_str}.csv"
        fallback = OUT / "predictions_enriched.csv"
        src = path if path.exists() else (fallback if fallback.exists() else None)
        if src is None:
            return jsonify({"date": today_str, "row_count": 0, "error": "no enriched predictions file"})
        df = _safe_read_csv(src)
        basis_counts = None
        if not df.empty and "pred_total_basis" in df.columns:
            try:
                basis_counts = df["pred_total_basis"].astype(str).value_counts().to_dict()
            except Exception:
                basis_counts = None
        sample = []
        if not df.empty:
            for _, r in df.head(8).iterrows():
                sample.append({
                    "game_id": r.get("game_id"),
                    "pred_total": r.get("pred_total"),
                    "pred_margin": r.get("pred_margin"),
                    "pred_total_basis": r.get("pred_total_basis"),
                    "market_total": r.get("market_total"),
                    "spread_home": r.get("spread_home")
                })
        return jsonify({
            "date": today_str,
            "row_count": int(len(df)),
            "basis_counts": basis_counts,
            "sample": sample,
            "source": str(src)
        })
    except Exception as e:
        return jsonify({"error": str(e)})

@app.route("/api/coverage-depth")
def api_coverage_depth():
    """Return quotes_count distribution, segmentation & blend suppression stats.

    Provides:
      quotes: total rows with quotes_count, distribution map, min/median/max, pct_low (quotes_count <2)
      segmentation: rows with seg_n_rows>0, pct_seg_used, mean_seg_n_rows_used, mean_blend_weight_used
      blend: suppressed vs effective counts from last pipeline stats
      meta: total prediction rows evaluated & timestamp
    """
    try:
        import statistics as _stats  # type: ignore
    except Exception:  # pragma: no cover
        _stats = None  # type: ignore
    preds = _load_predictions_current()
    total = len(preds)
    quotes_counts: list[int] = []
    seg_counts: list[float] = []
    blend_weights_used: list[float] = []
    if not preds.empty:
        if "quotes_count" in preds.columns:
            try:
                qc = pd.to_numeric(preds["quotes_count"], errors="coerce")
                quotes_counts = [int(x) for x in qc.dropna().tolist()]
            except Exception:
                quotes_counts = []
        if "seg_n_rows" in preds.columns:
            try:
                sc = pd.to_numeric(preds["seg_n_rows"], errors="coerce")
                seg_counts = [float(x) for x in sc.dropna().tolist() if x > 0]
            except Exception:
                seg_counts = []
        if "blend_weight" in preds.columns:
            try:
                bw = pd.to_numeric(preds["blend_weight"], errors="coerce")
                blend_weights_used = [float(x) for x in bw.dropna().tolist() if x > 0]
            except Exception:
                blend_weights_used = []
    # quotes distribution
    quotes_dist: dict[str, int] = {}
    for q in quotes_counts:
        key = str(q)
        quotes_dist[key] = quotes_dist.get(key, 0) + 1
    quotes_min = min(quotes_counts) if quotes_counts else None
    quotes_max = max(quotes_counts) if quotes_counts else None
    quotes_median = (_stats.median(quotes_counts) if (_stats and quotes_counts) else None)
    pct_low_quotes = round(len([x for x in quotes_counts if x < 2]) / len(quotes_counts), 3) if quotes_counts else None
    # segmentation stats
    seg_used = len(seg_counts)
    pct_seg_used = round(seg_used / total, 3) if total and seg_used else None
    mean_seg_n_rows = (round(sum(seg_counts) / seg_used, 2) if seg_used else None)
    mean_blend_weight_used = (round(sum(blend_weights_used) / len(blend_weights_used), 3) if blend_weights_used else None)
    # blend suppression from last pipeline stats
    blend_info: dict[str, Any] = {}
    if isinstance(_LAST_PIPELINE_STATS, dict):
        s = _LAST_PIPELINE_STATS
        suppressed = s.get("blend_rows_suppressed")
        effective = s.get("blend_rows_effective")
        if suppressed is not None or effective is not None:
            blend_info = {
                "suppressed": suppressed,
                "effective": effective,
                "pct_suppressed": (round(suppressed / (suppressed + effective), 3) if isinstance(suppressed, (int,float)) and isinstance(effective, (int,float)) and (suppressed + effective) > 0 else None)
            }
    payload = {
        "meta": {
            "ts": dt.datetime.utcnow().isoformat(timespec="seconds"),
            "total_rows": total,
        },
        "quotes": {
            "counts_present": len(quotes_counts),
            "distribution": quotes_dist,
            "min": quotes_min,
            "median": quotes_median,
            "max": quotes_max,
            "pct_low": pct_low_quotes,
        },
        "segmentation": {
            "rows_used": seg_used,
            "pct_seg_used": pct_seg_used,
            "mean_seg_n_rows_used": mean_seg_n_rows,
            "mean_blend_weight_used": mean_blend_weight_used,
        },
        "blend": blend_info,
    }
    return jsonify(payload)


@app.route("/api/diag")
def api_diag():
    """Return last pipeline_stats snapshot plus live lightweight context.

    Provides quick remote introspection without re-running full index synthesis.
    """
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    # Prefer enriched/unified exports if present for more representative sample
    preds = pd.DataFrame()
    try:
        if today_str:
            enrich_path = OUT / f"predictions_enriched_{today_str}.csv"
            if enrich_path.exists():
                preds = pd.read_csv(enrich_path)
        if preds.empty and today_str:
            uni_path = OUT / f"predictions_unified_{today_str}.csv"
            if uni_path.exists():
                preds = pd.read_csv(uni_path)
        if preds.empty:
            preds = _load_predictions_current()
    except Exception:
        preds = _load_predictions_current()
    sample_rows: list[dict[str, Any]] = []
    try:
        if not preds.empty:
            # Deduplicate again in case enriched/unified not yet deduped for this date
            try:
                preds['__pred_score'] = preds.get('pred_total').notna().astype(int) + preds.get('pred_margin').notna().astype(int)
                preds = preds.sort_values(['__pred_score'], ascending=False).drop_duplicates(subset=['game_id'])
                preds = preds.drop(columns=['__pred_score'])
            except Exception:
                pass
            keep = [c for c in ["game_id","date","pred_total","pred_total_basis","pred_margin","pred_margin_basis","market_total"] if c in preds.columns]
            # Prefer showing filled prediction rows; if fewer than 8, pad with remaining
            filled = preds[preds['pred_total'].notna() | preds['pred_margin'].notna()]
            base_sample = filled.head(8)
            if len(base_sample) < 8:
                remaining = preds[~preds['game_id'].isin(base_sample['game_id'])].head(8 - len(base_sample))
                base_sample = pd.concat([base_sample, remaining], ignore_index=True)
            sample_rows = base_sample[keep].to_dict(orient="records")
    except Exception:
        sample_rows = []
    missing_pred_rows = int((preds.get('pred_total').isna() & preds.get('pred_margin').isna()).sum()) if 'pred_total' in preds.columns and 'pred_margin' in preds.columns else None
    filled_pred_rows = int(len(preds) - missing_pred_rows) if missing_pred_rows is not None else None
    global _LAST_PIPELINE_STATS, _PREDICTIONS_SOURCE_PATH
    out: dict[str, Any] = {
        "today": today_str,
        "predictions_source": _PREDICTIONS_SOURCE_PATH,
        "last_pipeline_stats": _LAST_PIPELINE_STATS,
        "predictions_rows": int(len(preds)) if not preds.empty else 0,
        "sample": sample_rows,
        "pred_rows_filled": filled_pred_rows,
        "pred_rows_missing": missing_pred_rows,
    }
    try:
        # Surface current prefer_cal default if active today via last stats
        if _LAST_PIPELINE_STATS and isinstance(_LAST_PIPELINE_STATS, dict):
            pc = _LAST_PIPELINE_STATS.get("prefer_cal")
            if pc is not None:
                out["prefer_cal"] = bool(pc)
    except Exception:
        pass
    # Curated instrumentation subset for quick remote visibility without scanning full stats dict
    try:
        instr_keys = [
            "calibration_precedence_overrides_total",
            "calibration_precedence_total_candidates",
            "calibration_precedence_overrides_total_unified",
            "calibration_precedence_overrides_margin",
            "calibration_precedence_margin_candidates",
            "calibration_recomputed_edge_closing_total",
            "calibration_recomputed_edge_closing_margin",
            "second_pass_pred_total_fill_candidates",
            "second_pass_pred_total_fills",
            "second_pass_pred_total_fills_protected_skipped",
            "synthetic_shell_hidden",
            "synthetic_shell_suppressed_rows",
            # Reconstruction + final cal enforcement visibility
            "reconstructed_pred_total_rows",
            "reconstructed_pred_margin_rows",
            "reconstructed_proj_rows",
            "reconstructed_proj_from_total_only_rows",
            "final_cal_rows_total_present",
            "final_cal_rows_margin_present",
            "final_cal_override_total_rows",
            "final_cal_override_margin_rows",
        ]
        last_stats = _LAST_PIPELINE_STATS or {}
        instrumentation = {k: last_stats.get(k) for k in instr_keys if k in last_stats}
        if instrumentation:
            out["instrumentation"] = instrumentation
    except Exception:
        out["instrumentation_error"] = True
    return jsonify(out)


@app.route("/api/preds-summary")
def api_preds_summary():
    """Summarize prediction basis distribution and calibration/blend diagnostics for a date.

    Query params:
      - date: YYYY-MM-DD (optional; defaults to today or most recent)
    """
    # Robust wrapper: never raise; always surface error details JSON-safe.
    def _json_safe(x):
        try:
            import numpy as _np
            import pandas as _pd
        except Exception:
            _np = None  # type: ignore
            _pd = None  # type: ignore
        if x is None:
            return None
        if isinstance(x, (int, float, str, bool)):
            return x
        if _np and isinstance(x, _np.generic):  # numpy scalar
            return x.item()
        if isinstance(x, dict):
            return {str(k): _json_safe(v) for k, v in x.items()}
        if isinstance(x, (list, tuple, set)):
            return [_json_safe(i) for i in list(x)]
        if 'Series' in str(type(x)):
            # pandas Series -> dict of python scalars
            try:
                return {str(k): _json_safe(v) for k, v in dict(x).items()}
            except Exception:
                try:
                    return [ _json_safe(v) for v in list(x) ]
                except Exception:
                    return str(x)
        return str(x)
    date_q = (request.args.get("date") or "").strip()
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    summary: dict[str, Any] = {"date": date_q or today_str, "rows": 0}
    try:
        # Load enriched/unified or current
        preds = pd.DataFrame()
        try:
            if date_q:
                enrich_path = OUT / f"predictions_enriched_{date_q}.csv"
                if enrich_path.exists():
                    preds = pd.read_csv(enrich_path)
            if preds.empty and date_q:
                uni_path = OUT / f"predictions_unified_{date_q}.csv"
                if uni_path.exists():
                    preds = pd.read_csv(uni_path)
            if preds.empty and today_str:
                enrich_path = OUT / f"predictions_enriched_{today_str}.csv"
                if enrich_path.exists():
                    preds = pd.read_csv(enrich_path)
            if preds.empty and today_str:
                uni_path = OUT / f"predictions_unified_{today_str}.csv"
                if uni_path.exists():
                    preds = pd.read_csv(uni_path)
            if preds.empty:
                preds = _load_predictions_current()
        except Exception:
            preds = _load_predictions_current()
        summary["rows"] = int(len(preds)) if not preds.empty else 0
        if not preds.empty:
            # Basis value_counts (JSON-safe)
            for col in ("pred_total_basis","pred_margin_basis"):
                if col in preds.columns:
                    try:
                        vc_raw = preds[col].value_counts(dropna=True).to_dict()
                        summary[f"{col}_counts"] = {str(k): int(v) for k, v in vc_raw.items()}
                    except Exception as _vc_e:
                        summary[f"{col}_counts_error"] = str(_vc_e)
            # Calibrated columns presence
            summary["has_pred_total_calibrated"] = bool("pred_total_calibrated" in preds.columns)
            summary["has_pred_margin_calibrated"] = bool("pred_margin_calibrated" in preds.columns)
            # Group distributions
            def _grp(ser):
                try:
                    ser2 = ser.fillna("").astype(str)
                    cats = {
                        "CAL": int(ser2.str.startswith("model_calibrated").sum() + (ser2 == "cal").sum()),
                        "RAW": int(ser2.isin(["model_raw","model","model_v1"]).sum()),
                        "BLEND": int(ser2.isin(["blend","blended","blended_model_baseline","blend_model_market","blended_low"]).sum()),
                        "SYN": int(ser2.isin(["synthetic_baseline","synthetic_baseline_nomkt","synthetic_baseline_final","synthetic","features_derived","market_copy"]).sum()),
                    }
                    cats["OTHER"] = int(len(ser2)) - sum(int(v) for v in cats.values())
                    return cats
                except Exception as _g_e:
                    return {"error": str(_g_e)}
            if "pred_total_basis" in preds.columns:
                summary["total_basis_groups"] = _grp(preds["pred_total_basis"])  # type: ignore
            if "pred_margin_basis" in preds.columns:
                summary["margin_basis_groups"] = _grp(preds["pred_margin_basis"])  # type: ignore
            # Non-null calibration counts for visibility
            if 'pred_total_calibrated' in preds.columns:
                summary['pred_total_calibrated_nonnull'] = int(pd.to_numeric(preds['pred_total_calibrated'], errors='coerce').notna().sum())
            if 'pred_margin_calibrated' in preds.columns:
                summary['pred_margin_calibrated_nonnull'] = int(pd.to_numeric(preds['pred_margin_calibrated'], errors='coerce').notna().sum())
        # Highlights from last pipeline stats (JSON-safe coercion)
        try:
            hi_keys = [
                "calibration_precedence_overrides_total",
                "calibration_precedence_overrides_margin",
                "blend_rows_effective",
                "blend_rows_suppressed",
                "blend_rows_ineffective",
                "blend_uniform_reverted",
                "pred_total_basis_counts",
                "pred_margin_basis_counts",
                "prefer_cal",
            ]
            if _LAST_PIPELINE_STATS:
                raw_highlights = {k: _LAST_PIPELINE_STATS.get(k) for k in hi_keys if k in _LAST_PIPELINE_STATS}
                summary["highlights"] = _json_safe(raw_highlights)
        except Exception as _hl_e:
            summary["highlights_error"] = str(_hl_e)
    except Exception as e:
        # Catastrophic failure path
        summary["fatal_error"] = str(e)
        try:
            import traceback as _tb
            summary["fatal_trace"] = _tb.format_exc()[-900:]
        except Exception:
            pass
    return jsonify(_json_safe(summary))


@app.route("/dashboard")
def dashboard():
    metrics = _load_eval_metrics()
    return render_template("dashboard.html", metrics=metrics)

@app.route("/api/odds")
def api_odds():
    """Return per-game full-game totals odds quotes and aggregated market total.

    Structure: { rows: [ {game_id, market_total, commence_time, quotes: [ {book,total,price_over,price_under} ] } ], n: <int> }
    """
    date_q = (request.args.get("date") or "").strip()
    odds = _load_odds_joined(date_q or None)
    agg = _aggregate_full_game_totals(odds)
    rows = agg.to_dict(orient="records") if not agg.empty else []
    return jsonify({"n": len(rows), "rows": rows})


@app.route("/api/dates")
def api_dates():
    """Return sorted unique dates available from predictions (preferred) or games."""
    for name in ("predictions_week.csv", "predictions.csv", "predictions_all.csv"):
        df = _safe_read_csv(OUT / name)
        if not df.empty and "date" in df.columns:
            try:
                dates = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d").dropna().unique().tolist()
            except Exception:
                dates = df["date"].dropna().astype(str).unique().tolist()
            dates = sorted(set(dates))
            return jsonify({"dates": dates, "source": name})
    # fallback to daily_results summaries
    dr_dir = OUT / "daily_results"
    if dr_dir.exists():
        try:
            dates = []
            for p in dr_dir.glob("summary_*.json"):
                d = p.stem.split("_")[-1]
                dates.append(d)
            dates = sorted(set(dates))
            if dates:
                return jsonify({"dates": dates, "source": "daily_results"})
        except Exception:
            pass
    # fallback to games
    for name in ("games_curr.csv", "games.csv", "games_all.csv"):
        df = _safe_read_csv(OUT / name)
        if not df.empty and "date" in df.columns:
            try:
                dates = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d").dropna().unique().tolist()
            except Exception:
                dates = df["date"].dropna().astype(str).unique().tolist()
            dates = sorted(set(dates))
            return jsonify({"dates": dates, "source": name})
    return jsonify({"dates": [], "source": None})


@app.route("/api/data-status")
def api_data_status():
    """Quick status of common output files and row counts."""
    files = [
        "games_curr.csv", "games_all.csv", "boxscores.csv", "boxscores_last2.csv",
        "features_curr.csv", "features_all.csv", "features_last2.csv",
        "predictions_week.csv", "predictions_all.csv", "predictions_last2.csv",
        "games_with_odds_today.csv", "games_with_closing.csv", "picks_clean.csv",
    ]
    st: Dict[str, Any] = {}
    for name in files:
        p = OUT / name
        if p.exists():
            try:
                n = len(pd.read_csv(p))
            except Exception:
                n = None
            st[name] = {"exists": True, "rows": n}
        else:
            st[name] = {"exists": False, "rows": None}
    acc = _load_accuracy_summary()
    if acc is not None:
        st["accuracy_summary"] = acc
    return jsonify(st)

@app.route("/api/health")
def api_health():
    """Health/status endpoint for deployment diagnostics."""
    try:
        out_dir = OUT
        daily_dir = out_dir / "daily_results"
        games_files = [p.name for p in out_dir.glob("games*.csv")]
        odds_files = [p.name for p in out_dir.glob("odds*.csv")]
        preds_files = [p.name for p in out_dir.glob("predictions*.csv")]
        stake_files = [p.name for p in out_dir.glob("stake_sheet*.csv")]
        daily_results = sorted(daily_dir.glob("results_*.csv")) if daily_dir.exists() else []
        recent_results = [p.stem.split("_")[1] for p in daily_results[-7:]] if daily_results else []
        # Today counts for quick diagnostics
        try:
            today_str = _today_local().strftime("%Y-%m-%d")
        except Exception:
            today_str = None
        games_today_rows = None
        preds_today_rows = None
        try:
            gm = _safe_read_csv(out_dir / "games_curr.csv")
            if not gm.empty and today_str and "date" in gm.columns:
                gm["date"] = gm["date"].astype(str)
                games_today_rows = int((gm["date"] == today_str).sum())
        except Exception:
            games_today_rows = None
        try:
            pr = _load_predictions_current()
            if not pr.empty and today_str and "date" in pr.columns:
                pr["date"] = pd.to_datetime(pr["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                preds_today_rows = int((pr["date"].astype(str) == today_str).sum())
        except Exception:
            preds_today_rows = None
        # Expose predictions source path if previously loaded during this process lifetime
        try:
            global _PREDICTIONS_SOURCE_PATH
            predictions_source = _PREDICTIONS_SOURCE_PATH
        except Exception:
            predictions_source = None
        need_bootstrap = bool(today_str and (preds_today_rows is None or preds_today_rows == 0))
        # Providers (runtime inference backends) best-effort
        try:
            import onnxruntime as ort  # type: ignore
            providers = list(ort.get_available_providers())
        except Exception:
            providers = []
        # Guardrail / prediction diagnostics (best-effort) unified from earlier removed endpoint
        guardrail_summary: dict[str, Any] | None = None
        try:
            today_local = _today_local()
            pred_path = OUT / f"predictions_{today_local.isoformat()}.csv"
            if pred_path.exists():
                pdf = pd.read_csv(pred_path)
                n_rows = len(pdf)
                dv = pd.to_numeric(pdf.get("derived_total"), errors="coerce")
                adj_flags = pdf.get("pred_total_adjusted") if "pred_total_adjusted" in pdf.columns else None
                n_derived = int(dv.notna().sum()) if dv is not None else 0
                n_adjusted = int(adj_flags.sum()) if adj_flags is not None else 0
                guardrail_summary = {
                    "date": today_local.isoformat(),
                    "rows": n_rows,
                    "derived_available_rows": n_derived,
                    "derived_missing_rows": n_rows - n_derived,
                    "adjusted_rows": n_adjusted,
                    "adjusted_pct": round((n_adjusted / n_rows) * 100.0, 2) if n_rows else 0.0,
                    "uniform_flag_rows": int(pdf.get("pred_total_uniform_flag", pd.Series([False]*n_rows)).sum()) if "pred_total_uniform_flag" in pdf.columns else 0,
                }
        except Exception as _ge:
            guardrail_summary = {"error": str(_ge)}
        # Latest results date
        results_latest = None
        try:
            import re as _re
            pat = _re.compile(r'^results_(\d{4}-\d{2}-\d{2})\.csv$')
            dates = []
            if daily_dir.exists():
                for p in daily_dir.glob('results_*.csv'):
                    m = pat.match(p.name)
                    if m:
                        dates.append(m.group(1))
            dates = sorted(set(dates))
            results_latest = dates[-1] if dates else None
        except Exception:
            results_latest = None
        # Latest stake sheet
        stake_latest = None
        try:
            import re as _re
            pat = _re.compile(r'^stake_sheet_(\d{4}-\d{2}-\d{2})')
            sdates = []
            for p in out_dir.glob('stake_sheet*.csv'):
                m = pat.match(p.name)
                if m:
                    sdates.append(m.group(1))
            sdates = sorted(set(sdates))
            stake_latest = sdates[-1] if sdates else None
        except Exception:
            stake_latest = None
        # Finalize hint
        finalize = None
        try:
            date_q = dt.datetime.utcnow().strftime('%Y-%m-%d')
            enriched = out_dir / f'predictions_unified_enriched_{date_q}.csv'
            n_rows = n_final = n_pending = started = 0
            if enriched.exists():
                df = pd.read_csv(enriched)
                n_rows = int(len(df))
                hs = pd.to_numeric(df.get('home_score'), errors='coerce') if 'home_score' in df.columns else pd.Series(np.nan)
                as_ = pd.to_numeric(df.get('away_score'), errors='coerce') if 'away_score' in df.columns else pd.Series(np.nan)
                is_final = hs.notna() & as_.notna() & ((hs + as_) > 0)
                n_final = int(is_final.sum())
                n_pending = int(n_rows - n_final)
                now = pd.Timestamp.utcnow()
                if '_start_dt' in df.columns:
                    st = pd.to_datetime(df['_start_dt'], errors='coerce')
                    started = int((st.notna() & (st <= now)).sum())
            finalize = {
                'date': date_q,
                'rows': n_rows,
                'final': n_final,
                'pending': n_pending,
                'started': started,
                'ready': bool(n_rows and n_pending == 0),
            }
        except Exception:
            finalize = None
        payload = {
            "status": "ok",
            "outputs_dir": str(OUT),
            "games_files": games_files,
            "odds_files": odds_files,
            "predictions_files": preds_files,
            "stake_files": stake_files,
            "daily_results_count": len(daily_results),
            "recent_result_dates": recent_results,
            "predictions_source": predictions_source,
            "need_bootstrap": need_bootstrap,
            "today": {
                "date": today_str,
                "games_today_rows": games_today_rows,
                "preds_today_rows": preds_today_rows,
            },
            "providers": providers,
            "last_pipeline_stats": _LAST_PIPELINE_STATS,
            "guardrails": guardrail_summary,
            "timestamp": dt.datetime.utcnow().isoformat() + "Z",
            # Display predictions hash for alignment (prefer pipeline_stats then fallback to latest file)
            "display_hash": (lambda: (
                _LAST_PIPELINE_STATS.get('display_hash') if isinstance(_LAST_PIPELINE_STATS, dict) and _LAST_PIPELINE_STATS.get('display_hash') else (
                    (lambda _p: (_p.read_text() if _p.exists() else None))(OUT / 'display_hash_latest.txt')
                )
            ))(),
            "results_latest": results_latest,
            "stake_latest": stake_latest,
            "finalize": finalize,
        }
        return jsonify(payload), 200
    except Exception as e:
        logger.exception("/api/health failure")
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route('/api/health/meta')
def api_health_meta():
    try:
        cover = _META_FEATURES_CACHE.get("cover") if isinstance(_META_FEATURES_CACHE, dict) else None
        over = _META_FEATURES_CACHE.get("over") if isinstance(_META_FEATURES_CACHE, dict) else None
        return jsonify({
            "preload_done": bool(_META_PRELOAD_DONE),
            "cover_sidecar_present": bool(cover),
            "over_sidecar_present": bool(over),
            "cover_keys": list(cover.keys()) if isinstance(cover, dict) else [],
            "over_keys": list(over.keys()) if isinstance(over, dict) else [],
        }), 200
    except Exception as e:
        logger.exception('/api/health/meta failure')
        return jsonify({
            "preload_done": False,
            "cover_sidecar_present": False,
            "over_sidecar_present": False,
            "error": str(e),
        }), 500

@app.route('/healthz')
def healthz():
    """Minimal health check for platform probes (e.g., Render)."""
    try:
        return jsonify({"status": "ok"}), 200
    except Exception as e:
        # Even on exception, respond with 200 and include message
        return jsonify({"status": "ok", "note": str(e)}), 200

@app.route('/')
def root_index():
    try:
        return jsonify({"status": "ok", "message": "NCAAB app root"}), 200
    except Exception as e:
        return jsonify({"status": "ok", "error": str(e)}), 200

@app.route('/api/status')
def api_status():
    """Lightweight status for header strip.

    Returns latest `results_latest`, `stake_latest`, `finalize` hint, `display_hash`,
    and calibration metrics when available: `basis_share_total_cal`, `basis_share_margin_cal`,
    `drift_total_delta_vs_median`, `drift_margin_delta_vs_median`.
    """
    try:
        # Latest results date
        results_latest = None
        try:
            import re as _re
            daily_dir = OUT / 'daily_results'
            dates: list[str] = []
            if daily_dir.exists():
                pat = _re.compile(r'^results_(\d{4}-\d{2}-\d{2})\.csv$')
                for p in daily_dir.glob('results_*.csv'):
                    m = pat.match(p.name)
                    if m:
                        dates.append(m.group(1))
            results_latest = (sorted(set(dates))[-1] if dates else None)
        except Exception:
            results_latest = None
        # Latest stake sheet
        stake_latest = None
        try:
            import re as _re
            sdates: list[str] = []
            pat = _re.compile(r'^stake_sheet_(\d{4}-\d{2}-\d{2})')
            for p in OUT.glob('stake_sheet*.csv'):
                m = pat.match(p.name)
                if m:
                    sdates.append(m.group(1))
            stake_latest = (sorted(set(sdates))[-1] if sdates else None)
        except Exception:
            stake_latest = None
        # Finalize hint
        finalize = None
        try:
            use_date = dt.datetime.utcnow().strftime('%Y-%m-%d')
            enriched = OUT / f'predictions_unified_enriched_{use_date}.csv'
            n_rows = n_final = n_pending = started = 0
            if enriched.exists():
                df = pd.read_csv(enriched)
                n_rows = int(len(df))
                hs = pd.to_numeric(df.get('home_score'), errors='coerce') if 'home_score' in df.columns else pd.Series(np.nan)
                as_ = pd.to_numeric(df.get('away_score'), errors='coerce') if 'away_score' in df.columns else pd.Series(np.nan)
                is_final = hs.notna() & as_.notna() & ((hs + as_) > 0)
                n_final = int(is_final.sum())
                n_pending = int(n_rows - n_final)
                now = pd.Timestamp.utcnow()
                if '_start_dt' in df.columns:
                    st = pd.to_datetime(df['_start_dt'], errors='coerce')
                    started = int((st.notna() & (st <= now)).sum())
            finalize = {
                'date': use_date,
                'rows': n_rows,
                'final': n_final,
                'pending': n_pending,
                'started': started,
                'ready': bool(n_rows and n_pending == 0),
            }
        except Exception:
            finalize = None
        # Calibration metrics (best-effort from last pipeline stats snapshot or recent display files)
        basis_share_total_cal = None
        basis_share_margin_cal = None
        drift_total_delta_vs_median = None
        drift_margin_delta_vs_median = None
        try:
            if isinstance(_LAST_PIPELINE_STATS, dict) and _LAST_PIPELINE_STATS:
                basis_share_total_cal = _LAST_PIPELINE_STATS.get('basis_share_total_cal')
                basis_share_margin_cal = _LAST_PIPELINE_STATS.get('basis_share_margin_cal')
                drift_total_delta_vs_median = _LAST_PIPELINE_STATS.get('drift_total_delta_vs_median')
                drift_margin_delta_vs_median = _LAST_PIPELINE_STATS.get('drift_margin_delta_vs_median')
                ece_total_roll7 = _LAST_PIPELINE_STATS.get('ece_total_roll7')
                ece_margin_roll7 = _LAST_PIPELINE_STATS.get('ece_margin_roll7')
                sharpness_total_roll7 = _LAST_PIPELINE_STATS.get('sharpness_total_roll7')
                sharpness_margin_roll7 = _LAST_PIPELINE_STATS.get('sharpness_margin_roll7')
            else:
                # Fallback: compute shares from latest display file
                files = sorted([p for p in OUT.glob('predictions_display_*.csv')])
                if files:
                    dpf = pd.read_csv(files[-1])
                    if 'pred_total_basis' in dpf.columns:
                        basis_share_total_cal = float((dpf['pred_total_basis'].astype(str)=='cal').mean())
                    if 'pred_margin_basis' in dpf.columns:
                        basis_share_margin_cal = float((dpf['pred_margin_basis'].astype(str)=='cal').mean())
                ece_total_roll7 = None
                ece_margin_roll7 = None
                sharpness_total_roll7 = None
                sharpness_margin_roll7 = None
        except Exception:
            pass
        # Display hash
        display_hash = None
        try:
            if isinstance(_LAST_PIPELINE_STATS, dict):
                display_hash = _LAST_PIPELINE_STATS.get('display_hash')
            if not display_hash:
                p = OUT / 'display_hash_latest.txt'
                display_hash = (p.read_text().strip() if p.exists() else None)
        except Exception:
            display_hash = None
        # Server status timestamp and finalize suggestion
        server_timestamp = dt.datetime.utcnow().isoformat() + 'Z'
        finalize_suggested = bool(finalize and finalize.get('ready'))
        # Providers hint
        providers = []
        ep_hint = None
        try:
            import onnxruntime as ort  # type: ignore
            providers = list(ort.get_available_providers())
            if 'QNNExecutionProvider' in providers:
                ep_hint = 'QNN'
            elif 'DmlExecutionProvider' in providers:
                ep_hint = 'DirectML'
            elif 'CPUExecutionProvider' in providers:
                ep_hint = 'CPU'
        except Exception:
            providers = []
            ep_hint = None
        # Risk controls (best-effort from artifacts)
        risk = None
        try:
            # Read a simple risk_config.json if present, else infer from stake sheet
            rc = OUT / 'risk_config.json'
            if rc.exists():
                with open(rc, 'r', encoding='utf-8') as f:
                    risk = json.load(f)
            else:
                # Infer daily loss used from latest stake sheet PnL if available
                # Expect columns: units, pnl_units, etc.
                if stake_latest:
                    # Try match stake sheet filename
                    import re as _re
                    pat = _re.compile(rf'^stake_sheet_{stake_latest}.*\.csv$')
                    sheets = [p for p in OUT.glob('stake_sheet*.csv') if pat.match(p.name)]
                    if sheets:
                        try:
                            ss = pd.read_csv(sheets[-1])
                            if 'pnl_units' in ss.columns:
                                used = float(pd.to_numeric(ss['pnl_units'], errors='coerce').sum())
                                risk = (risk or {})
                                risk['daily_loss_used'] = abs(min(0.0, used))
                        except Exception:
                            pass
            # Live PnL hook (overrides inferred value if present)
            try:
                plive = OUT / 'pnl_live.json'
                if plive.exists():
                    with open(plive, 'r', encoding='utf-8') as f:
                        j = json.load(f)
                        v = j.get('daily_loss_used')
                        if isinstance(v, (int,float)):
                            risk = (risk or {})
                            risk['daily_loss_used'] = float(v)
            except Exception:
                pass
        except Exception:
            risk = None
        return jsonify({
            'results_latest': results_latest,
            'stake_latest': stake_latest,
            'finalize': finalize,
            'display_hash': display_hash,
            'basis_share_total_cal': basis_share_total_cal,
            'basis_share_margin_cal': basis_share_margin_cal,
            'drift_total_delta_vs_median': drift_total_delta_vs_median,
            'drift_margin_delta_vs_median': drift_margin_delta_vs_median,
            'ece_total_roll7': ece_total_roll7,
            'ece_margin_roll7': ece_margin_roll7,
            'sharpness_total_roll7': sharpness_total_roll7,
            'sharpness_margin_roll7': sharpness_margin_roll7,
            'providers': providers,
            'ep_hint': ep_hint,
            'server_timestamp': server_timestamp,
            'finalize_suggested': finalize_suggested,
            'risk': risk,
        }), 200
    except Exception as e:
        logger.exception('/api/status failure')
        return jsonify({'error': str(e)}), 500


@app.route("/api/bootstrap", methods=["POST", "GET"])
def api_bootstrap():
    """On-demand bootstrap to populate today's (or requested) games, features & predictions.

    Query/JSON params:
      - date: YYYY-MM-DD (optional; defaults to today)
      - provider: espn|ncaa|fused (optional; default espn)
      - force: if '1' or true, run even if predictions already exist for date

    Returns JSON with counts and any warnings. Safe to call multiple times; will skip heavy work if data present.
    """
    if cli_daily_run is None:
        return jsonify({"status": "error", "message": "daily_run not importable in this environment"}), 500
    date_param = (request.args.get("date") or (request.json.get("date") if request.is_json else None) or "").strip()
    provider_param = (request.args.get("provider") or (request.json.get("provider") if request.is_json else None) or "espn").strip().lower()
    force_param = (request.args.get("force") or (request.json.get("force") if request.is_json else None) or "").strip().lower() in ("1","true","yes","force")
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None

@app.route('/api/risk-config', methods=['POST'])
def api_risk_config():
    """Update risk_config.json via POST body.

    Accepts JSON with optional keys: daily_loss_cap (units), kelly_cap (fraction), exposure_cap_units (units).
    Writes to outputs/risk_config.json and returns the updated config.
    """
    try:
        data = request.get_json(silent=True) or {}
        cfg = {}
        for key in ('daily_loss_cap','kelly_cap','exposure_cap_units'):
            if key in data and data[key] is not None:
                try:
                    cfg[key] = float(data[key])
                except Exception:
                    pass
        outp = OUT / 'risk_config.json'
        outp.parent.mkdir(parents=True, exist_ok=True)
        with open(outp, 'w', encoding='utf-8') as f:
            json.dump(cfg, f, indent=2)
        return jsonify({'status':'ok','config':cfg,'path':str(outp)}), 200
    except Exception as e:
        return jsonify({'status':'error','message':str(e)}), 500
    # use_cache override (refresh) param
    use_cache_param = (request.args.get("use_cache") or (request.json.get("use_cache") if request.is_json else None) or "").strip().lower()
    refresh_param = (request.args.get("refresh") or (request.json.get("refresh") if request.is_json else None) or "").strip().lower()
    # Interpret: if use_cache in ['0','false','no'] OR refresh=1 => disable cache
    disable_cache = use_cache_param in ("0","false","no") or refresh_param in ("1","true","yes")
    target_date = date_param or today_str
    if not target_date:
        return jsonify({"status": "error", "message": "Unable to resolve target date"}), 400

    # If predictions already exist for this date and not forced, short-circuit
    existing_preds = _load_predictions_current()
    already = False
    pred_rows_for_date = 0
    if not existing_preds.empty and "date" in existing_preds.columns:
        try:
            existing_preds["date"] = pd.to_datetime(existing_preds["date"], errors="coerce").dt.strftime("%Y-%m-%d")
        except Exception:
            pass
        pred_rows_for_date = int((existing_preds["date"].astype(str) == target_date).sum())
        already = pred_rows_for_date > 0
    if already and not force_param:
        return jsonify({
            "status": "ok",
            "message": f"Predictions already present for {target_date}; skipping bootstrap (use force=1 to override)",
            "date": target_date,
            "pred_rows": pred_rows_for_date,
            "skipped": True,
        }), 200

    # Execute daily_run pipeline minimally (avoid retraining / heavy operations on server)
    logs: list[str] = []
    try:
        # Wrap prints by temporarily redirecting stdout if desired; here we just call and rely on server logs
        cli_daily_run(
            date=target_date,
            season=dt.date.fromisoformat(target_date).year,
            region="us",
            provider=provider_param,
            threshold=2.0,
            default_price=-110.0,
            retrain=False,
            segment="none",
            conf_map=None,
            use_cache=(not disable_cache),
            preseason_weight=0.0,
            preseason_only_sparse=True,
            apply_guardrails=True,
            half_ratio=0.485,
            auto_train_halves=False,
            halves_models_dir=OUT / "models_halves",
            enable_ort=False,
            accumulate_schedule=True,
            accumulate_predictions=True,
        )
    except Exception as e:
        logger.exception("Bootstrap daily_run failed")
        return jsonify({"status": "error", "message": f"daily_run failed: {e}"}), 500

    # Reload artifacts to report counts
    games_after = _safe_read_csv(OUT / "games_curr.csv")
    preds_after = _load_predictions_current()
    if "date" in preds_after.columns:
        try:
            preds_after["date"] = pd.to_datetime(preds_after["date"], errors="coerce").dt.strftime("%Y-%m-%d")
        except Exception:
            pass
    pred_rows_after = int(preds_after[preds_after.get("date","") == target_date].shape[0]) if not preds_after.empty and "date" in preds_after.columns else 0
    game_rows_after = int(games_after[games_after.get("date","") == target_date].shape[0]) if not games_after.empty and "date" in games_after.columns else len(games_after)

    # Optional auto-fallback: if today's slate looks sparse with provider 'espn' or 'ncaa', try fused
    fallback_info: dict[str, Any] = {"triggered": False}
    try:
        min_thresh = int(os.environ.get("NCAAB_MIN_TODAY_GAMES", "25"))
    except Exception:
        min_thresh = 25
    if (
        provider_param != "fused"
        and target_date == today_str
        and isinstance(game_rows_after, int)
        and game_rows_after < min_thresh
    ):
        try:
            cli_daily_run(
                date=target_date,
                season=dt.date.fromisoformat(target_date).year,
                region="us",
                provider="fused",
                threshold=2.0,
                default_price=-110.0,
                retrain=False,
                segment="none",
                conf_map=None,
                use_cache=(not disable_cache),
                preseason_weight=0.0,
                preseason_only_sparse=True,
                apply_guardrails=True,
                half_ratio=0.485,
                auto_train_halves=False,
                halves_models_dir=OUT / "models_halves",
                enable_ort=False,
                accumulate_schedule=True,
                accumulate_predictions=True,
            )
            # Recompute counts after fallback
            games_after2 = _safe_read_csv(OUT / "games_curr.csv")
            preds_after2 = _load_predictions_current()
            if "date" in preds_after2.columns:
                try:
                    preds_after2["date"] = pd.to_datetime(preds_after2["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                except Exception:
                    pass
            pred_rows_after2 = int(preds_after2[preds_after2.get("date","") == target_date].shape[0]) if not preds_after2.empty and "date" in preds_after2.columns else 0
            game_rows_after2 = int(games_after2[games_after2.get("date","") == target_date].shape[0]) if not games_after2.empty and "date" in games_after2.columns else len(games_after2)
            fallback_info = {
                "triggered": True,
                "reason": f"sparse slate ({game_rows_after}) with provider={provider_param}; retried fused",
                "prev": {"game_rows": game_rows_after, "pred_rows": pred_rows_after, "provider": provider_param},
                "after": {"game_rows": game_rows_after2, "pred_rows": pred_rows_after2, "provider": "fused"},
            }
            # Promote fused counts in primary response
            game_rows_after = game_rows_after2
            pred_rows_after = pred_rows_after2
            provider_param = f"{provider_param}->fused"
        except Exception as e:
            logger.exception("Fallback to fused provider failed")
            fallback_info = {
                "triggered": True,
                "error": str(e),
                "prev": {"game_rows": game_rows_after, "pred_rows": pred_rows_after, "provider": provider_param},
            }

    return jsonify({
        "status": "ok",
        "message": f"Bootstrap complete for {target_date}",
        "date": target_date,
        "pred_rows": pred_rows_after,
        "game_rows": game_rows_after,
        "provider": provider_param,
        "forced": force_param,
        "cache_used": not disable_cache,
        "fallback": fallback_info,
    }), 200


@app.route("/api/schedule-diagnostics")
def api_schedule_diagnostics():
    """Fetch today's (or requested date) games from ESPN + NCAA live (cache bypass optional) and report counts.

    Params:
      - date: YYYY-MM-DD (default today)
      - refresh=1 to bypass adapter cache
    Returns JSON with counts and sample team lists to help diagnose under-coverage.
    Does not write any output files.
    """
    date_param = (request.args.get("date") or "").strip()
    refresh = (request.args.get("refresh") or "").strip().lower() in ("1","true","yes")
    try:
        target_date = dt.date.fromisoformat(date_param) if date_param else _today_local()
    except Exception:
        return jsonify({"status": "error", "message": f"Invalid date: {date_param}"}), 400
    # Import adapters locally to avoid global import errors
    try:
        from ncaab_model.data.adapters.espn_scoreboard import _fetch_day as _espn_fetch, _parse_games as _espn_parse  # type: ignore
        from ncaab_model.data.adapters.ncaa_scoreboard import _fetch_scoreboard as _ncaa_fetch, _parse_games as _ncaa_parse  # type: ignore
    except Exception as e:
        return jsonify({"status": "error", "message": f"Adapter import failed: {e}"}), 500
    # Fetch raw payloads (optionally bypass cache)
    espn_payload = _espn_fetch(target_date, use_cache=not refresh)
    ncaa_payload = _ncaa_fetch(target_date, use_cache=not refresh)
    espn_games = _espn_parse(target_date, espn_payload) if espn_payload else []
    ncaa_games = _ncaa_parse(target_date, ncaa_payload) if ncaa_payload else []
    # Build fused keys (normalized simple lower-case names)
    def _norm(t: str | None) -> str | None:
        if not t:
            return None
        return "".join(ch for ch in t.lower() if ch.isalnum() or ch in (" ","-"))
    espn_set = {(_norm(g.home_team), _norm(g.away_team)) for g in espn_games}
    ncaa_set = {(_norm(g.home_team), _norm(g.away_team)) for g in ncaa_games}
    fused = espn_set | ncaa_set
    missing_from_espn = sorted(list(ncaa_set - espn_set))
    missing_from_ncaa = sorted(list(espn_set - ncaa_set))
    return jsonify({
        "status": "ok",
        "date": target_date.isoformat(),
        "refresh": refresh,
        "espn_count": len(espn_games),
        "ncaa_count": len(ncaa_games),
        "fused_unique_matchups": len(fused),
        "missing_from_espn": missing_from_espn[:25],
        "missing_from_ncaa": missing_from_ncaa[:25],
    }), 200


# ---------------------------------
# Time display diagnostics endpoint
# ---------------------------------
@app.get('/api/time-debug')
def api_time_debug():
    """Inspect time fields and computed Central display for a given date/game.

    Query params:
      - date: YYYY-MM-DD (required for file selection)
      - game_id: optional filter to a single game id
      - limit: optional max rows (default 50)
    """
    date_q = (request.args.get('date') or '').strip()
    gid_q = (request.args.get('game_id') or '').strip()
    try:
        limit = int(request.args.get('limit') or '50')
    except Exception:
        limit = 50
    if not date_q:
        return jsonify({'status':'error','message':'missing date'}), 400
    out_dir = ROOT / 'outputs'
    # Try enriched/unified predictions first (contains start fields), fallback to games_with_odds
    candidates = [
        out_dir / f'predictions_unified_enriched_{date_q}.csv',
        out_dir / f'predictions_unified_{date_q}.csv',
        out_dir / f'games_with_odds_{date_q}.csv',
    ]
    df = pd.DataFrame()
    src = None
    for p in candidates:
        if p.exists():
            try:
                df = pd.read_csv(p)
                src = p.name
                break
            except Exception:
                continue
    if df.empty:
        return jsonify({'status':'missing','message':'no source file','date':date_q}), 404
    # Normalize and filter
    try:
        if 'game_id' in df.columns:
            df['game_id'] = df['game_id'].astype(str).str.replace(r'\.0$','', regex=True)
        if gid_q:
            df = df[df['game_id'].astype(str) == gid_q]
    except Exception:
        pass
    # Build diagnostics rows
    cols_keep = [
        'game_id','home_team','away_team','date','display_date',
        'start_time_iso','commence_time','start_time',
        'start_time_local','start_tz_abbr','start_time_local_venue','start_tz_abbr_venue',
        '_start_dt'
    ]
    data = []
    for _r in df.head(limit).to_dict(orient='records'):
        r = dict(_r)
        # Ensure ISO then apply central display helper used on index route
        try:
            rr = _ensure_index_row_global(r)
            # If available, use normalization helpers; else proceed with central display directly
            try:
                rr = _backfill_start_fields(rr)
            except Exception:
                pass
            try:
                rr = _correct_midnight_drift(rr, slate_date=str(rr.get('date')) if rr.get('date') else None)
            except Exception:
                pass
            rr = _apply_site_display_global(rr)
            # Final derive: if `_start_dt` still missing, use canonical ISO helper
            try:
                if not rr.get('_start_dt'):
                    iso2 = _derive_start_iso(rr)
                    if iso2:
                        rr['_start_dt'] = pd.to_datetime(str(iso2).replace('Z','+00:00'), errors='coerce', utc=True)
            except Exception:
                pass
            disp = rr.get('display_time_str')
        except Exception:
            disp = None
        # Prefer the normalized/derived fields from rr for diagnostics output
        item = {k: rr.get(k) for k in cols_keep if k in rr}
        # Ensure _start_dt is rendered as a string when tz-aware
        try:
            _sd = rr.get('_start_dt')
            if _sd is not None and not pd.isna(_sd):
                try:
                    item['_start_dt'] = pd.to_datetime(_sd, errors='coerce').isoformat()
                except Exception:
                    item['_start_dt'] = str(_sd)
        except Exception:
            pass
        item['display_time_str'] = disp
        data.append(item)
    return jsonify({'status':'ok','date':date_q,'source':src,'rows':data})


@app.route("/recommendations")
def recommendations():
    picks = _load_picks()
    if not picks.empty and "date" in picks.columns:
        try:
            picks["date"] = pd.to_datetime(picks["date"])
            picks = picks.sort_values(["date", "abs_edge" if "abs_edge" in picks.columns else "edge"], ascending=[True, False])
        except Exception:
            pass
    # Ensure projected scores columns exist to satisfy template even if margin absent
    if not picks.empty and "pred_total" in picks.columns:
        if "pred_margin" not in picks.columns:
            # Assume zero margin if not available (neutral projection)
            picks["pred_margin"] = 0.0
        try:
            # Compute projected home/away from total + margin
            pt = pd.to_numeric(picks["pred_total"], errors="coerce")
            pm = pd.to_numeric(picks["pred_margin"], errors="coerce")
            picks["proj_home"] = (pt + pm) / 2.0
            picks["proj_away"] = pt - picks["proj_home"]
        except Exception:
            picks["proj_home"] = None
            picks["proj_away"] = None
    rows = picks.to_dict(orient="records") if not picks.empty else []
    # Harden display datetime using robust helper
    safe_rows: list[dict] = []
    for _r in rows:
        r = dict(_r)
        if not r.get('start_time_iso'):
            r['start_time_iso'] = _derive_start_iso(r)
        # Backfill local/display normalization for evening UTC rollover
        r = _backfill_start_fields(r)
        r = _correct_midnight_drift(r, slate_date=str(r.get('date')) if r.get('date') else None)
        # Enforce canonical Central display using robust helper
        r = _apply_site_display_global(r)
        safe_rows.append(r)
    rows = safe_rows
    return render_template("recommendations.html", rows=rows, total_rows=len(rows))


@app.route("/picks-raw")
def picks_raw_page():
    """Render expanded multi-market picks with basic filters (date, market, period)."""
    p = OUT / "picks_raw.csv"
    df = _safe_read_csv(p)
    date_q = (request.args.get("date") or "").strip()
    market_q = (request.args.get("market") or "").strip().lower()
    period_q = (request.args.get("period") or "").strip().lower()
    if df.empty:
        return render_template("recommendations.html", rows=[], total_rows=0)
    # Normalize columns
    if "game_id" in df.columns:
        df["game_id"] = df["game_id"].astype(str)
    if "date" in df.columns:
        try:
            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
        except Exception:
            pass
    if date_q and "date" in df.columns:
        df = df[df["date"] == date_q]
    if market_q and "market" in df.columns:
        df = df[df["market"].astype(str).str.lower() == market_q]
    if period_q and "period" in df.columns:
        df = df[df["period"].astype(str).str.lower() == period_q]
    # Sort by absolute edge desc
    if "edge" in df.columns:
        try:
            df["_abs_edge"] = pd.to_numeric(df["edge"], errors="coerce").abs()
            df = df.sort_values(["_abs_edge"], ascending=[False])
        except Exception:
            pass
    # Branding enrichment for quick display
    branding = _load_branding_map()
    def _enrich_row(r: dict[str, Any]) -> dict[str, Any]:
        for side in ["home","away"]:
            key = normalize_name(str(r.get(f"{side}_team") or ""))
            b = branding.get(key) or {}
            r[f"{side}_key"] = key
            r[f"{side}_logo"] = b.get("logo")
        return r
    rows = []
    for _r in df.to_dict(orient="records"):
        r = _enrich_row(dict(_r))
        if not r.get('start_time_iso'):
            r['start_time_iso'] = _derive_start_iso(r)
        # Use row date for slate validation if present
        slate = str(r.get('date')) if r.get('date') else None
        r = _backfill_start_fields(r)
        r = _correct_midnight_drift(r, slate_date=slate)
        # Enforce canonical Central display using robust helper
        r = _apply_site_display_global(r)
        rows.append(r)
    return render_template("picks_raw.html", rows=rows, total_rows=len(rows), date_val=date_q, market_val=market_q, period_val=period_q)


@app.route("/finals")
def finals():
    """Season-to-date final scores table, aggregated from daily_results.*"""
    df = _load_all_finals(limit=2000)
    # Normalize numeric fields to avoid template formatting errors
    try:
        for c in ("home_score","away_score","actual_total","pred_total","closing_total"):
            if c in df.columns and not df.empty:
                df[c] = pd.to_numeric(df[c], errors="coerce")
        # Replace NaNs with None for Jinja-friendly checks
        df = df.where(pd.notna(df), None)
    except Exception:
        pass
    rows_raw = df.to_dict(orient="records") if not df.empty else []
    rows = []
    for _r in rows_raw:
        r = dict(_r)
        # Ensure expected numeric keys exist for template formatting
        for k in ("home_score","away_score","actual_total","pred_total","closing_total"):
            if k not in r:
                r[k] = None
        if not r.get('start_time_iso'):
            r['start_time_iso'] = _derive_start_iso(r)
        slate = str(r.get('date')) if r.get('date') else None
        r = _backfill_start_fields(r)
        r = _correct_midnight_drift(r, slate_date=slate)
        rows.append(r)
    # Basic stats: count, last date range
    date_min = df["date"].min() if "date" in df.columns and not df.empty else None
    date_max = df["date"].max() if "date" in df.columns and not df.empty else None
    return render_template("finals.html", rows=rows, total_rows=len(rows), date_min=date_min, date_max=date_max)


@app.route("/api/recommendations")
def api_recommendations():
    picks = _load_picks()
    rows = picks.to_dict(orient="records") if not picks.empty else []
    return jsonify({"rows": len(rows), "data": rows})

@app.route("/api/midnight_drift_diagnostic")
def api_midnight_drift_diagnostic():
    """Identify games whose UTC ISO spills into next day but belong to slate date.

    Params:
      date=YYYY-MM-DD (required)
      provider=espn|ncaa (default espn)
      refresh=1 (bypass adapter cache)
    """
    date_param = (request.args.get("date") or "").strip()
    if not date_param:
        return jsonify({"status": "error", "message": "missing date"}), 400
    try:
        target_date = dt.date.fromisoformat(date_param)
    except Exception:
        return jsonify({"status": "error", "message": f"bad date: {date_param}"}), 400
    provider = (request.args.get("provider") or "espn").strip().lower()
    refresh = (request.args.get("refresh") or "").strip().lower() in ("1","true","yes")
    espn_games = []
    ncaa_games = []
    try:
        from ncaab_model.data.adapters.espn_scoreboard import _fetch_day as _espn_fetch, _parse_games as _espn_parse  # type: ignore
        payload = _espn_fetch(target_date, use_cache=not refresh)
        if payload:
            espn_games = _espn_parse(target_date, payload)
    except Exception:
        espn_games = []
    try:
        from ncaab_model.data.adapters.ncaa_scoreboard import _fetch_scoreboard as _ncaa_fetch, _parse_games as _ncaa_parse  # type: ignore
        payload2 = _ncaa_fetch(target_date, use_cache=not refresh)
        if payload2:
            ncaa_games = _ncaa_parse(target_date, payload2)
    except Exception:
        ncaa_games = []
    def _norm_team(t: str | None) -> str:
        if not t:
            return ""
        return "".join(ch for ch in t.lower() if ch.isalnum())
    def _mk_key(home: str, away: str) -> str:
        return _norm_team(home)+"__"+_norm_team(away)
    espn_map = {_mk_key(g.home_team, g.away_team): g for g in espn_games}
    ncaa_map = {_mk_key(g.home_team, g.away_team): g for g in ncaa_games}
    # Load predictions rows for that date (prefer enriched file)
    rendered_rows: list[dict[str, Any]] = []
    try:
        enriched = OUT / f"predictions_unified_enriched_{date_param}.csv"
        base = OUT / f"predictions_unified_{date_param}.csv"
        chosen = enriched if enriched.exists() else (base if base.exists() else None)
        if chosen:
            df_r = _safe_read_csv(chosen)
            rendered_rows = df_r.to_dict(orient="records")
    except Exception:
        rendered_rows = []
    suspects: list[dict[str, Any]] = []
    for row in rendered_rows:
        try:
            home = str(row.get("home_team") or "")
            away = str(row.get("away_team") or "")
            key = _mk_key(home, away)
            iso = str(row.get("start_time_iso") or "")
            disp = str(row.get("start_time_display") or row.get("start_time_local") or "")
            utc_date = None
            if iso:
                try:
                    iso_dt = pd.to_datetime(iso, errors="coerce", utc=True)
                    if pd.notna(iso_dt):
                        utc_date = iso_dt.date().isoformat()
                except Exception:
                    pass
            local_date = None
            if disp:
                m = re.search(r"(\d{4}-\d{2}-\d{2})", disp)
                if m:
                    local_date = m.group(1)
            midnight_hour_local = False
            try:
                if iso:
                    iso_dt2 = pd.to_datetime(iso, errors="coerce", utc=True)
                    if pd.notna(iso_dt2):
                        try:
                            chi = ZoneInfo("America/Chicago")
                            if iso_dt2.astimezone(chi).hour < 6:
                                midnight_hour_local = True
                        except Exception:
                            pass
            except Exception:
                pass
            provider_game = espn_map.get(key) if provider == "espn" else ncaa_map.get(key)
            prov_start = None
            if provider_game and getattr(provider_game, "start_time", None):
                try:
                    stp = provider_game.start_time
                    prov_start = stp.isoformat() if isinstance(stp, dt.datetime) else str(stp)
                except Exception:
                    prov_start = None
            if (utc_date and utc_date != date_param) or (local_date and local_date != date_param) or midnight_hour_local:
                # Compute recommended ISO using current local fields if available
                try:
                    row_copy = {
                        'start_time_local': row.get('start_time_local'),
                        'start_tz_abbr': row.get('start_tz_abbr'),
                        'start_time': row.get('start_time'),
                        'commence_time': row.get('commence_time'),
                    }
                    recommended_iso = _derive_start_iso(row_copy)
                except Exception:
                    recommended_iso = None
                suspects.append({
                    "home_team": home,
                    "away_team": away,
                    "row_iso": iso,
                    "utc_date": utc_date,
                    "local_date": local_date,
                    "midnight_hour_local": midnight_hour_local,
                    "provider_start_time": prov_start,
                    "provider": provider,
                    "key": key,
                    "recommended_iso": recommended_iso,
                })
        except Exception:
            continue
    return jsonify({
        "status": "ok",
        "date": date_param,
        "provider": provider,
        "refresh": refresh,
        "suspect_count": len(suspects),
        "suspects": suspects,
        "espn_count": len(espn_games),
        "ncaa_count": len(ncaa_games),
    })


@app.route("/api/picks_raw")
def api_picks_raw():
    """Expose expanded picks (totals/spreads/moneyline, incl. halves) from outputs/picks_raw.csv.

    Optional query: ?date=YYYY-MM-DD filters rows by date column when present.
    Enriches with team branding keys and logos if available.
    """
    p = OUT / "picks_raw.csv"
    df = _safe_read_csv(p)
    if df.empty:
        return jsonify({"rows": 0, "data": []})
    # Filter by date if provided
    date_q = (request.args.get("date") or "").strip()
    if date_q and "date" in df.columns:
        try:
            df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")
            df = df[df["date"] == date_q]
        except Exception:
            pass
    # Branding enrichment
    branding = _load_branding_map()
    def _enrich(row: dict[str, Any]) -> dict[str, Any]:
        for side in ["home","away"]:
            t = str(row.get(f"{side}_team") or "")
            key = normalize_name(t)
            b = branding.get(key) or {}
            row[f"{side}_key"] = key
            row[f"{side}_logo"] = b.get("logo")
            row[f"{side}_color"] = b.get("primary") or b.get("secondary")
            row[f"{side}_text_color"] = b.get("text") or "#ffffff"
        return row
    rows = [_enrich(r) for r in df.to_dict(orient="records")]
    return jsonify({"rows": len(rows), "data": rows})


@app.route("/api/accuracy")
def api_accuracy():
    acc_json = OUT / "eval_last2" / "accuracy_summary.json"
    if acc_json.exists():
        try:
            data = json.loads(acc_json.read_text(encoding="utf-8"))
            return jsonify(data)
        except Exception as e:
            return jsonify({"error": str(e)}), 500
    return jsonify({"error": "no accuracy report found"}), 404


@app.route("/api/scoring", methods=["GET"])
def api_scoring():
    """Serve per-date scoring metrics JSON (CRPS/log-likelihood).

    Query params:
    - date: YYYY-MM-DD (optional; defaults to today UTC)

    Returns 404 if the scoring artifact is missing for the requested date.
    """
    try:
        import datetime as dt
        import json as _json
        from pathlib import Path as _P
    except Exception:
        return jsonify({"error": "internal import error"}), 500

    date_q = (request.args.get("date") or "").strip()
    if not date_q:
        try:
            date_q = str(dt.datetime.utcnow().strftime('%Y-%m-%d'))
        except Exception:
            date_q = str(dt.datetime.now().strftime('%Y-%m-%d'))

    try:
        outputs_dir = OUT if 'OUT' in globals() else _P(__file__).parent / 'outputs'
        fp = outputs_dir / f"scoring_{date_q}.json"
        if not fp.exists():
            return jsonify({"status": "missing", "date": date_q, "path": str(fp) }), 404
        payload = _json.loads(fp.read_text(encoding='utf-8'))
        # Attach minimal metadata for consumers
        if isinstance(payload, dict):
            payload.setdefault("date", date_q)
            payload.setdefault("path", str(fp))
        return jsonify(payload)
    except Exception as e:
        return jsonify({"status": "error", "date": date_q, "message": str(e)}), 500


@app.route("/api/finalize-day", methods=["GET","POST"])
def api_finalize_day():
    """Run finalize-day for a given date. Example: /api/finalize-day?date=YYYY-MM-DD

    Returns JSON with status and message. This invokes the same logic as the CLI command.
    """
    # Accept date via GET query or POST JSON
    date_q = (request.args.get("date") or "").strip()
    if not date_q and request.method == "POST":
        try:
            payload = request.get_json(silent=True) or {}
            date_q = str(payload.get("date") or "").strip()
            # Optional confirmation flag; currently advisory, can be used for guardrails
            _confirm = bool(payload.get("confirm"))
        except Exception:
            payload = {}
            _confirm = False
    if not date_q:
        return jsonify({"ok": False, "error": "missing date"}), 400
    if cli_finalize_day is None:
        return jsonify({"ok": False, "error": "finalize-day not available"}), 500
    try:
        cli_finalize_day(date=date_q)  # type: ignore
        return jsonify({"ok": True, "date": date_q})
    except Exception as e:
        # Typer Exit raises BaseException; catch broadly
        if typer is not None and isinstance(e, typer.Exit):  # type: ignore
            # Treat non-zero code as error
            code = getattr(e, "exit_code", 0)
            if code and int(code) != 0:
                return jsonify({"ok": False, "date": date_q, "error": f"exit {code}"}), 500
            return jsonify({"ok": True, "date": date_q, "note": "completed with exit"})
        return jsonify({"ok": False, "date": date_q, "error": str(e)}), 500


@app.route("/api/refresh-odds")
def api_refresh_odds():
    """Refresh today's (or provided date's) odds snapshot and rebuild last odds merge.

    Query params:
      - date=YYYY-MM-DD (optional; defaults to local today)
      - region=us (optional TheOddsAPI region)
      - markets=comma,separated markets (defaults spreads,totals,h2h)
      - tolerance=seconds skew tolerance for last selection (default 60)

    Steps:
      1. Fetch current odds snapshot for the date into odds_history/odds_<date>.csv (multi-region not yet).
      2. Rebuild last_odds.csv across odds_history directory.
      3. Join last odds for that date into games_with_last_<date>.csv (any-D1, allow partial).
      4. Refresh master games_with_last.csv for that single date (append/replace row subset).
      5. Return coverage counts.
    """
    date_q = (request.args.get("date") or "").strip()
    region = (request.args.get("region") or "us").strip()
    markets = (request.args.get("markets") or "h2h,spreads,totals").strip()
    tolerance = int((request.args.get("tolerance") or "60").strip() or 60)
    # Default date = local today (same logic as schedule timezone)
    if not date_q:
        try:
            from zoneinfo import ZoneInfo as _ZoneInfo
            tz_name = os.getenv("NCAAB_SCHEDULE_TZ", "America/New_York")
            date_q = dt.datetime.now(_ZoneInfo(tz_name)).date().isoformat()
        except Exception:
            date_q = dt.date.today().isoformat()
    # Validate games file exists
    games_file = OUT / f"games_{date_q}.csv"
    if not games_file.exists():
        return jsonify({"ok": False, "error": f"games file missing for {date_q}"}), 404
    # 1. Fetch snapshot (single region)
    try:
        # Local import to avoid global import errors at app startup
        from ncaab_model.data.adapters.odds_theoddsapi import TheOddsAPIAdapter  # type: ignore
        adapter = TheOddsAPIAdapter(region=region)  # type: ignore
        rows = []
        for row in adapter.iter_current_odds_expanded(markets=markets, date_iso=date_q):  # type: ignore
            rows.append(row.model_dump())
        if rows:
            oh_dir = OUT / "odds_history"; oh_dir.mkdir(parents=True, exist_ok=True)
            snap_path = oh_dir / f"odds_{date_q}.csv"
            pd.DataFrame(rows).to_csv(snap_path, index=False)
        else:
            return jsonify({"ok": False, "error": "no odds rows fetched"}), 500
    except Exception as e:
        return jsonify({"ok": False, "error": f"fetch failed: {e}"}), 500
    # 2. Rebuild last_odds.csv
    try:
        from ncaab_model.data.odds_closing import make_last_odds as _make_last
        last_path = OUT / "last_odds.csv"
        _make_last(OUT / "odds_history", last_path, tolerance_seconds=tolerance)  # type: ignore
    except Exception as e:
        return jsonify({"ok": False, "error": f"make_last_odds failed: {e}"}), 500
    # 3. Join for date
    try:
        from ncaab_model.data.join_closing import join_games_with_closing as _join
        last_df = pd.read_csv(last_path)
        games_df = pd.read_csv(games_file)
        # Any-D1 filter (re-apply minimal subset like CLI) - optional
        try:
            d1 = pd.read_csv(settings.data_dir / "d1_conferences.csv")
            from ncaab_model.data.merge_odds import normalize_name as _norm
            d1set = set(d1['team'].astype(str).map(_norm))
            games_df['_home_ok'] = games_df['home_team'].astype(str).map(_norm).isin(d1set)
            games_df['_away_ok'] = games_df['away_team'].astype(str).map(_norm).isin(d1set)
            games_df = games_df[games_df['_home_ok'] | games_df['_away_ok']].copy()
            games_df.drop(columns=['_home_ok','_away_ok'], inplace=True, errors='ignore')
        except Exception:
            pass
        merged_date = _join(games_df, last_df)
        per_date_out = OUT / f"games_with_last_{date_q}.csv"
        merged_date.to_csv(per_date_out, index=False)
    except Exception as e:
        return jsonify({"ok": False, "error": f"join failed: {e}"}), 500
    # 4. Refresh master: replace rows for date
    master_path = OUT / "games_with_last.csv"
    try:
        if master_path.exists():
            master_df = pd.read_csv(master_path)
            if 'date' in master_df.columns:
                master_df['date'] = pd.to_datetime(master_df['date'], errors='coerce')
            # Remove existing rows for this date (match by date string if date parsing missing)
            mask_remove = (master_df.get('date').dt.strftime('%Y-%m-%d') == date_q) if 'date' in master_df.columns else (master_df.get('date_game', pd.Series(dtype=str)).astype(str) == date_q)
            master_df = master_df[~mask_remove]
            # Append new
            append_df = merged_date.copy()
            master_df = pd.concat([master_df, append_df], ignore_index=True)
        else:
            master_df = merged_date.copy()
        master_df.to_csv(master_path, index=False)
    except Exception as e:
        return jsonify({"ok": False, "error": f"master refresh failed: {e}"}), 500
    # 5. Coverage counts
    try:
        covered_exact = merged_date['game_id'].nunique() if 'game_id' in merged_date.columns else 0
        games_total = len(games_df)
        return jsonify({
            "ok": True,
            "date": date_q,
            "snapshot_rows": len(rows),
            "merged_rows": len(merged_date),
            "games_total": games_total,
            "covered_exact_games": covered_exact,
        })
    except Exception as e:
        return jsonify({"ok": True, "date": date_q, "note": "completed", "coverage_error": str(e)})

@app.route("/api/debug-card", methods=["POST"])
def api_debug_card():
    """Return a single card row as built by index() for debugging.

    Body JSON:
      - date: YYYY-MM-DD
      - game_id: ESPN game id (string/int)
    """
    try:
        payload = request.get_json(silent=True) or {}
    except Exception:
        payload = {}
    date_q = str(payload.get("date") or "").strip()
    game_id_q = str(payload.get("game_id") or "").strip()
    if not date_q or not game_id_q:
        return jsonify({"ok": False, "error": "date and game_id required"}), 400

    # Load core frames using same helpers as index()
    games = _load_games_current()
    preds = _load_predictions_current()
    odds = _load_odds_joined(date_q)

    try:
        games["game_id"] = games.get("game_id", pd.Series([])).astype(str)
    except Exception:
        pass
    try:
        preds["game_id"] = preds.get("game_id", pd.Series([])).astype(str)
    except Exception:
        pass
    try:
        odds["game_id"] = odds.get("game_id", pd.Series([])).astype(str)
    except Exception:
        pass

    # Restrict to requested date where possible
    for df_ in (games, preds):
        if not df_.empty and "date" in df_.columns:
            try:
                df_["date"] = pd.to_datetime(df_["date"], errors="coerce").dt.strftime("%Y-%m-%d")
                df_ = df_[df_["date"] == date_q]
            except Exception:
                pass
    if not games.empty and "date" in games.columns:
        try:
            games = games[games["date"].astype(str) == date_q]
        except Exception:
            pass
    if not preds.empty and "date" in preds.columns:
        try:
            preds = preds[preds["date"].astype(str) == date_q]
        except Exception:
            pass

    # Merge similar to index() non-daily path
    df = preds.copy() if not preds.empty else pd.DataFrame()
    if "game_id" in games.columns:
        g = games.copy()
        g["game_id"] = g["game_id"].astype(str)
        keep = [c for c in [
            "game_id","date","home_team","away_team","home","away","status",
            "home_score","away_score","start_time","commence_time","start_time_local","start_tz_abbr"
        ] if c in g.columns]
        if df.empty:
            df = g[keep].copy()
        else:
            df = df.merge(g[keep], on="game_id", how="left", suffixes=("","_g"))

    # Attach odds
    if not odds.empty:
        try:
            o = odds.copy()
            o["game_id"] = o["game_id"].astype(str)
            direct_cols = {}
            if "market_total" in o.columns:
                direct_cols["market_total"] = "market_total"
            if "closing_total" in o.columns:
                direct_cols["closing_total"] = "closing_total"
            if "spread_home" in o.columns:
                direct_cols["spread_home"] = "spread_home"
            elif "home_spread" in o.columns:
                direct_cols["home_spread"] = "spread_home"
            if "ml_home" in o.columns:
                direct_cols["ml_home"] = "ml_home"
            elif "moneyline_home" in o.columns:
                direct_cols["moneyline_home"] = "ml_home"
            keep_o = ["game_id"] + list(direct_cols.keys())
            o_sub = o[[c for c in keep_o if c in o.columns]]
            if not o_sub.empty:
                agg = o_sub.groupby("game_id").median(numeric_only=True).reset_index()
                agg = agg.rename(columns=direct_cols)
                if df.empty:
                    df = agg
                else:
                    df = df.merge(agg, on="game_id", how="left")
        except Exception:
            pass

    if df.empty or "game_id" not in df.columns:
        return jsonify({"ok": False, "error": "no rows for date/game_id"}), 404

    try:
        df["game_id"] = df["game_id"].astype(str)
    except Exception:
        pass
    df = df[df["game_id"] == game_id_q]
    if df.empty:
        return jsonify({"ok": False, "error": "game_id not found for date"}), 404

    # Apply odds backfill for this slice
    try:
        df = apply_odds_backfill(df)
    except Exception:
        pass

    # Row-level transforms following index() helpers
    rec = df.iloc[0].to_dict()
    try:
        if not rec.get("start_time_iso"):
            rec["start_time_iso"] = _derive_start_iso(rec)
    except Exception:
        pass
    try:
        rec = _backfill_start_fields(rec)
    except Exception:
        pass
    try:
        slate = str(rec.get("date")) if rec.get("date") else None
    except Exception:
        slate = None
    try:
        rec = _correct_midnight_drift(rec, slate_date=slate)
    except Exception:
        pass
    try:
        rec = _apply_site_display_global(rec)
    except Exception:
        pass

    # Restrict payload to most relevant fields for card debugging
    keep_keys = [
        "game_id","date","_slate_date","home_team","away_team","status",
        "start_time","start_time_local","start_tz_abbr","start_time_iso","_start_dt",
        "display_date","display_time_str","start_time_display",
        "pred_total","pred_margin","total_proj_display","margin_proj_display",
        "market_total","closing_total","spread_home","closing_spread_home","ml_home","ml_away"
    ]
    out = {k: rec.get(k) for k in keep_keys}
    return jsonify({"ok": True, "date": date_q, "game_id": game_id_q, "row": out})


@app.route("/api/results")
def api_results():
    """Structured per-date results (scores + predictions + market lines).

    Query params:
      - date=YYYY-MM-DD (required)
      - use_daily=1 to force using daily_results even if scores/preds missing
      - cols=comma,separated to restrict returned columns

    Response JSON:
      {
        "ok": true,
        "meta": { date, daily_used, results_pending, n_rows, n_finals, n_pending, all_final, columns },
        "rows": [ { game_id, home_team, away_team, home_score, away_score, pred_total, market_total, actual_total, edge_total, ... } ]
      }
    """
    date_q = (request.args.get("date") or "").strip()
    if not date_q:
        return jsonify({"ok": False, "error": "missing date"}), 400
    force_daily = (request.args.get("use_daily") or "").strip() in ("1","true","yes")
    df, meta = _build_results_df(date_q, force_use_daily=force_daily)
    if df.empty:
        return jsonify({"ok": True, "meta": meta, "rows": []})
    # Optional column restriction
    cols_req = (request.args.get("cols") or "").strip()
    if cols_req:
        want = [c.strip() for c in cols_req.split(",") if c.strip()]
        have = [c for c in want if c in df.columns]
        if have:
            df = df[have]
    # Ensure primitive types for JSON (avoid numpy types)
    rows: list[dict[str, Any]] = []
    for r in df.to_dict(orient="records"):
        clean = {}
        for k, v in r.items():
            if isinstance(v, (np.generic,)):
                try:
                    v = v.item()
                except Exception:
                    v = float(v) if hasattr(v, "__float__") else str(v)
            if isinstance(v, (dt.datetime, dt.date)):
                v = str(v)
            clean[k] = v
        rows.append(clean)
    meta["returned_columns"] = list(df.columns)
    return jsonify({"ok": True, "meta": meta, "rows": rows})


@app.route("/api/predictions_unified")
def api_predictions_unified():
    """Return unified predictions frame for a given date (or latest).

    Query params:
      - date=YYYY-MM-DD (optional; defaults to today's resolved slate if present)
      - cols=comma,separated list of columns to include (optional)
      - include_sigma=1 to force uncertainty columns even if not exported

    Behavior:
      1. Attempt to load outputs/predictions_unified_<date>.csv when date provided.
      2. If date omitted, try today's file; fallback to global _LAST_UNIFIED_FRAME.
      3. If file missing and global frame exists, filter by date column when possible.
    """
    date_q = (request.args.get("date") or "").strip()
    cols_req = (request.args.get("cols") or "").strip()
    include_sigma = (request.args.get("include_sigma") or "").strip().lower() in ("1","true","yes")
    today_str = None
    try:
        today_str = _today_local().strftime("%Y-%m-%d")
    except Exception:
        today_str = None
    target_date = date_q or today_str

@app.route("/api/backtest")
def api_backtest():
    """Return daily backtest metrics JSON for a given date.

    Query params:
      - date=YYYY-MM-DD (optional; defaults to yesterday if missing to ensure resolution)

    Response: { ok: bool, date: str, metrics: {...} }
    """
    date_q = (request.args.get("date") or "").strip()
    if not date_q:
        # default to yesterday for resolved outcomes
        try:
            date_q = (_today_local() - dt.timedelta(days=1)).strftime('%Y-%m-%d')
        except Exception:
            date_q = None
    if not date_q:
        return jsonify({"ok": False, "error": "no_date"}), 400
    path = OUT / f"backtest_metrics_{date_q}.json"
    if not path.exists():
        return jsonify({"ok": False, "error": "not_found", "date": date_q}), 404
    try:
        import json as _json
        payload = _json.loads(path.read_text(encoding='utf-8'))
        return jsonify({"ok": True, "date": date_q, "metrics": payload})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e), "date": date_q}), 500

@app.route("/api/residuals")
def api_residuals():
    """Return per-day residual distribution summary (totals/margins).

    Query params:
      - date=YYYY-MM-DD (optional; defaults to yesterday for completed games)
    """
    date_q = (request.args.get("date") or "").strip()
    if not date_q:
        try:
            date_q = (_today_local() - dt.timedelta(days=1)).strftime('%Y-%m-%d')
        except Exception:
            date_q = None
    if not date_q:
        return jsonify({"ok": False, "error": "no_date"}), 400
    path = OUT / f"residuals_{date_q}.json"
    if not path.exists():
        return jsonify({"ok": False, "error": "not_found", "date": date_q}), 404
    try:
        import json as _json
        payload = _json.loads(path.read_text(encoding='utf-8'))
        return jsonify({"ok": True, "date": date_q, "residuals": payload})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e), "date": date_q}), 500
    df = pd.DataFrame()
    loaded_path = None
    if target_date:
        path = OUT / f"predictions_unified_{target_date}.csv"
        if path.exists():
            try:
                df = pd.read_csv(path)
                loaded_path = str(path)
            except Exception:
                df = pd.DataFrame()
    # Fallback to global frame
    if df.empty:
        try:
            global _LAST_UNIFIED_FRAME
            if _LAST_UNIFIED_FRAME is not None and isinstance(_LAST_UNIFIED_FRAME, pd.DataFrame) and not _LAST_UNIFIED_FRAME.empty:
                df = _LAST_UNIFIED_FRAME.copy()
                if target_date and "date" in df.columns:
                    df = df[df["date"].astype(str) == target_date]
        except Exception:
            df = pd.DataFrame()
    if df.empty:
        return jsonify({"ok": True, "date": target_date, "rows": [], "note": "no data"})
    # Optional sigma injection if requested and missing
    if include_sigma and "pred_total_sigma" not in df.columns:
        try:
            sigma_t = float(pd.to_numeric(df.get("pred_total"), errors="coerce").std()) if "pred_total" in df.columns else 12.0
            sigma_m = float(pd.to_numeric(df.get("pred_margin"), errors="coerce").std()) if "pred_margin" in df.columns else 8.0
            df["pred_total_sigma"] = sigma_t
            df["pred_margin_sigma"] = sigma_m
        except Exception:
            pass
    if cols_req:
        want = [c.strip() for c in cols_req.split(",") if c.strip()]
        have = [c for c in want if c in df.columns]
        if have:
            df = df[have]
    # Sanitize types for JSON
    out_rows: list[dict[str, Any]] = []
    for r in df.to_dict(orient="records"):
        clean = {}
        for k,v in r.items():
            if isinstance(v, (np.generic,)):
                try:
                    v = v.item()
                except Exception:
                    v = float(v) if hasattr(v, '__float__') else str(v)
            if isinstance(v, (dt.datetime, dt.date)):
                v = str(v)
            clean[k] = v
        out_rows.append(clean)
    meta = {
        "date": target_date,
        "n_rows": len(out_rows),
        "columns": list(df.columns),
        "loaded_path": loaded_path,
        "from_global": loaded_path is None,
    }
    return jsonify({"ok": True, "meta": meta, "rows": out_rows})


# ---------------- New Pages: Stake Sheet, Coverage, Calibration -----------------

@app.route("/stake-sheet")
def stake_sheet_page():
    """Render stake sheet variants and comparison.

    Query params:
      ?view=orig|cal|compare (default orig)
    """
    view = (request.args.get("view") or "orig").strip().lower()
    date_q = (request.args.get("date") or "").strip()
    if view not in {"orig","cal","compare"}:
        view = "orig"
    df = _load_stake_sheet(view, date_q or None)
    # Ensure expected columns exist to avoid Jinja UndefinedError
    expected_common = [
        "game_id","home_team","away_team","market","selection","line","price",
        "prob","kelly_fraction","ev","stake","book","date",
        # deltas (safe to include for all views)
        "delta_prob","delta_kelly","delta_ev","delta_stake",
    ]
    expected_compare = [
        "prob_orig","prob_cal","kelly_fraction_orig","kelly_fraction_cal",
        "ev_orig","ev_cal","stake_orig","stake_cal",
    ]
    if df.empty:
        rows: list[dict[str, Any]] = []
    else:
        for col in expected_common + (expected_compare if view == "compare" else []):
            if col not in df.columns:
                df[col] = None
        rows = df.to_dict(orient="records")
    summary = _summarize_stake_sheet(df)
    return render_template(
        "stake_sheet.html",
        rows=rows,
        total_rows=len(rows),
        view=view,
        date_val=date_q,
        summary=summary,
    )


@app.route("/download/stake")
def download_stake():
    """Download stake sheet CSV for the requested view.

    Query: ?view=orig|cal|compare
    """
    view = (request.args.get("view") or "orig").strip().lower()
    date_q = (request.args.get("date") or "").strip()
    def _cands(kind: str) -> list[Path]:
        lst: list[Path] = []
        if date_q:
            if kind == "orig":
                lst += [OUT / f"stake_sheet_{date_q}.csv"]
            elif kind == "cal":
                lst += [OUT / f"stake_sheet_{date_q}_cal.csv"]
            elif kind == "compare":
                lst += [OUT / f"stake_sheet_{date_q}_compare.csv"]
        if kind == "orig":
            lst += [OUT / "stake_sheet_today.csv"]
        elif kind == "cal":
            lst += [OUT / "stake_sheet_today_cal.csv"]
        elif kind == "compare":
            lst += [OUT / "stake_sheet_today_compare.csv"]
        return lst
    p = None
    for cand in _cands(view):
        if cand.exists():
            p = cand
            break
    if not p or not p.exists():
        return jsonify({"error": "file not found"}), 404
    try:
        return send_file(str(p), as_attachment=True, download_name=p.name)
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route("/coverage")
def coverage_page():
    snap = _compute_coverage_snapshot()
    return render_template("coverage.html", snap=snap)


@app.route("/calibration")
def calibration_page():
    artifact = _load_calibration_artifact()
    compare_df = _load_stake_sheet("compare")
    compare_summary = _summarize_stake_sheet(compare_df)
    # Derive aggregate deltas if comparison present
    deltas: dict[str, Any] = {}
    if not compare_df.empty:
        for col in ["delta_prob","delta_kelly","delta_ev","delta_stake"]:
            if col in compare_df.columns:
                try:
                    ser = pd.to_numeric(compare_df[col], errors="coerce")
                    deltas[f"mean_{col}"] = float(ser.dropna().mean())
                    deltas[f"median_{col}"] = float(ser.dropna().median())
                    deltas[f"sum_{col}"] = float(ser.dropna().sum())
                except Exception:
                    continue
    rows_compare = compare_df.head(50).to_dict(orient="records") if not compare_df.empty else []
    return render_template(
        "calibration.html",
        artifact=artifact,
        compare_rows=rows_compare,
        compare_summary=compare_summary,
        deltas=deltas,
    )

# ---------------- Display Predictions & Dates API (enhancement) -----------------
import re as _re_mod, hashlib as _hashlib_mod

_CLASSIFY_TOL = 1e-6

def _classify_pred_total(row: dict[str, Any]) -> str:
    v = row.get('pred_total')
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return 'unknown'
    try:
        v_float = float(v)
    except Exception:
        return 'unknown'
    cal = row.get('pred_total_calibrated')
    if cal is not None and not pd.isna(cal) and abs(float(cal) - v_float) < _CLASSIFY_TOL:
        return 'cal'
    mr = row.get('pred_total_model')
    if mr is not None and not pd.isna(mr) and abs(float(mr) - v_float) < _CLASSIFY_TOL:
        return 'model_raw'
    mb = row.get('pred_total_model_blended')
    if mb is not None and not pd.isna(mb) and abs(float(mb) - v_float) < _CLASSIFY_TOL:
        return 'blend_model_market'
    dtot = row.get('derived_total')
    if dtot is not None and not pd.isna(dtot) and abs(float(dtot) - v_float) < _CLASSIFY_TOL:
        return 'derived_full'
    # Synthetic baseline heuristics: broad clipped range used in synthetic fills
    if 60 <= v_float <= 192 and row.get('pred_total_basis') in (None, '', 'unknown', np.nan):
        return 'synthetic_baseline_final'
    existing = row.get('pred_total_basis')
    if isinstance(existing, str) and existing:
        return existing
    return 'unknown'

def _classify_pred_margin(row: dict[str, Any]) -> str:
    v = row.get('pred_margin')
    if v is None or (isinstance(v, float) and pd.isna(v)):
        return 'unknown'
    try:
        v_float = float(v)
    except Exception:
        return 'unknown'
    cal = row.get('pred_margin_calibrated')
    if cal is not None and not pd.isna(cal) and abs(float(cal) - v_float) < _CLASSIFY_TOL:
        return 'cal'
    mr = row.get('pred_margin_model')
    if mr is not None and not pd.isna(mr) and abs(float(mr) - v_float) < _CLASSIFY_TOL:
        return 'model_raw'
    sh = row.get('spread_home')
    if sh is not None and not pd.isna(sh) and abs(float(-sh) - v_float) < 1e-3:
        return 'synthetic_from_spread_final'
    if abs(v_float) < 1e-9:
        return 'synthetic_even_final'
    existing = row.get('pred_margin_basis')
    if isinstance(existing, str) and existing:
        return existing
    return 'unknown'

def _normalize_display(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    out = df.copy()
    if 'pred_total_basis' not in out.columns:
        out['pred_total_basis'] = None
    if 'pred_margin_basis' not in out.columns:
        out['pred_margin_basis'] = None
    t_bases = []
    m_bases = []
    for _, r in out.iterrows():
        rr = r.to_dict()
        t_bases.append(_classify_pred_total(rr))
        m_bases.append(_classify_pred_margin(rr))
    out['pred_total_basis'] = t_bases
    out['pred_margin_basis'] = m_bases
    return out

def _persist_display(df: pd.DataFrame, date_str: str) -> tuple[Path, str]:
    norm = _normalize_display(df)
    # Ensure canonical display-date/time fields are present so the cards UI
    # can rely on them directly without recomputing from UTC on each request.
    try:
        if isinstance(norm, pd.DataFrame) and not norm.empty:
            work = norm.copy()
            # display_date: prefer existing display_date/_slate_date, else date.
            if 'display_date' not in work.columns:
                disp_date = None
                if 'display_date' in df.columns:
                    disp_date = df['display_date']
                elif '_slate_date' in df.columns:
                    disp_date = df['_slate_date']
                elif 'date' in df.columns:
                    disp_date = df['date']
                if disp_date is not None:
                    try:
                        work['display_date'] = pd.to_datetime(disp_date, errors='coerce').dt.strftime('%Y-%m-%d')
                    except Exception:
                        work['display_date'] = disp_date.astype(str)
            # display_time_str: prefer preformatted, else local/start time.
            if 'display_time_str' not in work.columns:
                disp_time = None
                if 'display_time_str' in df.columns:
                    disp_time = df['display_time_str']
                elif 'start_time_display' in df.columns:
                    disp_time = df['start_time_display']
                elif 'start_time_local' in df.columns:
                    disp_time = df['start_time_local']
                elif 'start_time' in df.columns:
                    disp_time = df['start_time']
                if disp_time is not None:
                    try:
                        ser = pd.to_datetime(disp_time, errors='coerce')
                        work['display_time_str'] = ser.dt.strftime('%Y-%m-%d %H:%M %Z')
                    except Exception:
                        work['display_time_str'] = disp_time.astype(str)
            norm = work
    except Exception:
        pass
    # Apply date and ESPN subset filters to ensure persisted artifact matches curated slate
    try:
        if isinstance(norm, pd.DataFrame) and not norm.empty and date_str:
            # Date filter
            if 'date' in norm.columns:
                try:
                    norm['date'] = norm['date'].astype(str)
                    norm = norm[norm['date'] == str(date_str)]
                except Exception:
                    pass
            elif 'start_time' in norm.columns:
                try:
                    st_str_p = norm['start_time'].astype(str)
                    st_date_p = st_str_p.str.slice(0, 10)
                    norm = norm[st_date_p == str(date_str)]
                except Exception:
                    pass
            # ESPN subset
            try:
                subset_path_p = OUT / f'schedule_espn_subset_{date_str}.json'
                if subset_path_p.exists():
                    import json as _json
                    payload_p = _json.loads(subset_path_p.read_text(encoding='utf-8'))
                    subset_ids_p: set[str] = set()
                    if isinstance(payload_p, dict):
                        for k in ('ids','espn_ids','game_ids'):
                            v = payload_p.get(k)
                            if isinstance(v, list) and v:
                                subset_ids_p = {str(x) for x in v}
                                break
                    elif isinstance(payload_p, list):
                        subset_ids_p = {str(x) for x in payload_p}
                    if subset_ids_p and 'game_id' in norm.columns:
                        norm['game_id'] = norm['game_id'].astype(str)
                        norm = norm[norm['game_id'].isin(subset_ids_p)].reset_index(drop=True)
            except Exception:
                pass
    except Exception:
        pass
    path = OUT / f'predictions_display_{date_str}.csv'
    try:
        norm.to_csv(path, index=False)
    except Exception:
        pass
    # Hash for alignment checks
    try:
        core = norm[['game_id','pred_total','pred_margin']] if {'game_id','pred_total','pred_margin'}.issubset(norm.columns) else norm
        if 'game_id' in core.columns:
            core = core.sort_values('game_id')
        blob = '\n'.join([
            ','.join(map(str, [row.get(col, '') for col in core.columns])) for _, row in core.iterrows()
        ])
        digest = _hashlib_mod.sha256(blob.encode()).hexdigest()
    except Exception:
        digest = 'hash_error'
    return path, digest

@app.route('/api/display_predictions')
def api_display_predictions():
    date_q = (request.args.get('date') or '').strip()
    # Infer date from last_index_df if not provided
    if not date_q:
        base_df = getattr(app, 'last_index_df', pd.DataFrame())
        if isinstance(base_df, pd.DataFrame) and 'date' in base_df.columns and base_df['date'].notna().any():
            try:
                date_q = str(base_df['date'].dropna().astype(str).unique()[0])
            except Exception:
                date_q = dt.datetime.utcnow().strftime('%Y-%m-%d')
        else:
            date_q = dt.datetime.utcnow().strftime('%Y-%m-%d')
    path = OUT / f'predictions_display_{date_q}.csv'
    if path.exists():
        try:
            df = pd.read_csv(path)
        except Exception:
            df = pd.DataFrame()
    else:
        df = getattr(app, 'last_index_df', pd.DataFrame())
        if not df.empty:
            _persist_display(df, date_q)
    df = _normalize_display(df)
    # Persist again with refined bases (idempotent safe)
    _, digest = _persist_display(df, date_q)
    keep_cols = ['game_id','home_team','away_team','pred_total','pred_margin','pred_total_basis','pred_margin_basis','market_total','spread_home','edge_total','edge_ats','start_time']
    rows: list[dict[str, Any]] = []
    for _, r in df.iterrows():
        item = {}
        for c in keep_cols:
            if c in df.columns:
                item[c] = r.get(c)
        rows.append(item)
    return jsonify({'date': date_q, 'count': len(rows), 'hash': digest, 'rows': rows})

@app.route('/api/display_prediction_dates')
def api_display_prediction_dates():
    dates: list[str] = []
    pat = _re_mod.compile(r'^predictions_display_(\d{4}-\d{2}-\d{2})\.csv$')
    try:
        for p in OUT.glob('predictions_display_*.csv'):
            m = pat.match(p.name)
            if m:
                dates.append(m.group(1))
    except Exception:
        pass
    dates = sorted(set(dates))
    return jsonify({'dates': dates, 'latest': dates[-1] if dates else None})

@app.route('/api/results_dates')
def api_results_dates():
    """List available daily results dates from outputs/daily_results/results_*.csv."""
    import re as _re
    dates: list[str] = []
    pat = _re.compile(r'^results_(\d{4}-\d{2}-\d{2})\.csv$')
    try:
        daily_dir = OUT / 'daily_results'
        if daily_dir.exists():
            for p in daily_dir.glob('results_*.csv'):
                m = pat.match(p.name)
                if m:
                    dates.append(m.group(1))
    except Exception:
        pass
    dates = sorted(set(dates))
    return jsonify({'dates': dates, 'latest': dates[-1] if dates else None})

@app.route('/api/results_by_date')
def api_results_by_date():
    """Return daily results rows for a given date.

    Query params:
      - date: YYYY-MM-DD (required)
    """
    date_q = (request.args.get('date') or '').strip()
    if not date_q:
        return jsonify({'error': 'missing date param'}), 400
    daily_path = OUT / 'daily_results' / f'results_{date_q}.csv'
    if not daily_path.exists():
        return jsonify({'error': 'results file not found', 'date': date_q}), 404
    try:
        df = pd.read_csv(daily_path)
    except Exception as e:
        return jsonify({'error': str(e)}), 500
    rows: list[dict[str, Any]] = []
    keep = [c for c in df.columns if c in ['game_id','date','home_team','away_team','home_score','away_score','ats_result','ou_result_full','ats_home_cover','ou_over']]
    if keep:
        df = df[keep]
    for _, r in df.iterrows():
        rows.append({c: r.get(c) for c in df.columns})
    return jsonify({'date': date_q, 'count': len(rows), 'rows': rows})

@app.route('/api/finalize_hint')
def api_finalize_hint():
    """Provide a lightweight hint for whether finalization is ready based on current artifacts.

    Heuristics:
      - Use today's enriched predictions to count games with final scores.
      - "ready" when pending == 0; else provide pending count and started/commence info if available.
    """
    from datetime import datetime as _dt
    date_q = _dt.utcnow().strftime('%Y-%m-%d')
    enriched = OUT / f'predictions_unified_enriched_{date_q}.csv'
    n_rows = 0
    n_final = 0
    n_pending = 0
    started = 0
    try:
        if enriched.exists():
            df = pd.read_csv(enriched)
            n_rows = int(len(df))
            hs = pd.to_numeric(df.get('home_score'), errors='coerce') if 'home_score' in df.columns else pd.Series(np.nan)
            as_ = pd.to_numeric(df.get('away_score'), errors='coerce') if 'away_score' in df.columns else pd.Series(np.nan)
            is_final = hs.notna() & as_.notna() & ((hs + as_) > 0)
            n_final = int(is_final.sum())
            n_pending = int(n_rows - n_final)
            # Started heuristic: commence_time_odds or _start_dt before now
            now = pd.Timestamp.utcnow()
            if '_start_dt' in df.columns:
                st = pd.to_datetime(df['_start_dt'], errors='coerce')
                started = int((st.notna() & (st <= now)).sum())
    except Exception:
        pass
    ready = bool(n_rows and n_pending == 0)
    return jsonify({'date': date_q, 'rows': n_rows, 'final': n_final, 'pending': n_pending, 'started': started, 'ready': ready})

@app.route('/api/stake_sheet_dates')
def api_stake_sheet_dates():
    """List available dates for stake sheets by scanning outputs/stake_sheet_*.csv files."""
    import re as _re
    dates: list[str] = []
    pat = _re.compile(r'^stake_sheet_(\d{4}-\d{2}-\d{2})')
    try:
        for p in OUT.glob('stake_sheet*.csv'):
            m = pat.match(p.name)
            if m:
                dates.append(m.group(1))
    except Exception:
        pass
    dates = sorted(set(dates))
    return jsonify({'dates': dates, 'latest': dates[-1] if dates else None})

@app.route('/api/stake_sheets')
def api_stake_sheets():
    """Return stake sheet rows for a given date.

    Query params:
      - date: YYYY-MM-DD; if omitted, try today's stake_sheet_today.csv
    """
    date_q = (request.args.get('date') or '').strip()
    from datetime import datetime as _dt
    today = _dt.utcnow().strftime('%Y-%m-%d')
    paths = []
    try:
        if date_q:
            # Try dated stake sheets first
            for p in OUT.glob(f'stake_sheet_{date_q}*.csv'):
                paths.append(p)
        else:
            # Fallback to today's rolling stake sheet
            p = OUT / 'stake_sheet_today.csv'
            if p.exists():
                paths.append(p)
            else:
                for p in OUT.glob(f'stake_sheet_{today}*.csv'):
                    paths.append(p)
    except Exception:
        paths = []
    rows: list[dict[str, Any]] = []
    chosen = None
    for p in sorted(paths):
        try:
            df = pd.read_csv(p)
            if df.empty:
                continue
            chosen = p
            # Pass through as json objects but clip to common fields if huge
            keep = [c for c in df.columns if c in ['game_id','home_team','away_team','pick','market','edge','stake','kelly','confidence','date']]
            if keep:
                df = df[keep]
            rows = [dict(r._asdict()) if hasattr(r, '_asdict') else {c: r[c] for c in df.columns} for _, r in df.iterrows()]
            break
        except Exception:
            continue
    return jsonify({'date': date_q or today, 'file': (str(chosen) if chosen else None), 'count': len(rows), 'rows': rows})

@app.route('/download/display-predictions')
def download_display_predictions_csv():
    """Download the normalized display predictions CSV for a given date (or inferred)."""
    date_q = (request.args.get('date') or '').strip()
    if not date_q:
        base_df = getattr(app, 'last_index_df', pd.DataFrame())
        if isinstance(base_df, pd.DataFrame) and 'date' in base_df.columns and base_df['date'].notna().any():
            date_q = str(base_df['date'].dropna().astype(str).unique()[0])
        else:
            date_q = dt.datetime.utcnow().strftime('%Y-%m-%d')
    path = OUT / f'predictions_display_{date_q}.csv'
    if not path.exists():
        df = getattr(app, 'last_index_df', pd.DataFrame())
        if not df.empty:
            _persist_display(df, date_q)
    if not path.exists():
        return jsonify({'error': 'display file not found', 'date': date_q}), 404
    try:
        return send_file(str(path), as_attachment=True, download_name=path.name)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/display_hash_diff')
def api_display_hash_diff():
    """Compare provided hash with current display predictions hash for date."""
    provided = (request.args.get('hash') or '').strip().lower()
    date_q = (request.args.get('date') or '').strip()
    if not date_q:
        date_q = dt.datetime.utcnow().strftime('%Y-%m-%d')
    path = OUT / f'predictions_display_{date_q}.csv'
    try:
        if path.exists():
            df = pd.read_csv(path)
        else:
            df = getattr(app, 'last_index_df', pd.DataFrame())
        _, current_hash = _persist_display(df, date_q) if not path.exists() else ('unused', None)
        if current_hash is None:
            # Derive from existing file if persistence didn't run
            try:
                df2 = pd.read_csv(path)
                if 'game_id' in df2.columns:
                    df2 = df2.sort_values('game_id')
                blob = '\n'.join([
                    ','.join(map(str, [row.get(col, '') for col in ['game_id','pred_total','pred_margin'] if col in df2.columns]))
                    for _, row in df2.iterrows()
                ])
                current_hash = _hashlib_mod.sha256(blob.encode()).hexdigest()
            except Exception:
                current_hash = 'hash_error'
    except Exception as e:
        return jsonify({'error': str(e), 'date': date_q}), 500
    return jsonify({'date': date_q, 'match': bool(provided and current_hash and provided == current_hash), 'current_hash': current_hash, 'provided': provided or None})

@app.route('/display-archive')
def display_archive():
    """HTML archive explorer for normalized display prediction snapshots.

    Lists available `predictions_display_<date>.csv` artifacts with utility links:
      - View JSON (/api/display_predictions?date=)
      - Download CSV (/download/display-predictions?date=)
      - Hash (computed) and hash diff probe (/api/display_hash_diff?date=&hash=)

    Optional query params:
      remote_base: base URL of remote deployment to show remote health hash for quick compare.
    """
    pat = _re_mod.compile(r'^predictions_display_(\d{4}-\d{2}-\d{2})\.csv$')
    dates: list[str] = []
    try:
        for p in OUT.glob('predictions_display_*.csv'):
            m = pat.match(p.name)
            if m:
                dates.append(m.group(1))
    except Exception:
        pass
    dates = sorted(set(dates))
    remote_base = (request.args.get('remote_base') or '').strip()
    remote_hash = None
    if remote_base:
        try:
            import json as _json_mod, urllib.request as _url_req
            url = f"{remote_base.rstrip('/')}/api/health"
            with _url_req.urlopen(url, timeout=10) as resp:  # type: ignore
                data = resp.read()
            health = _json_mod.loads(data.decode()) if data else {}
            remote_hash = health.get('display_hash')
        except Exception:
            remote_hash = None
    # Compute current local latest hash for convenience
    latest_hash = None
    if dates:
        latest_date = dates[-1]
        latest_path = OUT / f'predictions_display_{latest_date}.csv'
        try:
            if latest_path.exists():
                df_latest = pd.read_csv(latest_path)
                _, latest_hash = _persist_display(df_latest, latest_date)
        except Exception:
            latest_hash = None
    return render_template('archive.html', dates=dates, latest=(dates[-1] if dates else None), remote_base=remote_base or None, remote_hash=remote_hash, latest_hash=latest_hash)

@app.route('/stake-archive')
def stake_archive():
    """HTML page to browse stake sheet archives across dates.

    Uses `/api/stake_sheet_dates` and `/api/stake_sheets` for data.
    """
    return render_template('stake_archive.html')

@app.route('/results-archive')
def results_archive():
    """HTML page to browse finalized results across dates.

    Uses `/api/results_dates` and `/api/results_by_date` for data.
    """
    return render_template('results_archive.html')

## Duplicate results endpoints removed; earlier implementations already exist.

    # (duplicate /api/health removed; existing implementation earlier provides health details)
@app.route('/home')
def home():
    """Simple dashboard with navigation and finalize hint badge."""
    return render_template('home.html')

## Duplicate /api/backtest-summary removed; CSV-based endpoint defined earlier.

@app.route('/backtest')
def backtest_page():
    """Minimal UI page to display latest backtest summary metrics from canonical CSV."""
    try:
        import pandas as _pd
        p = OUT / 'backtest_summary_latest.csv'
        data = None
        if p.exists():
            try:
                df = _pd.read_csv(p)
                if not df.empty:
                    data = df.iloc[0].to_dict()
            except Exception:
                data = None
        return render_template('backtest.html', summary=data)
    except Exception:
        return make_response("<html><body><h2>Backtest summary unavailable</h2></body></html>", 200)

@app.route('/download/backtest-cohort')
def download_backtest_cohort():
    """Download the latest backtest cohort CSV if present."""
    try:
        from pathlib import Path
        p = Path('outputs') / 'backtest_reports' / 'backtest_cohort.csv'
        if p.exists():
            return send_file(str(p), as_attachment=True)
        return jsonify({"status": "missing"}), 404
    except Exception as e:
        return jsonify({"status": "error", "error": str(e)}), 500

# (Unified coverage API moved earlier; legacy summary endpoint removed.)

if __name__ == "__main__":
    port = int(os.environ.get("PORT", "5050"))
    debug_flag = str(os.environ.get("FLASK_DEBUG", "")).lower()
    debug = debug_flag in ("1", "true", "yes", "on")
    try:
        logger.info("Starting Flask app on 0.0.0.0:%s (debug=%s)", port, debug)
    except Exception:
        pass
    try:
        # Perform lightweight bootstrap only when explicitly enabled
        if str(os.environ.get("NCAAB_BOOTSTRAP_ON_START", "")).lower() in ("1","true","yes"):
            ensure_runtime_artifacts()
    except Exception:
        pass
    app.run(host="0.0.0.0", port=port, debug=debug, use_reloader=debug)
